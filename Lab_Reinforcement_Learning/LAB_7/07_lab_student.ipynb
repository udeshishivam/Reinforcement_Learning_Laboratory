{"cells":[{"cell_type":"markdown","id":"0052d469","metadata":{"id":"0052d469"},"source":["# EECS 598 Lab 7: Simplified Rapid Motor Adaptation (Phase 1)\n","\n","![rma](https://www.therobotreport.com/wp-content/uploads/2021/07/legged-robot-adapts.jpg)"]},{"cell_type":"markdown","id":"35fe0d94","metadata":{"id":"35fe0d94"},"source":["This notebook is worth **80 points**. Your score will be calculated as `score = min(score, 80)`.\n","Questions and implementations are marked with relevent `#TODO(student)` markers.\n","\n","Before starting the assignment, please put your name and UMID in the following format:\n","\n","Firstname LASTNAME, #00000000 (ex. Drew SCHEFFER #31415926)"]},{"cell_type":"markdown","id":"80e2a8b0","metadata":{"id":"80e2a8b0"},"source":["**YOUR ANSWER**\n","\n","SHIVAM UDESHI, #87841376"]},{"cell_type":"markdown","id":"051985ae","metadata":{"id":"051985ae"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"id":"7b4d4e9d","metadata":{"id":"7b4d4e9d","executionInfo":{"status":"ok","timestamp":1760639264900,"user_tz":240,"elapsed":32,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["import sys, types, importlib\n","\n","# Create a tiny fake 'imp' module exposing only 'reload'\n","_imp = types.ModuleType(\"imp\")\n","_imp.reload = importlib.reload\n","sys.modules[\"imp\"] = _imp\n","\n","# load autoreload\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":2,"id":"b241b29a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b241b29a","executionInfo":{"status":"ok","timestamp":1760639265488,"user_tz":240,"elapsed":52,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"a0a953e3-b6f9-4327-ed79-fac5a32e8e70"},"outputs":[{"output_type":"stream","name":"stdout","text":["Setting environment variable to use GPU rendering:\n","env: MUJOCO_GL=egl\n","env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"]}],"source":["print('Setting environment variable to use GPU rendering:')\n","%env MUJOCO_GL=egl\n","%env XLA_PYTHON_CLIENT_PREALLOCATE=false"]},{"cell_type":"code","execution_count":3,"id":"e71b5ee4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e71b5ee4","executionInfo":{"status":"ok","timestamp":1760639274864,"user_tz":240,"elapsed":8754,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"270b1bf5-b01c-4ae5-c59b-20336eed807a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Installing mediapy:\n"]}],"source":["#@title Import packages for plotting and creating graphics\n","import time\n","import itertools\n","import numpy as np\n","from typing import Callable, NamedTuple, Optional, Union, List\n","\n","# Graphics and plotting.\n","print('Installing mediapy:')\n","!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n","!pip install -q mediapy\n","import mediapy as media\n","import matplotlib.pyplot as plt\n","\n","# More legible printing from numpy.\n","np.set_printoptions(precision=3, suppress=True, linewidth=100)"]},{"cell_type":"markdown","id":"8ee8f44b","metadata":{"id":"8ee8f44b"},"source":["### Google Colab Setup"]},{"cell_type":"markdown","id":"ee71ce41","metadata":{"id":"ee71ce41"},"source":["Next, we'll run a few commands to set up the environment on Google Colab. If you are running this notebook locally you can skip this section"]},{"cell_type":"markdown","id":"a16e6299","metadata":{"id":"a16e6299"},"source":["Run the following to mount this notebook to your Google Drive. Follow the link and sign into the Google account following the prompts. Use the same Google account that you used to store this notebook."]},{"cell_type":"code","execution_count":4,"id":"547f0b4e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"547f0b4e","executionInfo":{"status":"ok","timestamp":1760639275293,"user_tz":240,"elapsed":428,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"bdb3cc00-b845-488a-e833-ebfd21538124"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","id":"4786706d","metadata":{"id":"4786706d"},"source":["Now update the path below to point to the folder in your Google Drive where you uploaded this notebook. If everything worked correctly you should see the following filenames at least: [`07_lab_student.ipynb`, `EECS598RSLRLBraxWrapper.py`, `rma_go1_locomote.py`, `rma_rsl_rl/`]"]},{"cell_type":"code","execution_count":5,"id":"3f2f3840","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3f2f3840","executionInfo":{"status":"ok","timestamp":1760639275322,"user_tz":240,"elapsed":30,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"77efbf99-d450-4a0f-e506-e617ab854ca8"},"outputs":[{"output_type":"stream","name":"stdout","text":["['EECS598RSLRLBraxWrapper.py', '.DS_Store', 'rma_go1_locomote.py', 'media', 'rma_rsl_rl', 'logs', '__pycache__', '07_lab_student.ipynb']\n"]}],"source":["import os\n","\n","# TODO: Fill in the Google Drive path where you uploaded project 2\n","# Example: If you create a 2025FA folder and put all the files under Lab6, then '2025FA/Lab6'\n","# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2025FA/Lab7'\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '/content/drive/MyDrive/CSE_598/lab7-rma-phase1'\n","GOOGLE_DRIVE_PATH_LAB7 = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","\n","print(os.listdir(GOOGLE_DRIVE_PATH_LAB7))\n","\n","# Add to path and change directory for good measure\n","sys.path.append(GOOGLE_DRIVE_PATH_LAB7)\n","os.chdir(GOOGLE_DRIVE_PATH_LAB7)"]},{"cell_type":"code","execution_count":6,"id":"85dfee08","metadata":{"id":"85dfee08","executionInfo":{"status":"ok","timestamp":1760639275467,"user_tz":240,"elapsed":144,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["from google.colab import files\n","\n","import distutils.util\n","import os\n","import subprocess\n","if subprocess.run('nvidia-smi').returncode:\n","  raise RuntimeError(\n","      'Cannot communicate with GPU. '\n","      'Make sure you are using a GPU Colab runtime. '\n","      'Go to the Runtime menu and select Choose runtime type.')\n","\n","# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n","# This is usually installed as part of an Nvidia driver package, but the Colab\n","# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n","# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n","NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n","if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n","  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n","    f.write(\"\"\"{\n","    \"file_format_version\" : \"1.0.0\",\n","    \"ICD\" : {\n","        \"library_path\" : \"libEGL_nvidia.so.0\"\n","    }\n","}\n","\"\"\")"]},{"cell_type":"markdown","id":"9181618a","metadata":{"id":"9181618a"},"source":["### Install customized rsl_rl library"]},{"cell_type":"code","execution_count":7,"id":"422b1323","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"422b1323","executionInfo":{"status":"ok","timestamp":1760639275512,"user_tz":240,"elapsed":43,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"52bb0493-8f9a-4fa4-ecfb-b1992878b523"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CSE_598/lab7-rma-phase1/rma_rsl_rl\n"]}],"source":["%cd rma_rsl_rl"]},{"cell_type":"code","execution_count":8,"id":"9df7c43f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9df7c43f","executionInfo":{"status":"ok","timestamp":1760639292158,"user_tz":240,"elapsed":16645,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"29319693-c372-4706-b762-5dd1ee07f695"},"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content/drive/MyDrive/CSE_598/lab7-rma-phase1/rma_rsl_rl\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from rsl_rl==1.0.2) (2.8.0+cu126)\n","Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from rsl_rl==1.0.2) (0.23.0+cu126)\n","Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.12/dist-packages (from rsl_rl==1.0.2) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.4.0->rsl_rl==1.0.2) (3.4.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.5.0->rsl_rl==1.0.2) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.4.0->rsl_rl==1.0.2) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.4.0->rsl_rl==1.0.2) (3.0.3)\n","Installing collected packages: rsl_rl\n","  Attempting uninstall: rsl_rl\n","    Found existing installation: rsl_rl 1.0.2\n","    Uninstalling rsl_rl-1.0.2:\n","      Successfully uninstalled rsl_rl-1.0.2\n","  Running setup.py develop for rsl_rl\n","Successfully installed rsl_rl-1.0.2\n"]}],"source":["!pip install -e ."]},{"cell_type":"code","execution_count":9,"id":"f42c102f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f42c102f","executionInfo":{"status":"ok","timestamp":1760639292180,"user_tz":240,"elapsed":20,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"ea4107cb-54b1-41cf-9359-396f8fc8705c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CSE_598/lab7-rma-phase1\n"]}],"source":["%cd .."]},{"cell_type":"markdown","id":"061e8271","metadata":{"id":"061e8271"},"source":["If the below, throws an error, you likely have to restart your runtime."]},{"cell_type":"code","execution_count":10,"id":"168d35af","metadata":{"id":"168d35af","executionInfo":{"status":"ok","timestamp":1760639323632,"user_tz":240,"elapsed":31451,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["import rsl_rl\n","from rsl_rl.runners import OnPolicyRunnerRMA"]},{"cell_type":"markdown","id":"1cc4b0ba","metadata":{"id":"1cc4b0ba"},"source":["## Mujoco, JAX, MJX, BRAX, and Playground Setup & Imports"]},{"cell_type":"code","execution_count":11,"id":"65316e07","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65316e07","executionInfo":{"status":"ok","timestamp":1760639356945,"user_tz":240,"elapsed":33284,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"fbea3288-261b-41ff-c01c-d8fe9d932001"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mujoco\n","  Downloading mujoco-3.3.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (41 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.4.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.13.0)\n","Collecting glfw (from mujoco)\n","  Downloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mujoco) (2.0.2)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco) (3.1.10)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (2025.3.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (6.5.2)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (4.15.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (3.23.0)\n","Downloading mujoco-3.3.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: glfw, mujoco\n","Successfully installed glfw-2.10.0 mujoco-3.3.7\n","Collecting mujoco_mjx\n","  Downloading mujoco_mjx-3.3.7-py3-none-any.whl.metadata (3.4 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco_mjx) (1.4.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco_mjx) (1.13.0)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mujoco_mjx) (0.5.3)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mujoco_mjx) (0.5.3)\n","Requirement already satisfied: mujoco>=3.3.7.dev0 in /usr/local/lib/python3.12/dist-packages (from mujoco_mjx) (3.3.7)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from mujoco_mjx) (1.16.2)\n","Collecting trimesh (from mujoco_mjx)\n","  Downloading trimesh-4.8.3-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: glfw in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.3.7.dev0->mujoco_mjx) (2.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.3.7.dev0->mujoco_mjx) (2.0.2)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.3.7.dev0->mujoco_mjx) (3.1.10)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco_mjx) (2025.3.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco_mjx) (6.5.2)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco_mjx) (4.15.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco_mjx) (3.23.0)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mujoco_mjx) (0.5.3)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mujoco_mjx) (3.4.0)\n","Downloading mujoco_mjx-3.3.7-py3-none-any.whl (7.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trimesh-4.8.3-py3-none-any.whl (735 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.5/735.5 kB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: trimesh, mujoco_mjx\n","Successfully installed mujoco_mjx-3.3.7 trimesh-4.8.3\n","Collecting brax\n","  Downloading brax-0.13.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from brax) (1.4.0)\n","Requirement already satisfied: etils in /usr/local/lib/python3.12/dist-packages (from brax) (1.13.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (from brax) (3.1.2)\n","Collecting flask-cors (from brax)\n","  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (from brax) (0.10.6)\n","Requirement already satisfied: jax>=0.4.6 in /usr/local/lib/python3.12/dist-packages (from brax) (0.5.3)\n","Requirement already satisfied: jaxlib>=0.4.6 in /usr/local/lib/python3.12/dist-packages (from brax) (0.5.3)\n","Collecting jaxopt (from brax)\n","  Downloading jaxopt-0.8.5-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from brax) (3.1.6)\n","Collecting ml-collections (from brax)\n","  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: mujoco in /usr/local/lib/python3.12/dist-packages (from brax) (3.3.7)\n","Requirement already satisfied: mujoco-mjx in /usr/local/lib/python3.12/dist-packages (from brax) (3.3.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from brax) (2.0.2)\n","Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from brax) (0.2.6)\n","Requirement already satisfied: orbax-checkpoint>=0.11.22 in /usr/local/lib/python3.12/dist-packages (from brax) (0.11.24)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from brax) (11.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from brax) (1.16.2)\n","Collecting tensorboardx (from brax)\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: trimesh in /usr/local/lib/python3.12/dist-packages (from brax) (4.8.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from brax) (4.15.0)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.6->brax) (0.5.3)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.6->brax) (3.4.0)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (1.1.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (6.0.3)\n","Requirement already satisfied: tensorstore>=0.1.71 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (0.1.78)\n","Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (1.6.0)\n","Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (24.1.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (5.29.5)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (4.13.0)\n","Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (3.20.2)\n","Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask->brax) (1.9.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask->brax) (8.3.0)\n","Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask->brax) (2.2.0)\n","Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask->brax) (3.0.3)\n","Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask->brax) (3.1.3)\n","Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax->brax) (13.9.4)\n","Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax->brax) (0.1.10)\n","Requirement already satisfied: glfw in /usr/local/lib/python3.12/dist-packages (from mujoco->brax) (2.10.0)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco->brax) (3.1.10)\n","Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->brax) (0.1.90)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboardx->brax) (25.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->brax) (75.2.0)\n","Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->brax) (0.12.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->brax) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->brax) (2.19.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint>=0.11.22->brax) (2025.3.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint>=0.11.22->brax) (6.5.2)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint>=0.11.22->brax) (3.23.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->brax) (0.1.2)\n","Downloading brax-0.13.0-py3-none-any.whl (344 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n","Downloading jaxopt-0.8.5-py3-none-any.whl (172 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.4/172.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboardx, ml-collections, flask-cors, jaxopt, brax\n","Successfully installed brax-0.13.0 flask-cors-6.0.1 jaxopt-0.8.5 ml-collections-1.1.0 tensorboardx-2.6.4\n","Collecting noise\n","  Downloading noise-1.2.2.zip (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: noise\n","  Building wheel for noise (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for noise: filename=noise-1.2.2-cp312-cp312-linux_x86_64.whl size=56629 sha256=11a5d3c67d5f8961b44134c007f54a883c757e3e2c6752554b3bb260770ba32a\n","  Stored in directory: /root/.cache/pip/wheels/78/71/a2/47a0c6acdeb8f7a2f4e69067d3c737219e36414136441a1ef8\n","Successfully built noise\n","Installing collected packages: noise\n","Successfully installed noise-1.2.2\n"]}],"source":["!pip install mujoco\n","!pip install mujoco_mjx\n","!pip install brax\n","!pip install noise\n","\n","# TODO(student): If you're running this locally, make sure to install cuda enabled jax via something like:\n","# !pip install \"jax[cuda12]\""]},{"cell_type":"code","execution_count":12,"id":"cee0de70","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cee0de70","executionInfo":{"status":"ok","timestamp":1760639378021,"user_tz":240,"elapsed":21066,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"f4e1e4e2-cfe8-41aa-946d-a355eacf423c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting playground\n","  Downloading playground-0.0.5-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: brax>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from playground) (0.13.0)\n","Requirement already satisfied: etils in /usr/local/lib/python3.12/dist-packages (from playground) (1.13.0)\n","Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (from playground) (0.10.6)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from playground) (0.5.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from playground) (5.4.0)\n","Requirement already satisfied: ml-collections in /usr/local/lib/python3.12/dist-packages (from playground) (1.1.0)\n","Requirement already satisfied: mujoco-mjx>=3.2.7 in /usr/local/lib/python3.12/dist-packages (from playground) (3.3.7)\n","Requirement already satisfied: mujoco>=3.2.7 in /usr/local/lib/python3.12/dist-packages (from playground) (3.3.7)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from playground) (4.67.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (1.4.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (3.1.2)\n","Requirement already satisfied: flask-cors in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (6.0.1)\n","Requirement already satisfied: jaxlib>=0.4.6 in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (0.5.3)\n","Requirement already satisfied: jaxopt in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (0.8.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (2.0.2)\n","Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (0.2.6)\n","Requirement already satisfied: orbax-checkpoint>=0.11.22 in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (0.11.24)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (11.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (1.16.2)\n","Requirement already satisfied: tensorboardx in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (2.6.4)\n","Requirement already satisfied: trimesh in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (4.8.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (4.15.0)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->playground) (0.5.3)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->playground) (3.4.0)\n","Requirement already satisfied: glfw in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.2.7->playground) (2.10.0)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.2.7->playground) (3.1.10)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax->playground) (1.1.2)\n","Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax->playground) (0.1.78)\n","Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax->playground) (13.9.4)\n","Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax->playground) (6.0.3)\n","Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax->playground) (0.1.10)\n","Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax>=0.12.1->playground) (1.6.0)\n","Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax>=0.12.1->playground) (24.1.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax>=0.12.1->playground) (5.29.5)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax>=0.12.1->playground) (4.13.0)\n","Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax>=0.12.1->playground) (3.20.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->playground) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->playground) (2.19.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=3.2.7->playground) (2025.3.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=3.2.7->playground) (6.5.2)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=3.2.7->playground) (3.23.0)\n","Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask->brax>=0.12.1->playground) (1.9.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask->brax>=0.12.1->playground) (8.3.0)\n","Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask->brax>=0.12.1->playground) (2.2.0)\n","Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask->brax>=0.12.1->playground) (3.0.3)\n","Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask->brax>=0.12.1->playground) (3.1.3)\n","Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->brax>=0.12.1->playground) (0.1.90)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboardx->brax>=0.12.1->playground) (25.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->brax>=0.12.1->playground) (75.2.0)\n","Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->brax>=0.12.1->playground) (0.12.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->playground) (0.1.2)\n","Downloading playground-0.0.5-py3-none-any.whl (7.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: playground\n","Successfully installed playground-0.0.5\n","Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.2)\n","Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.40.0)\n","Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.75.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.9)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n","Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n"]}],"source":["!pip install playground\n","!pip install wandb\n","!pip install tensorboard"]},{"cell_type":"code","execution_count":13,"id":"72dd4342","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72dd4342","executionInfo":{"status":"ok","timestamp":1760639378500,"user_tz":240,"elapsed":486,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"01d9a50a-f381-45eb-e288-3a3842a42d61"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking that the installation succeeded:\n","Installation successful.\n"]}],"source":["import os\n","\n","try:\n","  print('Checking that the installation succeeded:')\n","  import mujoco\n","  mujoco.MjModel.from_xml_string('<mujoco/>')\n","except Exception as e:\n","  raise e from RuntimeError(\n","      'Something went wrong during installation. Check the shell output above '\n","      'for more information.\\n'\n","      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n","      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n","\n","print('Installation successful.')\n","\n","# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n","xla_flags = os.environ.get('XLA_FLAGS', '')\n","xla_flags += ' --xla_gpu_triton_gemm_any=True'\n","os.environ['XLA_FLAGS'] = xla_flags"]},{"cell_type":"markdown","id":"01f742de","metadata":{"id":"01f742de"},"source":["Ensure that the output of the following cell is `[CudaDevice(id=0)]`"]},{"cell_type":"code","execution_count":14,"id":"1519cc79","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1519cc79","executionInfo":{"status":"ok","timestamp":1760639379458,"user_tz":240,"elapsed":956,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"b9267769-5a80-40b5-9efd-b5d5e04a5f7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["[CudaDevice(id=0)]\n"]}],"source":["import jax\n","print(jax.devices())"]},{"cell_type":"code","execution_count":15,"id":"9538916c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9538916c","executionInfo":{"status":"ok","timestamp":1760639389894,"user_tz":240,"elapsed":10432,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"f8ebe83d-eb8c-4715-b5e8-91a63d01ec17"},"outputs":[{"output_type":"stream","name":"stdout","text":["Failed to import warp: No module named 'warp'\n","Failed to import mujoco_warp: No module named 'warp'\n"]}],"source":["#@title Import MuJoCo, MJX, and Brax\n","from datetime import datetime\n","from etils import epath\n","import functools\n","from IPython.display import HTML\n","from typing import Any, Dict, Sequence, Tuple, Union\n","import os\n","from ml_collections import config_dict\n","\n","\n","import jax\n","from jax import numpy as jp\n","import numpy as np\n","from flax.training import orbax_utils\n","from flax import struct\n","from matplotlib import pyplot as plt\n","import mediapy as media\n","from orbax import checkpoint as ocp\n","\n","import mujoco\n","from mujoco import mjx\n","\n","from brax import base\n","from brax import envs\n","from brax import math\n","from brax.base import Base, Motion, Transform\n","from brax.base import State as PipelineState\n","from brax.envs.base import Env, PipelineEnv, State\n","from brax.mjx.base import State as MjxState\n","from brax.training.agents.ppo import train as ppo\n","from brax.training.agents.ppo import networks as ppo_networks\n","from brax.io import html, mjcf, model"]},{"cell_type":"code","execution_count":16,"id":"b4218c78","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4218c78","executionInfo":{"status":"ok","timestamp":1760639429739,"user_tz":240,"elapsed":39842,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"f3a750f2-3173-49b9-95a6-c8841d6cf241"},"outputs":[{"output_type":"stream","name":"stdout","text":["mujoco_menagerie not found. Downloading...\n"]},{"output_type":"stream","name":"stderr","text":["Cloning mujoco_menagerie: ██████████| 100/100 [00:38<00:00]\n"]},{"output_type":"stream","name":"stdout","text":["Checking out commit 14ceccf557cc47240202f2354d684eca58ff8de4\n","Successfully downloaded mujoco_menagerie\n"]}],"source":["\n","import os\n","\n","xla_flags = os.environ.get(\"XLA_FLAGS\", \"\")\n","xla_flags += \" --xla_gpu_triton_gemm_any=True\"\n","os.environ[\"XLA_FLAGS\"] = xla_flags\n","os.environ[\"MUJOCO_GL\"] = \"egl\"\n","\n","from datetime import datetime\n","import json\n","\n","from absl import app\n","from absl import flags\n","from absl import logging\n","import jax\n","import mediapy as media\n","from ml_collections import config_dict\n","import mujoco\n","import torch\n","\n","import mujoco_playground\n","from mujoco_playground import registry\n","from mujoco_playground import wrapper_torch\n","from mujoco_playground import wrapper\n","from mujoco_playground.config import locomotion_params\n","from mujoco_playground.config import manipulation_params\n","from mujoco_playground.config import dm_control_suite_params\n","\n","# Suppress logs if you want\n","logging.set_verbosity(logging.WARNING)"]},{"cell_type":"markdown","id":"5593089f","metadata":{"id":"5593089f"},"source":["# Understanding Rapid Motor Adaptation (RMA)\n","\n","![here](https://ar5iv.labs.arxiv.org/html/2107.04034/assets/x1.png)\n","\n","Today, we'll begin to investigate one popular Sim2Real reinforcement learning paradigm: learning (and eventually distilling) from privileged information available in simulation. One popular implementation of this idea is [Rapid Motor Adaption for Legged Robots (RMA)](https://ashish-kmr.github.io/rma-legged-robots/). In this formulation, training robotic locomotion policies are split up into two main phases, as shown in the figure above.\n","\n","In **phase 1**, a motor policy, $\\pi$, is trained that takes as input the state, $x_t$, the previous action $a_t$ and a learned environment latent vector $z_t$ that encodes features of the environment that may be relevant for adapting the robot's policy, but may not be directly observable when the robot is deployed in the real world (such as friction, center of mass position, etc). In this sense, you can think of $z_t$ as conditioning the policy. At this stage, however, the policy would not be able to be applied directly on a real robot, because the policy requires access to privilegded state to generate $z_t$.\n","\n","In **phase 2**, the motor policy is frozen (no longer updated), and an adaptation module is trained to *approximate* the environment feature $z_t$ using *only* the robot's extended state history $x_{1...T}, a_{1...T}$. Crucially, since we're training both phases in simulation, we can rollout the phase 1 privileged information encoder to get \"ground truth\" $z_t$ labels for each timestep. So, we can do simple supervised learning to estimate $\\hat{z}_t$ from state history!\n","\n","In today's lab, we'll just focus on getting used to the structure of privileged information and implement some key parts of **phase 1** training. I suggest that you briefly look through [the paper](https://ashish-kmr.github.io/rma-legged-robots/rma-locomotion-final.pdf)."]},{"cell_type":"markdown","id":"b01339b2","metadata":{"id":"b01339b2"},"source":["`TODO(student):` Based on the paper, how does this learning strategy compare to simple domain randomization? In other words, what's one negative effect that naive domain randomization can have on a trained policy? **(10 points)**.\n","\n","**[Answer Here]**\n","\n","In the RMA paper, the authors explain that simple domain randomization, where you randomly vary environment parameters like friction, mass, or slopes during training can make the policy too broad and conservative. Because the robot must handle every possible variation it sees, it often learns a “safe” but less optimal behavior that doesnt adapt well to specific real-world conditions. In contrast, RMA trains a student policy that can quickly adapt to new environments by estimating hidden physical properties (like terrain friction) from its recent motion history. This makes it more efficient and robust, allowing the robot to adjust on the fly instead of relying on overly generalized behavior learned from randomization alone.\n","\n"]},{"cell_type":"markdown","id":"8e94a57b","metadata":{"id":"8e94a57b"},"source":["# Setup the Environment for RMA\n","The first step of training Phase 1 of RMA is setting up the structure of the task. This includes modifying the MujocoPlayground environment to define the relevant reward functions, the observation space, and notably the *privileged obserbation space*. Most of this setup has already been done, but fill in the TODOs in `rma_go1_locomote.py` to finish defining the `obs` and `privileged_obs`.\n","\n","`TODO(student):` Implement the TODOs in `rma_go1_locomote.py` **(30 points)**\n","\n"]},{"cell_type":"code","source":["from mujoco_playground._src.locomotion.go1 import base as go1_base\n","import inspect\n","\n","# List all methods in Go1Env\n","methods = inspect.getmembers(go1_base.Go1Env, predicate=inspect.isfunction)\n","for name, func in methods:\n","    print(name)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Ci8voDYcKm8","executionInfo":{"status":"ok","timestamp":1760639657087,"user_tz":240,"elapsed":263,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"76dcfd9d-2d6d-4fde-8f5f-50dafbc6905d"},"id":"0Ci8voDYcKm8","execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["__init__\n","get_accelerometer\n","get_feet_pos\n","get_global_angvel\n","get_global_linvel\n","get_gravity\n","get_gyro\n","get_local_linvel\n","get_upvector\n","render\n","reset\n","step\n"]}]},{"cell_type":"code","source":["# Show source code\n","print(inspect.getsource(go1_base.Go1Env.get_local_linvel))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q0OmqJJSdLbi","executionInfo":{"status":"ok","timestamp":1760639657887,"user_tz":240,"elapsed":63,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"13f16c70-7710-44ee-fb59-cf53b44561b0"},"id":"q0OmqJJSdLbi","execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["  def get_local_linvel(self, data: mjx.Data) -> jax.Array:\n","    return mjx_env.get_sensor_data(\n","        self.mj_model, data, consts.LOCAL_LINVEL_SENSOR\n","    )\n","\n"]}]},{"cell_type":"code","execution_count":null,"id":"e9d6bdd4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e9d6bdd4","executionInfo":{"status":"ok","timestamp":1760590147652,"user_tz":240,"elapsed":24598,"user":{"displayName":"Shivam Udeshi","userId":"16189832898650947981"}},"outputId":"ccf5bc97-b3c8-49d9-f970-0f902d1a19da"},"outputs":[{"output_type":"stream","name":"stdout","text":["State (dim=(45,)):      [ 0.142 -0.47   0.402  0.467 -0.306 -0.29   0.     0.    -1.     0.     0.     0.     0.     0.\n","  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.\n","  0.     0.     0.   ]\n","Privileged State (dim=(6,)):  [ 0.6   12.743  0.     0.     0.     0.   ]\n"]}],"source":["# Test out to see what the priveleged information is...\n","from rma_go1_locomote import LocomotionRMAEnv, go1_rma_default_config\n","\n","env_cfg = go1_rma_default_config()\n","env = LocomotionRMAEnv(task=\"flat_terrain\", config=env_cfg)\n","\n","jit_reset = jax.jit(env.reset)\n","jit_step = jax.jit(env.step)\n","\n","state = jit_reset(jax.random.PRNGKey(0))\n","\n","policy_obs = state.obs[\"state\"]\n","print(f\"State (dim={policy_obs.shape}):     \", policy_obs)\n","\n","env_obs = state.obs[\"privileged_state\"]\n","print(f\"Privileged State (dim={env_obs.shape}): \", env_obs)\n","\n"]},{"cell_type":"markdown","id":"9d7574df","metadata":{"id":"9d7574df"},"source":["`TODO(student):` Take a look at the reward functions used to train the quadruped in the paper. In contrast to the locomotion RL setups that we have seen in our previous labs, the original RMA implementation doesn't include gait-based reward terms encouraging things like foot height, or foot contact time. How do the authors propose to have natural gaits (walking patterns) emerge without this reward engineering?  **(10 points)**\n","\n","**[Answer Here]**\n","\n","In the original RMA paper, the authors avoid manually engineering gait-based rewards like foot height or contact time. Instead, they rely on rapid motor adaptation with a privileged teacher policy during training. The teacher policy has access to detailed state information (like ground friction, contact forces, and torso mass) and is trained to track high-level velocity commands. By learning to satisfy these task-level objectives (like forward velocity tracking and stability penalties), the policy naturally discovers efficient walking patterns and periodic gaits without needing explicit foot placement rewards. Essentially, the gaits emerge implicitly as a consequence of optimizing for overall task performance, rather than being directly enforced.\n","\n","\n","Note: we actually won't be doing this in this lab because it significantly slows down learning. But feel free to implement and run it on your own if you're curious!"]},{"cell_type":"markdown","id":"4abe2a96","metadata":{"id":"4abe2a96"},"source":["# Modifying RSL_RL To Train RMA Phase 1\n","To train Phase 1 of our simplified RMA policy, we’ll need additional components beyond the standard reinforcement learning setup. In particular, this phase requires mechanisms for handling privileged information and training the environment encoder which are features not present in vanilla RL pipelines.\n","\n","To do this, we’ll build off of the familiar [RSL_RL](https://github.com/leggedrobotics/rsl_rl) framework discussed in previous labs, extending its training loop to incorporate these new elements.\n","\n","Our adapted RSL_RL library resides in the `rma_rsl_rl` folder. There are a few important files to look at:\n","- `./rsl_rl/runners/on_policy_runner_rma.py`. This file contains the main training and logging loop and is responsible for rolling out the policy in simulation.\n","- `./rsl_rl/algorithms/ppo_priv.py`. This is a slightly altered version of Proximal Policy Optimization (PPO) that is adjusted to properly handle the privileged environmental data.\n","- `./rsl_rl/modules/actor_critic_latent.py`. This is the main change for Phase 1 where we define and use the \"environment encoder\" as a submodule of the actor critic policy.\n","\n","\n","In essense there are two main things that we had to do to adapt rsl_rl to work in our case\n","\n","1. (Already Done) We had to change how the Runner and PPO scripts accept and handle privileged information. This is mostly boilerplate code. This is necessary because the original rsl_rl implementation uses a different meaning of \"privilieged information\" focused on [Asyncronous Actor Critic (A2C)](https://arxiv.org/pdf/1602.01783).\n","\n","2. (Your Job) Instead of using the vanilla ActorCritic policy that would map (obs, priv_obs) -> actoins, follow the structure of RMA to map (obs, priv_obs) -> (obs, z_t) -> actions. This can be done my creating a new policy, defined in `./rsl_rl/modules/actor_critic_latent.py`."]},{"cell_type":"markdown","id":"86865abe","metadata":{"id":"86865abe"},"source":["`TODO(student):` Fill in the TODOs in `./rsl_rl/modules/actor_critic_latent.py` to follow the structure of RMA. Specifically, implement the `__init__`, `forward`, and `only_latent` functions. **(30 points)**"]},{"cell_type":"markdown","id":"d66e298a","metadata":{"id":"d66e298a"},"source":["# Training Using Privileged Information (optional)\n","In this section, you can combine the above steps to train a Phase-1 RMA policy that learns to encode and use priveleged environment features $z_t$.\n","\n","Training your policy can be a bit finiky at this point, especially using Colab resources. Don't expect the final policy to be of the same quality as the original paper as a few shortcuts were made:\n","1. Not using bumpy terrain\n","2. We train for significantly fewer epochs\n","3. The reward curriculum as defined in the paper has yet to me implemented.\n","4. The reward hyperparameters have not been optimized for our particular simulation platform.\n","\n","To see some results of the limited training feel free to look in the `media/` folder.\n","\n","**The below is optional,** althrough I'd recommend you use the training code to at least test that your above implementations don't crash or error out."]},{"cell_type":"markdown","id":"491a39c5","metadata":{"id":"491a39c5"},"source":["Load all the necessary imports..."]},{"cell_type":"code","execution_count":19,"id":"2e364510","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2e364510","executionInfo":{"status":"ok","timestamp":1760639666996,"user_tz":240,"elapsed":2040,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"0a078973-63a9-423b-a90b-19f7582b0dd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Setting environment variable to use GPU rendering:\n"]}],"source":["import os\n","\n","print(\"Setting environment variable to use GPU rendering:\")\n","os.environ[\"MUJOCO_GL\"] = \"egl\"\n","os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n","\n","from datetime import datetime\n","import os\n","import json\n","\n","from mujoco_playground import registry\n","from mujoco_playground.config import locomotion_params\n","\n","from brax import envs\n","\n","\n","import jax\n","from jax import numpy as jp\n","import numpy as np\n","from flax.training import orbax_utils\n","from flax import struct\n","from matplotlib import pyplot as plt\n","import mediapy as media\n","from orbax import checkpoint as ocp\n","\n","import mujoco\n","from mujoco import mjx\n","\n","import torch\n","\n","import mujoco_playground\n","from mujoco_playground import registry\n","from mujoco_playground import wrapper_torch\n","from mujoco_playground.config import locomotion_params\n","from mujoco_playground.config import manipulation_params\n","from mujoco_playground.config import dm_control_suite_params\n","\n","from EECS598RSLRLBraxWrapper import EECS598RSLRLBraxWrapper\n","\n","from rsl_rl.runners import OnPolicyRunnerRMA\n","\n","from rma_go1_locomote import go1_rma_default_config\n","from EECS598RSLRLBraxWrapper import EECS598RSLRLBraxWrapper\n","\n","import functools\n","import numpy as np\n","from tqdm import tqdm\n","\n","from datetime import datetime\n"]},{"cell_type":"markdown","id":"2907cafd","metadata":{"id":"2907cafd"},"source":["Setup the environment, wrap using the wrapper, and set up the training configuration file"]},{"cell_type":"code","execution_count":20,"id":"5e40111f","metadata":{"id":"5e40111f","executionInfo":{"status":"ok","timestamp":1760639667045,"user_tz":240,"elapsed":48,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["from rma_go1_locomote import go1_rma_default_config, LocomotionRMAEnv\n","\n","_LOAD_RUN_NAME = None\n","_CHECKPOINT_NUM = -1\n","_SUFFIX = None\n","_SEED = 1\n","_NUM_ENVS = 8192\n","_DEVICE = \"cuda:0\"\n","_EXP_NAME = \"RMA\"\n","\n","device=_DEVICE\n","\n","def setup_env_rsl(num_envs):\n","    '''\n","    Create the experiment name, logs/chkpt dirs, load the environment,\n","    and wrap it in a EECS598RSLRLBraxWrapper to make it compatible with rsl_rl\n","    '''\n","\n","    # Create the environment\n","    env_cfg = go1_rma_default_config()\n","    raw_env = LocomotionRMAEnv(task=\"flat_terrain\", config=env_cfg)\n","\n","    # Experiment name\n","    now = datetime.now()\n","    timestamp = now.strftime(\"%Y%m%d-%H%M%S\")\n","    exp_name = f\"{_EXP_NAME}-{timestamp}\"\n","    exp_name += f\"-{_SUFFIX}\"\n","    print(f\"Experiment name: {exp_name}\")\n","\n","    # Logging directory\n","    logdir = os.path.abspath(os.path.join(\"logs\", exp_name))\n","    os.makedirs(logdir, exist_ok=True)\n","    print(f\"Logs are being stored in: {logdir}\")\n","\n","    # Checkpoint directory\n","    ckpt_path = os.path.join(logdir, \"checkpoints\")\n","    os.makedirs(ckpt_path, exist_ok=True)\n","    print(f\"Checkpoint path: {ckpt_path}\")\n","\n","    # Save environment config to JSON\n","    with open(\n","        os.path.join(ckpt_path, \"config.json\"), \"w\", encoding=\"utf-8\"\n","    ) as fp:\n","        json.dump(env_cfg.to_dict(), fp, indent=4)\n","\n","    # Domain randomization\n","    randomizer = registry.get_domain_randomizer(\"Go1JoystickFlatTerrain\")\n","\n","    brax_env = EECS598RSLRLBraxWrapper(\n","        raw_env,\n","        num_envs,\n","        _SEED,\n","        env_cfg.episode_length,\n","        1,\n","        randomization_fn=randomizer,\n","    )\n","\n","    # Build RSL-RL config\n","    train_cfg = locomotion_params.rsl_rl_config(\"Go1JoystickFlatTerrain\")\n","\n","    # Overwrite default config for RMA\n","    train_cfg.seed = _SEED\n","    train_cfg.run_name = exp_name\n","    train_cfg.resume = _LOAD_RUN_NAME is not None\n","    train_cfg.load_run = _LOAD_RUN_NAME if _LOAD_RUN_NAME else \"-1\"\n","    train_cfg.checkpoint = _CHECKPOINT_NUM\n","    train_cfg.runner_class_name = \"OnPolicyRunnerRMA\"\n","\n","    train_cfg.algorithm.num_mini_batches = 32\n","    train_cfg.algorithm.gamma = 0.97\n","\n","    train_cfg_dict = train_cfg.to_dict()\n","    train_cfg_dict[\"policy_class_name\"] = \"ActorCriticLatent\"\n","    train_cfg_dict[\"algorithm_class_name\"] = \"PPO_priv\"\n","\n","    return exp_name, logdir, brax_env, train_cfg_dict"]},{"cell_type":"code","execution_count":21,"id":"80b5fde2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80b5fde2","executionInfo":{"status":"ok","timestamp":1760639712039,"user_tz":240,"elapsed":43297,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"034078f1-73a8-448d-ba20-4c30fc189038"},"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment name: RMA-20251016-183430-None\n","Logs are being stored in: /content/drive/MyDrive/CSE_598/lab7-rma-phase1/logs/RMA-20251016-183430-None\n","Checkpoint path: /content/drive/MyDrive/CSE_598/lab7-rma-phase1/logs/RMA-20251016-183430-None/checkpoints\n","obs_shape: {'privileged_state': (6,), 'state': (45,)}\n","Asymmetric observation space\n","JITing reset and step\n","Done JITing reset and step\n","Running RMA With Privileged Information ...\n","{'algorithm': {'class_name': 'PPO', 'clip_param': 0.2, 'desired_kl': 0.01, 'entropy_coef': 0.001, 'gamma': 0.97, 'lam': 0.95, 'learning_rate': 0.0003, 'max_grad_norm': 1.0, 'num_learning_epochs': 5, 'num_mini_batches': 32, 'schedule': 'fixed', 'use_clipped_value_loss': True, 'value_loss_coef': 1.0}, 'checkpoint': -1, 'empirical_normalization': True, 'experiment_name': 'test', 'load_run': '-1', 'max_iterations': 1000, 'num_steps_per_env': 24, 'policy': {'activation': 'elu', 'actor_hidden_dims': [512, 256, 128], 'class_name': 'ActorCritic', 'critic_hidden_dims': [512, 256, 128], 'init_noise_std': 1.0}, 'resume': False, 'resume_path': None, 'run_name': 'RMA-20251016-183430-None', 'runner_class_name': 'OnPolicyRunnerRMA', 'save_interval': 50, 'seed': 1, 'policy_class_name': 'ActorCriticLatent', 'algorithm_class_name': 'PPO_priv'}\n","ActorCriticLatent.__init__ got unexpected arguments, which will be ignored: ['class_name']\n","Actor MLP: MLPEncode(\n","  (activation_fn): ELU(alpha=1.0)\n","  (output_activation_fn): Tanh()\n","  (priv_encoder): Sequential(\n","    (0): Linear(in_features=6, out_features=256, bias=True)\n","    (1): ELU(alpha=1.0)\n","    (2): Linear(in_features=256, out_features=128, bias=True)\n","    (3): ELU(alpha=1.0)\n","    (4): Linear(in_features=128, out_features=4, bias=True)\n","  )\n","  (action_mlp): Sequential(\n","    (0): Linear(in_features=49, out_features=512, bias=True)\n","    (1): ELU(alpha=1.0)\n","    (2): Linear(in_features=512, out_features=256, bias=True)\n","    (3): ELU(alpha=1.0)\n","    (4): Linear(in_features=256, out_features=128, bias=True)\n","    (5): ELU(alpha=1.0)\n","    (6): Linear(in_features=128, out_features=12, bias=True)\n","    (7): Tanh()\n","  )\n",")\n","Critic MLP: MLPEncode(\n","  (activation_fn): ELU(alpha=1.0)\n","  (priv_encoder): Sequential(\n","    (0): Linear(in_features=6, out_features=256, bias=True)\n","    (1): ELU(alpha=1.0)\n","    (2): Linear(in_features=256, out_features=128, bias=True)\n","    (3): ELU(alpha=1.0)\n","    (4): Linear(in_features=128, out_features=4, bias=True)\n","  )\n","  (action_mlp): Sequential(\n","    (0): Linear(in_features=49, out_features=512, bias=True)\n","    (1): ELU(alpha=1.0)\n","    (2): Linear(in_features=512, out_features=256, bias=True)\n","    (3): ELU(alpha=1.0)\n","    (4): Linear(in_features=256, out_features=128, bias=True)\n","    (5): ELU(alpha=1.0)\n","    (6): Linear(in_features=128, out_features=1, bias=True)\n","  )\n",")\n","This is the algorithm class:  PPO_priv\n","PPO_priv.__init__ got unexpected arguments, which will be ignored: ['class_name']\n","Priveleged PPO is loaded\n","Runner loaded\n"]}],"source":["exp_name, logdir, brax_env, train_cfg_dict = setup_env_rsl(num_envs=_NUM_ENVS)\n","\n","print(\"Running RMA With Privileged Information ...\")\n","print(train_cfg_dict)\n","\n","# train the locomotion policy\n","runner = OnPolicyRunnerRMA(brax_env, train_cfg_dict, logdir, device=device)"]},{"cell_type":"markdown","id":"164f8b1e","metadata":{"id":"164f8b1e"},"source":["Train the policy:"]},{"cell_type":"code","execution_count":22,"id":"06b96e56","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"06b96e56","outputId":"1c4334e7-1e9b-46c6-8b5a-f9800eb4c424","executionInfo":{"status":"ok","timestamp":1760643026648,"user_tz":240,"elapsed":1810351,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","                        Total time: 2232.45s\n","                               ETA: 1117.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 667/1000 \u001b[0m                      \n","\n","                       Computation: 59944 steps/s (collection: 1.835s, learning 1.444s)\n","               Value function loss: 0.0021\n","                    Surrogate loss: 0.0180\n","             Mean action noise std: 0.05\n","                       Mean reward: 97.75\n","               Mean episode length: 838.47\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 131334144\n","                    Iteration time: 3.28s\n","                        Total time: 2235.73s\n","                               ETA: 1114.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 668/1000 \u001b[0m                      \n","\n","                       Computation: 60180 steps/s (collection: 1.873s, learning 1.394s)\n","               Value function loss: 0.0024\n","                    Surrogate loss: 0.0202\n","             Mean action noise std: 0.05\n","                       Mean reward: 99.70\n","               Mean episode length: 853.93\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 131530752\n","                    Iteration time: 3.27s\n","                        Total time: 2239.00s\n","                               ETA: 1111.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 669/1000 \u001b[0m                      \n","\n","                       Computation: 61483 steps/s (collection: 1.828s, learning 1.369s)\n","               Value function loss: 0.0022\n","                    Surrogate loss: 0.0279\n","             Mean action noise std: 0.05\n","                       Mean reward: 99.54\n","               Mean episode length: 852.81\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 131727360\n","                    Iteration time: 3.20s\n","                        Total time: 2242.20s\n","                               ETA: 1107.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 670/1000 \u001b[0m                      \n","\n","                       Computation: 61270 steps/s (collection: 1.842s, learning 1.367s)\n","               Value function loss: 0.0024\n","                    Surrogate loss: 0.0219\n","             Mean action noise std: 0.05\n","                       Mean reward: 103.51\n","               Mean episode length: 886.69\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 131923968\n","                    Iteration time: 3.21s\n","                        Total time: 2245.40s\n","                               ETA: 1104.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 671/1000 \u001b[0m                      \n","\n","                       Computation: 59444 steps/s (collection: 1.831s, learning 1.476s)\n","               Value function loss: 0.0037\n","                    Surrogate loss: 0.0226\n","             Mean action noise std: 0.05\n","                       Mean reward: 113.59\n","               Mean episode length: 964.45\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 132120576\n","                    Iteration time: 3.31s\n","                        Total time: 2248.71s\n","                               ETA: 1100.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 672/1000 \u001b[0m                      \n","\n","                       Computation: 60281 steps/s (collection: 1.871s, learning 1.391s)\n","               Value function loss: 0.0057\n","                    Surrogate loss: 0.0309\n","             Mean action noise std: 0.05\n","                       Mean reward: 114.82\n","               Mean episode length: 973.77\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 132317184\n","                    Iteration time: 3.26s\n","                        Total time: 2251.97s\n","                               ETA: 1097.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 673/1000 \u001b[0m                      \n","\n","                       Computation: 61560 steps/s (collection: 1.831s, learning 1.363s)\n","               Value function loss: 0.0040\n","                    Surrogate loss: 0.0272\n","             Mean action noise std: 0.05\n","                       Mean reward: 112.80\n","               Mean episode length: 957.06\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 132513792\n","                    Iteration time: 3.19s\n","                        Total time: 2255.17s\n","                               ETA: 1094.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 674/1000 \u001b[0m                      \n","\n","                       Computation: 61483 steps/s (collection: 1.828s, learning 1.370s)\n","               Value function loss: 0.0053\n","                    Surrogate loss: 0.0327\n","             Mean action noise std: 0.05\n","                       Mean reward: 109.13\n","               Mean episode length: 927.35\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 132710400\n","                    Iteration time: 3.20s\n","                        Total time: 2258.36s\n","                               ETA: 1090.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 675/1000 \u001b[0m                      \n","\n","                       Computation: 58907 steps/s (collection: 1.837s, learning 1.501s)\n","               Value function loss: 0.0084\n","                    Surrogate loss: 0.0303\n","             Mean action noise std: 0.05\n","                       Mean reward: 104.91\n","               Mean episode length: 892.81\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 132907008\n","                    Iteration time: 3.34s\n","                        Total time: 2261.70s\n","                               ETA: 1087.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 676/1000 \u001b[0m                      \n","\n","                       Computation: 60692 steps/s (collection: 1.867s, learning 1.373s)\n","               Value function loss: 0.0131\n","                    Surrogate loss: 0.0407\n","             Mean action noise std: 0.05\n","                       Mean reward: 98.34\n","               Mean episode length: 834.93\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 133103616\n","                    Iteration time: 3.24s\n","                        Total time: 2264.94s\n","                               ETA: 1084.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 677/1000 \u001b[0m                      \n","\n","                       Computation: 61432 steps/s (collection: 1.829s, learning 1.371s)\n","               Value function loss: 0.0136\n","                    Surrogate loss: 0.0318\n","             Mean action noise std: 0.05\n","                       Mean reward: 102.84\n","               Mean episode length: 869.62\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 133300224\n","                    Iteration time: 3.20s\n","                        Total time: 2268.14s\n","                               ETA: 1080.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 678/1000 \u001b[0m                      \n","\n","                       Computation: 61431 steps/s (collection: 1.832s, learning 1.368s)\n","               Value function loss: 0.0145\n","                    Surrogate loss: 0.0336\n","             Mean action noise std: 0.05\n","                       Mean reward: 97.52\n","               Mean episode length: 826.31\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 133496832\n","                    Iteration time: 3.20s\n","                        Total time: 2271.34s\n","                               ETA: 1077.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 679/1000 \u001b[0m                      \n","\n","                       Computation: 58790 steps/s (collection: 1.836s, learning 1.508s)\n","               Value function loss: 0.0141\n","                    Surrogate loss: 0.0379\n","             Mean action noise std: 0.05\n","                       Mean reward: 88.27\n","               Mean episode length: 749.14\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 133693440\n","                    Iteration time: 3.34s\n","                        Total time: 2274.69s\n","                               ETA: 1073.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 680/1000 \u001b[0m                      \n","\n","                       Computation: 60969 steps/s (collection: 1.855s, learning 1.370s)\n","               Value function loss: 0.0180\n","                    Surrogate loss: 0.0332\n","             Mean action noise std: 0.05\n","                       Mean reward: 93.83\n","               Mean episode length: 792.15\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 133890048\n","                    Iteration time: 3.22s\n","                        Total time: 2277.91s\n","                               ETA: 1070.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 681/1000 \u001b[0m                      \n","\n","                       Computation: 61268 steps/s (collection: 1.836s, learning 1.373s)\n","               Value function loss: 0.0225\n","                    Surrogate loss: 0.0516\n","             Mean action noise std: 0.05\n","                       Mean reward: 95.18\n","               Mean episode length: 804.31\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 134086656\n","                    Iteration time: 3.21s\n","                        Total time: 2281.12s\n","                               ETA: 1067.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 682/1000 \u001b[0m                      \n","\n","                       Computation: 61186 steps/s (collection: 1.832s, learning 1.381s)\n","               Value function loss: 0.0126\n","                    Surrogate loss: 0.0357\n","             Mean action noise std: 0.05\n","                       Mean reward: 87.03\n","               Mean episode length: 745.49\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 134283264\n","                    Iteration time: 3.21s\n","                        Total time: 2284.33s\n","                               ETA: 1063.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 683/1000 \u001b[0m                      \n","\n","                       Computation: 58939 steps/s (collection: 1.840s, learning 1.496s)\n","               Value function loss: 0.0138\n","                    Surrogate loss: 0.0323\n","             Mean action noise std: 0.05\n","                       Mean reward: 91.46\n","               Mean episode length: 773.33\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 134479872\n","                    Iteration time: 3.34s\n","                        Total time: 2287.67s\n","                               ETA: 1060.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 684/1000 \u001b[0m                      \n","\n","                       Computation: 60933 steps/s (collection: 1.851s, learning 1.376s)\n","               Value function loss: 0.0147\n","                    Surrogate loss: 0.0402\n","             Mean action noise std: 0.05\n","                       Mean reward: 93.08\n","               Mean episode length: 788.97\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 134676480\n","                    Iteration time: 3.23s\n","                        Total time: 2290.90s\n","                               ETA: 1056.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 685/1000 \u001b[0m                      \n","\n","                       Computation: 61476 steps/s (collection: 1.834s, learning 1.364s)\n","               Value function loss: 0.0115\n","                    Surrogate loss: 0.0366\n","             Mean action noise std: 0.05\n","                       Mean reward: 95.01\n","               Mean episode length: 802.53\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 134873088\n","                    Iteration time: 3.20s\n","                        Total time: 2294.09s\n","                               ETA: 1053.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 686/1000 \u001b[0m                      \n","\n","                       Computation: 61520 steps/s (collection: 1.830s, learning 1.366s)\n","               Value function loss: 0.0095\n","                    Surrogate loss: 0.0292\n","             Mean action noise std: 0.05\n","                       Mean reward: 101.08\n","               Mean episode length: 848.19\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 135069696\n","                    Iteration time: 3.20s\n","                        Total time: 2297.29s\n","                               ETA: 1050.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 687/1000 \u001b[0m                      \n","\n","                       Computation: 58952 steps/s (collection: 1.845s, learning 1.490s)\n","               Value function loss: 0.0103\n","                    Surrogate loss: 0.0282\n","             Mean action noise std: 0.05\n","                       Mean reward: 94.66\n","               Mean episode length: 796.15\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 135266304\n","                    Iteration time: 3.34s\n","                        Total time: 2300.63s\n","                               ETA: 1046.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 688/1000 \u001b[0m                      \n","\n","                       Computation: 61091 steps/s (collection: 1.846s, learning 1.373s)\n","               Value function loss: 0.0100\n","                    Surrogate loss: 0.0310\n","             Mean action noise std: 0.05\n","                       Mean reward: 89.69\n","               Mean episode length: 755.81\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 135462912\n","                    Iteration time: 3.22s\n","                        Total time: 2303.84s\n","                               ETA: 1043.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 689/1000 \u001b[0m                      \n","\n","                       Computation: 61375 steps/s (collection: 1.838s, learning 1.366s)\n","               Value function loss: 0.0108\n","                    Surrogate loss: 0.0398\n","             Mean action noise std: 0.05\n","                       Mean reward: 100.78\n","               Mean episode length: 845.88\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 135659520\n","                    Iteration time: 3.20s\n","                        Total time: 2307.05s\n","                               ETA: 1039.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 690/1000 \u001b[0m                      \n","\n","                       Computation: 61326 steps/s (collection: 1.836s, learning 1.370s)\n","               Value function loss: 0.0080\n","                    Surrogate loss: 0.0204\n","             Mean action noise std: 0.05\n","                       Mean reward: 102.03\n","               Mean episode length: 851.78\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 135856128\n","                    Iteration time: 3.21s\n","                        Total time: 2310.25s\n","                               ETA: 1036.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 691/1000 \u001b[0m                      \n","\n","                       Computation: 57827 steps/s (collection: 1.850s, learning 1.550s)\n","               Value function loss: 0.0057\n","                    Surrogate loss: 0.0210\n","             Mean action noise std: 0.05\n","                       Mean reward: 101.50\n","               Mean episode length: 859.88\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 136052736\n","                    Iteration time: 3.40s\n","                        Total time: 2313.65s\n","                               ETA: 1033.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 692/1000 \u001b[0m                      \n","\n","                       Computation: 61268 steps/s (collection: 1.838s, learning 1.371s)\n","               Value function loss: 0.0061\n","                    Surrogate loss: 0.0247\n","             Mean action noise std: 0.05\n","                       Mean reward: 106.04\n","               Mean episode length: 886.69\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 136249344\n","                    Iteration time: 3.21s\n","                        Total time: 2316.86s\n","                               ETA: 1029.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 693/1000 \u001b[0m                      \n","\n","                       Computation: 61292 steps/s (collection: 1.838s, learning 1.370s)\n","               Value function loss: 0.0066\n","                    Surrogate loss: 0.0238\n","             Mean action noise std: 0.05\n","                       Mean reward: 104.57\n","               Mean episode length: 872.69\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 136445952\n","                    Iteration time: 3.21s\n","                        Total time: 2320.07s\n","                               ETA: 1026.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 694/1000 \u001b[0m                      \n","\n","                       Computation: 61461 steps/s (collection: 1.830s, learning 1.369s)\n","               Value function loss: 0.0062\n","                    Surrogate loss: 0.0270\n","             Mean action noise std: 0.05\n","                       Mean reward: 104.38\n","               Mean episode length: 882.90\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 136642560\n","                    Iteration time: 3.20s\n","                        Total time: 2323.27s\n","                               ETA: 1022.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 695/1000 \u001b[0m                      \n","\n","                       Computation: 58549 steps/s (collection: 1.852s, learning 1.506s)\n","               Value function loss: 0.0072\n","                    Surrogate loss: 0.0249\n","             Mean action noise std: 0.05\n","                       Mean reward: 96.18\n","               Mean episode length: 807.47\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 136839168\n","                    Iteration time: 3.36s\n","                        Total time: 2326.63s\n","                               ETA: 1019.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 696/1000 \u001b[0m                      \n","\n","                       Computation: 61410 steps/s (collection: 1.831s, learning 1.370s)\n","               Value function loss: 0.0058\n","                    Surrogate loss: 0.0260\n","             Mean action noise std: 0.05\n","                       Mean reward: 95.13\n","               Mean episode length: 801.09\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 137035776\n","                    Iteration time: 3.20s\n","                        Total time: 2329.83s\n","                               ETA: 1016.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 697/1000 \u001b[0m                      \n","\n","                       Computation: 61248 steps/s (collection: 1.835s, learning 1.375s)\n","               Value function loss: 0.0060\n","                    Surrogate loss: 0.0287\n","             Mean action noise std: 0.05\n","                       Mean reward: 93.46\n","               Mean episode length: 789.30\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 137232384\n","                    Iteration time: 3.21s\n","                        Total time: 2333.04s\n","                               ETA: 1012.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 698/1000 \u001b[0m                      \n","\n","                       Computation: 61321 steps/s (collection: 1.829s, learning 1.377s)\n","               Value function loss: 0.0042\n","                    Surrogate loss: 0.0295\n","             Mean action noise std: 0.05\n","                       Mean reward: 95.46\n","               Mean episode length: 804.98\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 137428992\n","                    Iteration time: 3.21s\n","                        Total time: 2336.24s\n","                               ETA: 1009.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 699/1000 \u001b[0m                      \n","\n","                       Computation: 58360 steps/s (collection: 1.858s, learning 1.511s)\n","               Value function loss: 0.0039\n","                    Surrogate loss: 0.0322\n","             Mean action noise std: 0.05\n","                       Mean reward: 101.87\n","               Mean episode length: 855.10\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 137625600\n","                    Iteration time: 3.37s\n","                        Total time: 2339.61s\n","                               ETA: 1006.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 700/1000 \u001b[0m                      \n","\n","                       Computation: 60714 steps/s (collection: 1.843s, learning 1.396s)\n","               Value function loss: 0.0036\n","                    Surrogate loss: 0.0211\n","             Mean action noise std: 0.05\n","                       Mean reward: 96.32\n","               Mean episode length: 813.22\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 137822208\n","                    Iteration time: 3.24s\n","                        Total time: 2342.85s\n","                               ETA: 1002.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 701/1000 \u001b[0m                      \n","\n","                       Computation: 61314 steps/s (collection: 1.833s, learning 1.374s)\n","               Value function loss: 0.0034\n","                    Surrogate loss: 0.0218\n","             Mean action noise std: 0.05\n","                       Mean reward: 88.45\n","               Mean episode length: 745.75\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 138018816\n","                    Iteration time: 3.21s\n","                        Total time: 2346.06s\n","                               ETA: 999.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 702/1000 \u001b[0m                      \n","\n","                       Computation: 61431 steps/s (collection: 1.832s, learning 1.369s)\n","               Value function loss: 0.0033\n","                    Surrogate loss: 0.0207\n","             Mean action noise std: 0.05\n","                       Mean reward: 94.89\n","               Mean episode length: 799.80\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 138215424\n","                    Iteration time: 3.20s\n","                        Total time: 2349.26s\n","                               ETA: 995.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 703/1000 \u001b[0m                      \n","\n","                       Computation: 58730 steps/s (collection: 1.869s, learning 1.478s)\n","               Value function loss: 0.0024\n","                    Surrogate loss: 0.0232\n","             Mean action noise std: 0.05\n","                       Mean reward: 82.59\n","               Mean episode length: 708.72\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 138412032\n","                    Iteration time: 3.35s\n","                        Total time: 2352.61s\n","                               ETA: 992.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 704/1000 \u001b[0m                      \n","\n","                       Computation: 61337 steps/s (collection: 1.835s, learning 1.370s)\n","               Value function loss: 0.0020\n","                    Surrogate loss: 0.0181\n","             Mean action noise std: 0.05\n","                       Mean reward: 76.15\n","               Mean episode length: 659.44\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 138608640\n","                    Iteration time: 3.21s\n","                        Total time: 2355.81s\n","                               ETA: 989.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 705/1000 \u001b[0m                      \n","\n","                       Computation: 61357 steps/s (collection: 1.829s, learning 1.375s)\n","               Value function loss: 0.0020\n","                    Surrogate loss: 0.0180\n","             Mean action noise std: 0.05\n","                       Mean reward: 84.04\n","               Mean episode length: 713.94\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 138805248\n","                    Iteration time: 3.20s\n","                        Total time: 2359.02s\n","                               ETA: 985.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 706/1000 \u001b[0m                      \n","\n","                       Computation: 60632 steps/s (collection: 1.836s, learning 1.407s)\n","               Value function loss: 0.0023\n","                    Surrogate loss: 0.0170\n","             Mean action noise std: 0.05\n","                       Mean reward: 101.72\n","               Mean episode length: 852.22\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 139001856\n","                    Iteration time: 3.24s\n","                        Total time: 2362.26s\n","                               ETA: 982.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 707/1000 \u001b[0m                      \n","\n","                       Computation: 58895 steps/s (collection: 1.866s, learning 1.472s)\n","               Value function loss: 0.0017\n","                    Surrogate loss: 0.0269\n","             Mean action noise std: 0.05\n","                       Mean reward: 110.49\n","               Mean episode length: 920.47\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 139198464\n","                    Iteration time: 3.34s\n","                        Total time: 2365.60s\n","                               ETA: 979.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 708/1000 \u001b[0m                      \n","\n","                       Computation: 61567 steps/s (collection: 1.832s, learning 1.362s)\n","               Value function loss: 0.0022\n","                    Surrogate loss: 0.0201\n","             Mean action noise std: 0.05\n","                       Mean reward: 104.62\n","               Mean episode length: 875.87\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 139395072\n","                    Iteration time: 3.19s\n","                        Total time: 2368.79s\n","                               ETA: 975.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 709/1000 \u001b[0m                      \n","\n","                       Computation: 61322 steps/s (collection: 1.834s, learning 1.372s)\n","               Value function loss: 0.0015\n","                    Surrogate loss: 0.0225\n","             Mean action noise std: 0.05\n","                       Mean reward: 101.95\n","               Mean episode length: 862.39\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 139591680\n","                    Iteration time: 3.21s\n","                        Total time: 2372.00s\n","                               ETA: 972.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 710/1000 \u001b[0m                      \n","\n","                       Computation: 60426 steps/s (collection: 1.842s, learning 1.412s)\n","               Value function loss: 0.0015\n","                    Surrogate loss: 0.0199\n","             Mean action noise std: 0.05\n","                       Mean reward: 102.89\n","               Mean episode length: 863.20\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 139788288\n","                    Iteration time: 3.25s\n","                        Total time: 2375.25s\n","                               ETA: 968.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 711/1000 \u001b[0m                      \n","\n","                       Computation: 59406 steps/s (collection: 1.864s, learning 1.445s)\n","               Value function loss: 0.0018\n","                    Surrogate loss: 0.0249\n","             Mean action noise std: 0.05\n","                       Mean reward: 116.13\n","               Mean episode length: 968.39\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 139984896\n","                    Iteration time: 3.31s\n","                        Total time: 2378.56s\n","                               ETA: 965.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 712/1000 \u001b[0m                      \n","\n","                       Computation: 61455 steps/s (collection: 1.836s, learning 1.363s)\n","               Value function loss: 0.0020\n","                    Surrogate loss: 0.0214\n","             Mean action noise std: 0.04\n","                       Mean reward: 115.45\n","               Mean episode length: 961.92\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 140181504\n","                    Iteration time: 3.20s\n","                        Total time: 2381.76s\n","                               ETA: 962.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 713/1000 \u001b[0m                      \n","\n","                       Computation: 61134 steps/s (collection: 1.832s, learning 1.384s)\n","               Value function loss: 0.0025\n","                    Surrogate loss: 0.0245\n","             Mean action noise std: 0.04\n","                       Mean reward: 120.32\n","               Mean episode length: 999.57\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 140378112\n","                    Iteration time: 3.22s\n","                        Total time: 2384.97s\n","                               ETA: 958.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 714/1000 \u001b[0m                      \n","\n","                       Computation: 60005 steps/s (collection: 1.837s, learning 1.440s)\n","               Value function loss: 0.0027\n","                    Surrogate loss: 0.0271\n","             Mean action noise std: 0.04\n","                       Mean reward: 115.20\n","               Mean episode length: 960.58\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 140574720\n","                    Iteration time: 3.28s\n","                        Total time: 2388.25s\n","                               ETA: 955.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 715/1000 \u001b[0m                      \n","\n","                       Computation: 60377 steps/s (collection: 1.864s, learning 1.393s)\n","               Value function loss: 0.0020\n","                    Surrogate loss: 0.0288\n","             Mean action noise std: 0.04\n","                       Mean reward: 113.30\n","               Mean episode length: 951.83\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 140771328\n","                    Iteration time: 3.26s\n","                        Total time: 2391.51s\n","                               ETA: 951.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 716/1000 \u001b[0m                      \n","\n","                       Computation: 61461 steps/s (collection: 1.829s, learning 1.370s)\n","               Value function loss: 0.0017\n","                    Surrogate loss: 0.0192\n","             Mean action noise std: 0.04\n","                       Mean reward: 116.52\n","               Mean episode length: 966.97\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 140967936\n","                    Iteration time: 3.20s\n","                        Total time: 2394.71s\n","                               ETA: 948.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 717/1000 \u001b[0m                      \n","\n","                       Computation: 61220 steps/s (collection: 1.835s, learning 1.377s)\n","               Value function loss: 0.0013\n","                    Surrogate loss: 0.0253\n","             Mean action noise std: 0.04\n","                       Mean reward: 115.65\n","               Mean episode length: 962.47\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 141164544\n","                    Iteration time: 3.21s\n","                        Total time: 2397.92s\n","                               ETA: 945.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 718/1000 \u001b[0m                      \n","\n","                       Computation: 59456 steps/s (collection: 1.835s, learning 1.471s)\n","               Value function loss: 0.0014\n","                    Surrogate loss: 0.0216\n","             Mean action noise std: 0.04\n","                       Mean reward: 116.68\n","               Mean episode length: 971.64\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 141361152\n","                    Iteration time: 3.31s\n","                        Total time: 2401.22s\n","                               ETA: 941.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 719/1000 \u001b[0m                      \n","\n","                       Computation: 60520 steps/s (collection: 1.874s, learning 1.375s)\n","               Value function loss: 0.0018\n","                    Surrogate loss: 0.0364\n","             Mean action noise std: 0.04\n","                       Mean reward: 118.37\n","               Mean episode length: 983.05\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 141557760\n","                    Iteration time: 3.25s\n","                        Total time: 2404.47s\n","                               ETA: 938.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 720/1000 \u001b[0m                      \n","\n","                       Computation: 61291 steps/s (collection: 1.831s, learning 1.377s)\n","               Value function loss: 0.0014\n","                    Surrogate loss: 0.0302\n","             Mean action noise std: 0.04\n","                       Mean reward: 118.23\n","               Mean episode length: 980.95\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 141754368\n","                    Iteration time: 3.21s\n","                        Total time: 2407.68s\n","                               ETA: 935.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 721/1000 \u001b[0m                      \n","\n","                       Computation: 61446 steps/s (collection: 1.833s, learning 1.367s)\n","               Value function loss: 0.0021\n","                    Surrogate loss: 0.0261\n","             Mean action noise std: 0.04\n","                       Mean reward: 117.34\n","               Mean episode length: 971.67\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 141950976\n","                    Iteration time: 3.20s\n","                        Total time: 2410.88s\n","                               ETA: 931.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 722/1000 \u001b[0m                      \n","\n","                       Computation: 59181 steps/s (collection: 1.836s, learning 1.486s)\n","               Value function loss: 0.0019\n","                    Surrogate loss: 0.0254\n","             Mean action noise std: 0.04\n","                       Mean reward: 117.85\n","               Mean episode length: 980.27\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 142147584\n","                    Iteration time: 3.32s\n","                        Total time: 2414.20s\n","                               ETA: 928.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 723/1000 \u001b[0m                      \n","\n","                       Computation: 60859 steps/s (collection: 1.860s, learning 1.370s)\n","               Value function loss: 0.0025\n","                    Surrogate loss: 0.0297\n","             Mean action noise std: 0.04\n","                       Mean reward: 116.93\n","               Mean episode length: 973.86\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 142344192\n","                    Iteration time: 3.23s\n","                        Total time: 2417.43s\n","                               ETA: 924.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 724/1000 \u001b[0m                      \n","\n","                       Computation: 61167 steps/s (collection: 1.833s, learning 1.381s)\n","               Value function loss: 0.0027\n","                    Surrogate loss: 0.0278\n","             Mean action noise std: 0.04\n","                       Mean reward: 116.29\n","               Mean episode length: 967.77\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 142540800\n","                    Iteration time: 3.21s\n","                        Total time: 2420.65s\n","                               ETA: 921.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 725/1000 \u001b[0m                      \n","\n","                       Computation: 61583 steps/s (collection: 1.830s, learning 1.362s)\n","               Value function loss: 0.0021\n","                    Surrogate loss: 0.0248\n","             Mean action noise std: 0.04\n","                       Mean reward: 117.08\n","               Mean episode length: 972.11\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 142737408\n","                    Iteration time: 3.19s\n","                        Total time: 2423.84s\n","                               ETA: 918.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 726/1000 \u001b[0m                      \n","\n","                       Computation: 58853 steps/s (collection: 1.832s, learning 1.508s)\n","               Value function loss: 0.0018\n","                    Surrogate loss: 0.0365\n","             Mean action noise std: 0.04\n","                       Mean reward: 116.77\n","               Mean episode length: 968.32\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 142934016\n","                    Iteration time: 3.34s\n","                        Total time: 2427.18s\n","                               ETA: 914.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 727/1000 \u001b[0m                      \n","\n","                       Computation: 60833 steps/s (collection: 1.862s, learning 1.370s)\n","               Value function loss: 0.0022\n","                    Surrogate loss: 0.0337\n","             Mean action noise std: 0.04\n","                       Mean reward: 113.98\n","               Mean episode length: 943.07\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 143130624\n","                    Iteration time: 3.23s\n","                        Total time: 2430.41s\n","                               ETA: 911.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 728/1000 \u001b[0m                      \n","\n","                       Computation: 61078 steps/s (collection: 1.833s, learning 1.386s)\n","               Value function loss: 0.0019\n","                    Surrogate loss: 0.0310\n","             Mean action noise std: 0.04\n","                       Mean reward: 118.38\n","               Mean episode length: 975.08\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 143327232\n","                    Iteration time: 3.22s\n","                        Total time: 2433.63s\n","                               ETA: 908.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 729/1000 \u001b[0m                      \n","\n","                       Computation: 61409 steps/s (collection: 1.830s, learning 1.372s)\n","               Value function loss: 0.0027\n","                    Surrogate loss: 0.0300\n","             Mean action noise std: 0.04\n","                       Mean reward: 117.48\n","               Mean episode length: 968.96\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 143523840\n","                    Iteration time: 3.20s\n","                        Total time: 2436.83s\n","                               ETA: 904.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 730/1000 \u001b[0m                      \n","\n","                       Computation: 58891 steps/s (collection: 1.836s, learning 1.503s)\n","               Value function loss: 0.0027\n","                    Surrogate loss: 0.0222\n","             Mean action noise std: 0.04\n","                       Mean reward: 117.86\n","               Mean episode length: 973.28\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 143720448\n","                    Iteration time: 3.34s\n","                        Total time: 2440.17s\n","                               ETA: 901.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 731/1000 \u001b[0m                      \n","\n","                       Computation: 60768 steps/s (collection: 1.861s, learning 1.375s)\n","               Value function loss: 0.0033\n","                    Surrogate loss: 0.0539\n","             Mean action noise std: 0.04\n","                       Mean reward: 112.36\n","               Mean episode length: 933.29\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 143917056\n","                    Iteration time: 3.24s\n","                        Total time: 2443.41s\n","                               ETA: 897.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 732/1000 \u001b[0m                      \n","\n","                       Computation: 61447 steps/s (collection: 1.835s, learning 1.365s)\n","               Value function loss: 0.0053\n","                    Surrogate loss: 0.0390\n","             Mean action noise std: 0.04\n","                       Mean reward: 111.90\n","               Mean episode length: 925.90\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 144113664\n","                    Iteration time: 3.20s\n","                        Total time: 2446.61s\n","                               ETA: 894.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 733/1000 \u001b[0m                      \n","\n","                       Computation: 61154 steps/s (collection: 1.829s, learning 1.386s)\n","               Value function loss: 0.0040\n","                    Surrogate loss: 0.0416\n","             Mean action noise std: 0.04\n","                       Mean reward: 107.28\n","               Mean episode length: 888.09\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 144310272\n","                    Iteration time: 3.21s\n","                        Total time: 2449.82s\n","                               ETA: 891.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 734/1000 \u001b[0m                      \n","\n","                       Computation: 58800 steps/s (collection: 1.838s, learning 1.505s)\n","               Value function loss: 0.0038\n","                    Surrogate loss: 0.0326\n","             Mean action noise std: 0.04\n","                       Mean reward: 110.16\n","               Mean episode length: 912.98\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 144506880\n","                    Iteration time: 3.34s\n","                        Total time: 2453.16s\n","                               ETA: 887.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 735/1000 \u001b[0m                      \n","\n","                       Computation: 61051 steps/s (collection: 1.852s, learning 1.368s)\n","               Value function loss: 0.0045\n","                    Surrogate loss: 0.0416\n","             Mean action noise std: 0.04\n","                       Mean reward: 115.36\n","               Mean episode length: 952.28\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 144703488\n","                    Iteration time: 3.22s\n","                        Total time: 2456.38s\n","                               ETA: 884.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 736/1000 \u001b[0m                      \n","\n","                       Computation: 61538 steps/s (collection: 1.828s, learning 1.367s)\n","               Value function loss: 0.0047\n","                    Surrogate loss: 0.0387\n","             Mean action noise std: 0.04\n","                       Mean reward: 105.94\n","               Mean episode length: 879.02\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 144900096\n","                    Iteration time: 3.19s\n","                        Total time: 2459.58s\n","                               ETA: 881.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 737/1000 \u001b[0m                      \n","\n","                       Computation: 61119 steps/s (collection: 1.827s, learning 1.390s)\n","               Value function loss: 0.0035\n","                    Surrogate loss: 0.0417\n","             Mean action noise std: 0.04\n","                       Mean reward: 105.88\n","               Mean episode length: 881.31\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 145096704\n","                    Iteration time: 3.22s\n","                        Total time: 2462.80s\n","                               ETA: 877.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 738/1000 \u001b[0m                      \n","\n","                       Computation: 58773 steps/s (collection: 1.848s, learning 1.497s)\n","               Value function loss: 0.0030\n","                    Surrogate loss: 0.0397\n","             Mean action noise std: 0.04\n","                       Mean reward: 99.92\n","               Mean episode length: 832.08\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 145293312\n","                    Iteration time: 3.35s\n","                        Total time: 2466.14s\n","                               ETA: 874.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 739/1000 \u001b[0m                      \n","\n","                       Computation: 61178 steps/s (collection: 1.840s, learning 1.374s)\n","               Value function loss: 0.0020\n","                    Surrogate loss: 0.0403\n","             Mean action noise std: 0.04\n","                       Mean reward: 105.97\n","               Mean episode length: 879.64\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 145489920\n","                    Iteration time: 3.21s\n","                        Total time: 2469.36s\n","                               ETA: 870.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 740/1000 \u001b[0m                      \n","\n","                       Computation: 61400 steps/s (collection: 1.832s, learning 1.370s)\n","               Value function loss: 0.0029\n","                    Surrogate loss: 0.0517\n","             Mean action noise std: 0.04\n","                       Mean reward: 111.72\n","               Mean episode length: 925.54\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 145686528\n","                    Iteration time: 3.20s\n","                        Total time: 2472.56s\n","                               ETA: 867.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 741/1000 \u001b[0m                      \n","\n","                       Computation: 61358 steps/s (collection: 1.833s, learning 1.372s)\n","               Value function loss: 0.0021\n","                    Surrogate loss: 0.0371\n","             Mean action noise std: 0.04\n","                       Mean reward: 111.12\n","               Mean episode length: 920.10\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 145883136\n","                    Iteration time: 3.20s\n","                        Total time: 2475.76s\n","                               ETA: 864.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 742/1000 \u001b[0m                      \n","\n","                       Computation: 58839 steps/s (collection: 1.849s, learning 1.492s)\n","               Value function loss: 0.0019\n","                    Surrogate loss: 0.0270\n","             Mean action noise std: 0.04\n","                       Mean reward: 106.70\n","               Mean episode length: 886.39\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 146079744\n","                    Iteration time: 3.34s\n","                        Total time: 2479.10s\n","                               ETA: 860.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 743/1000 \u001b[0m                      \n","\n","                       Computation: 61245 steps/s (collection: 1.843s, learning 1.367s)\n","               Value function loss: 0.0030\n","                    Surrogate loss: 0.0329\n","             Mean action noise std: 0.04\n","                       Mean reward: 103.52\n","               Mean episode length: 859.71\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 146276352\n","                    Iteration time: 3.21s\n","                        Total time: 2482.31s\n","                               ETA: 857.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 744/1000 \u001b[0m                      \n","\n","                       Computation: 61383 steps/s (collection: 1.833s, learning 1.370s)\n","               Value function loss: 0.0016\n","                    Surrogate loss: 0.0249\n","             Mean action noise std: 0.04\n","                       Mean reward: 102.53\n","               Mean episode length: 853.06\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 146472960\n","                    Iteration time: 3.20s\n","                        Total time: 2485.52s\n","                               ETA: 854.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 745/1000 \u001b[0m                      \n","\n","                       Computation: 61469 steps/s (collection: 1.835s, learning 1.363s)\n","               Value function loss: 0.0031\n","                    Surrogate loss: 0.0432\n","             Mean action noise std: 0.04\n","                       Mean reward: 102.80\n","               Mean episode length: 855.75\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 146669568\n","                    Iteration time: 3.20s\n","                        Total time: 2488.71s\n","                               ETA: 850.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 746/1000 \u001b[0m                      \n","\n","                       Computation: 58346 steps/s (collection: 1.855s, learning 1.514s)\n","               Value function loss: 0.0024\n","                    Surrogate loss: 0.0291\n","             Mean action noise std: 0.04\n","                       Mean reward: 87.54\n","               Mean episode length: 733.52\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 146866176\n","                    Iteration time: 3.37s\n","                        Total time: 2492.08s\n","                               ETA: 847.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 747/1000 \u001b[0m                      \n","\n","                       Computation: 61077 steps/s (collection: 1.845s, learning 1.374s)\n","               Value function loss: 0.0021\n","                    Surrogate loss: 0.0314\n","             Mean action noise std: 0.04\n","                       Mean reward: 88.00\n","               Mean episode length: 741.34\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 147062784\n","                    Iteration time: 3.22s\n","                        Total time: 2495.30s\n","                               ETA: 844.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 748/1000 \u001b[0m                      \n","\n","                       Computation: 61237 steps/s (collection: 1.838s, learning 1.372s)\n","               Value function loss: 0.0042\n","                    Surrogate loss: 0.0303\n","             Mean action noise std: 0.04\n","                       Mean reward: 104.99\n","               Mean episode length: 866.16\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 147259392\n","                    Iteration time: 3.21s\n","                        Total time: 2498.51s\n","                               ETA: 840.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 749/1000 \u001b[0m                      \n","\n","                       Computation: 61117 steps/s (collection: 1.829s, learning 1.387s)\n","               Value function loss: 0.0036\n","                    Surrogate loss: 0.0232\n","             Mean action noise std: 0.04\n","                       Mean reward: 98.63\n","               Mean episode length: 818.41\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 147456000\n","                    Iteration time: 3.22s\n","                        Total time: 2501.73s\n","                               ETA: 837.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 750/1000 \u001b[0m                      \n","\n","                       Computation: 58747 steps/s (collection: 1.859s, learning 1.488s)\n","               Value function loss: 0.0033\n","                    Surrogate loss: 0.0297\n","             Mean action noise std: 0.04\n","                       Mean reward: 93.89\n","               Mean episode length: 784.74\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 147652608\n","                    Iteration time: 3.35s\n","                        Total time: 2505.08s\n","                               ETA: 833.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 751/1000 \u001b[0m                      \n","\n","                       Computation: 61188 steps/s (collection: 1.833s, learning 1.380s)\n","               Value function loss: 0.0026\n","                    Surrogate loss: 0.0303\n","             Mean action noise std: 0.04\n","                       Mean reward: 89.72\n","               Mean episode length: 748.34\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 147849216\n","                    Iteration time: 3.21s\n","                        Total time: 2508.29s\n","                               ETA: 830.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 752/1000 \u001b[0m                      \n","\n","                       Computation: 61246 steps/s (collection: 1.838s, learning 1.372s)\n","               Value function loss: 0.0031\n","                    Surrogate loss: 0.0391\n","             Mean action noise std: 0.04\n","                       Mean reward: 99.27\n","               Mean episode length: 825.51\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 148045824\n","                    Iteration time: 3.21s\n","                        Total time: 2511.50s\n","                               ETA: 827.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 753/1000 \u001b[0m                      \n","\n","                       Computation: 61275 steps/s (collection: 1.826s, learning 1.383s)\n","               Value function loss: 0.0034\n","                    Surrogate loss: 0.0392\n","             Mean action noise std: 0.04\n","                       Mean reward: 103.43\n","               Mean episode length: 859.09\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 148242432\n","                    Iteration time: 3.21s\n","                        Total time: 2514.71s\n","                               ETA: 823.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 754/1000 \u001b[0m                      \n","\n","                       Computation: 58618 steps/s (collection: 1.865s, learning 1.489s)\n","               Value function loss: 0.0033\n","                    Surrogate loss: 0.0316\n","             Mean action noise std: 0.04\n","                       Mean reward: 111.80\n","               Mean episode length: 921.97\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 148439040\n","                    Iteration time: 3.35s\n","                        Total time: 2518.06s\n","                               ETA: 820.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 755/1000 \u001b[0m                      \n","\n","                       Computation: 61101 steps/s (collection: 1.837s, learning 1.381s)\n","               Value function loss: 0.0062\n","                    Surrogate loss: 0.0637\n","             Mean action noise std: 0.04\n","                       Mean reward: 114.83\n","               Mean episode length: 945.61\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 148635648\n","                    Iteration time: 3.22s\n","                        Total time: 2521.28s\n","                               ETA: 817.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 756/1000 \u001b[0m                      \n","\n","                       Computation: 61294 steps/s (collection: 1.835s, learning 1.372s)\n","               Value function loss: 0.0061\n","                    Surrogate loss: 0.0482\n","             Mean action noise std: 0.04\n","                       Mean reward: 109.79\n","               Mean episode length: 904.90\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 148832256\n","                    Iteration time: 3.21s\n","                        Total time: 2524.49s\n","                               ETA: 813.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 757/1000 \u001b[0m                      \n","\n","                       Computation: 60636 steps/s (collection: 1.831s, learning 1.411s)\n","               Value function loss: 0.0039\n","                    Surrogate loss: 0.0434\n","             Mean action noise std: 0.04\n","                       Mean reward: 99.36\n","               Mean episode length: 824.25\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 149028864\n","                    Iteration time: 3.24s\n","                        Total time: 2527.73s\n","                               ETA: 810.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 758/1000 \u001b[0m                      \n","\n","                       Computation: 59756 steps/s (collection: 1.857s, learning 1.433s)\n","               Value function loss: 0.0049\n","                    Surrogate loss: 0.0638\n","             Mean action noise std: 0.04\n","                       Mean reward: 103.00\n","               Mean episode length: 850.58\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 149225472\n","                    Iteration time: 3.29s\n","                        Total time: 2531.02s\n","                               ETA: 807.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 759/1000 \u001b[0m                      \n","\n","                       Computation: 61364 steps/s (collection: 1.835s, learning 1.369s)\n","               Value function loss: 0.0063\n","                    Surrogate loss: 0.0393\n","             Mean action noise std: 0.04\n","                       Mean reward: 105.64\n","               Mean episode length: 873.25\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 149422080\n","                    Iteration time: 3.20s\n","                        Total time: 2534.23s\n","                               ETA: 803.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 760/1000 \u001b[0m                      \n","\n","                       Computation: 61518 steps/s (collection: 1.832s, learning 1.364s)\n","               Value function loss: 0.0068\n","                    Surrogate loss: 0.0329\n","             Mean action noise std: 0.04\n","                       Mean reward: 109.06\n","               Mean episode length: 899.54\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 149618688\n","                    Iteration time: 3.20s\n","                        Total time: 2537.42s\n","                               ETA: 800.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 761/1000 \u001b[0m                      \n","\n","                       Computation: 60162 steps/s (collection: 1.833s, learning 1.435s)\n","               Value function loss: 0.0076\n","                    Surrogate loss: 0.0413\n","             Mean action noise std: 0.04\n","                       Mean reward: 108.64\n","               Mean episode length: 896.56\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 149815296\n","                    Iteration time: 3.27s\n","                        Total time: 2540.69s\n","                               ETA: 796.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 762/1000 \u001b[0m                      \n","\n","                       Computation: 60073 steps/s (collection: 1.867s, learning 1.406s)\n","               Value function loss: 0.0081\n","                    Surrogate loss: 0.0488\n","             Mean action noise std: 0.04\n","                       Mean reward: 106.89\n","               Mean episode length: 883.61\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 150011904\n","                    Iteration time: 3.27s\n","                        Total time: 2543.96s\n","                               ETA: 793.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 763/1000 \u001b[0m                      \n","\n","                       Computation: 61296 steps/s (collection: 1.833s, learning 1.374s)\n","               Value function loss: 0.0070\n","                    Surrogate loss: 0.0683\n","             Mean action noise std: 0.04\n","                       Mean reward: 109.18\n","               Mean episode length: 897.82\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 150208512\n","                    Iteration time: 3.21s\n","                        Total time: 2547.17s\n","                               ETA: 790.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 764/1000 \u001b[0m                      \n","\n","                       Computation: 61427 steps/s (collection: 1.833s, learning 1.367s)\n","               Value function loss: 0.0058\n","                    Surrogate loss: 0.0710\n","             Mean action noise std: 0.04\n","                       Mean reward: 110.29\n","               Mean episode length: 903.71\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 150405120\n","                    Iteration time: 3.20s\n","                        Total time: 2550.37s\n","                               ETA: 786.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 765/1000 \u001b[0m                      \n","\n","                       Computation: 59152 steps/s (collection: 1.833s, learning 1.491s)\n","               Value function loss: 0.0052\n","                    Surrogate loss: 0.0351\n","             Mean action noise std: 0.04\n","                       Mean reward: 107.40\n","               Mean episode length: 885.60\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 150601728\n","                    Iteration time: 3.32s\n","                        Total time: 2553.69s\n","                               ETA: 783.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 766/1000 \u001b[0m                      \n","\n","                       Computation: 60930 steps/s (collection: 1.864s, learning 1.363s)\n","               Value function loss: 0.0086\n","                    Surrogate loss: 0.0659\n","             Mean action noise std: 0.04\n","                       Mean reward: 107.83\n","               Mean episode length: 887.86\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 150798336\n","                    Iteration time: 3.23s\n","                        Total time: 2556.92s\n","                               ETA: 780.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 767/1000 \u001b[0m                      \n","\n","                       Computation: 61390 steps/s (collection: 1.837s, learning 1.366s)\n","               Value function loss: 0.0094\n","                    Surrogate loss: 0.0475\n","             Mean action noise std: 0.04\n","                       Mean reward: 106.32\n","               Mean episode length: 878.32\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 150994944\n","                    Iteration time: 3.20s\n","                        Total time: 2560.12s\n","                               ETA: 776.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 768/1000 \u001b[0m                      \n","\n","                       Computation: 61458 steps/s (collection: 1.833s, learning 1.366s)\n","               Value function loss: 0.0153\n","                    Surrogate loss: 0.0600\n","             Mean action noise std: 0.04\n","                       Mean reward: 104.66\n","               Mean episode length: 872.98\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 151191552\n","                    Iteration time: 3.20s\n","                        Total time: 2563.32s\n","                               ETA: 773.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 769/1000 \u001b[0m                      \n","\n","                       Computation: 59219 steps/s (collection: 1.843s, learning 1.477s)\n","               Value function loss: 0.0123\n","                    Surrogate loss: 0.0478\n","             Mean action noise std: 0.04\n","                       Mean reward: 98.46\n","               Mean episode length: 831.87\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 151388160\n","                    Iteration time: 3.32s\n","                        Total time: 2566.64s\n","                               ETA: 770.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 770/1000 \u001b[0m                      \n","\n","                       Computation: 61031 steps/s (collection: 1.855s, learning 1.366s)\n","               Value function loss: 0.0127\n","                    Surrogate loss: 0.0420\n","             Mean action noise std: 0.04\n","                       Mean reward: 107.76\n","               Mean episode length: 892.45\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 151584768\n","                    Iteration time: 3.22s\n","                        Total time: 2569.86s\n","                               ETA: 766.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 771/1000 \u001b[0m                      \n","\n","                       Computation: 61192 steps/s (collection: 1.837s, learning 1.376s)\n","               Value function loss: 0.0079\n","                    Surrogate loss: 0.0539\n","             Mean action noise std: 0.04\n","                       Mean reward: 109.14\n","               Mean episode length: 899.31\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 151781376\n","                    Iteration time: 3.21s\n","                        Total time: 2573.08s\n","                               ETA: 763.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 772/1000 \u001b[0m                      \n","\n","                       Computation: 61124 steps/s (collection: 1.835s, learning 1.382s)\n","               Value function loss: 0.0046\n","                    Surrogate loss: 0.0289\n","             Mean action noise std: 0.04\n","                       Mean reward: 109.58\n","               Mean episode length: 901.18\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 151977984\n","                    Iteration time: 3.22s\n","                        Total time: 2576.29s\n","                               ETA: 759.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 773/1000 \u001b[0m                      \n","\n","                       Computation: 58751 steps/s (collection: 1.834s, learning 1.512s)\n","               Value function loss: 0.0044\n","                    Surrogate loss: 0.0248\n","             Mean action noise std: 0.04\n","                       Mean reward: 119.81\n","               Mean episode length: 979.89\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 152174592\n","                    Iteration time: 3.35s\n","                        Total time: 2579.64s\n","                               ETA: 756.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 774/1000 \u001b[0m                      \n","\n","                       Computation: 60640 steps/s (collection: 1.858s, learning 1.384s)\n","               Value function loss: 0.0033\n","                    Surrogate loss: 0.0254\n","             Mean action noise std: 0.04\n","                       Mean reward: 117.12\n","               Mean episode length: 962.07\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 152371200\n","                    Iteration time: 3.24s\n","                        Total time: 2582.88s\n","                               ETA: 753.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 775/1000 \u001b[0m                      \n","\n","                       Computation: 61043 steps/s (collection: 1.841s, learning 1.380s)\n","               Value function loss: 0.0031\n","                    Surrogate loss: 0.0305\n","             Mean action noise std: 0.04\n","                       Mean reward: 113.72\n","               Mean episode length: 933.17\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 152567808\n","                    Iteration time: 3.22s\n","                        Total time: 2586.10s\n","                               ETA: 749.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 776/1000 \u001b[0m                      \n","\n","                       Computation: 61201 steps/s (collection: 1.837s, learning 1.375s)\n","               Value function loss: 0.0022\n","                    Surrogate loss: 0.0201\n","             Mean action noise std: 0.04\n","                       Mean reward: 116.08\n","               Mean episode length: 951.77\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 152764416\n","                    Iteration time: 3.21s\n","                        Total time: 2589.31s\n","                               ETA: 746.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 777/1000 \u001b[0m                      \n","\n","                       Computation: 58742 steps/s (collection: 1.839s, learning 1.508s)\n","               Value function loss: 0.0027\n","                    Surrogate loss: 0.0243\n","             Mean action noise std: 0.04\n","                       Mean reward: 112.75\n","               Mean episode length: 925.64\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 152961024\n","                    Iteration time: 3.35s\n","                        Total time: 2592.66s\n","                               ETA: 743.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 778/1000 \u001b[0m                      \n","\n","                       Computation: 61185 steps/s (collection: 1.845s, learning 1.368s)\n","               Value function loss: 0.0025\n","                    Surrogate loss: 0.0316\n","             Mean action noise std: 0.04\n","                       Mean reward: 113.22\n","               Mean episode length: 930.35\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 153157632\n","                    Iteration time: 3.21s\n","                        Total time: 2595.87s\n","                               ETA: 739.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 779/1000 \u001b[0m                      \n","\n","                       Computation: 61323 steps/s (collection: 1.836s, learning 1.370s)\n","               Value function loss: 0.0027\n","                    Surrogate loss: 0.0234\n","             Mean action noise std: 0.04\n","                       Mean reward: 117.72\n","               Mean episode length: 969.76\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 153354240\n","                    Iteration time: 3.21s\n","                        Total time: 2599.08s\n","                               ETA: 736.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 780/1000 \u001b[0m                      \n","\n","                       Computation: 61203 steps/s (collection: 1.830s, learning 1.382s)\n","               Value function loss: 0.0033\n","                    Surrogate loss: 0.0342\n","             Mean action noise std: 0.04\n","                       Mean reward: 105.96\n","               Mean episode length: 876.20\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 153550848\n","                    Iteration time: 3.21s\n","                        Total time: 2602.29s\n","                               ETA: 733.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 781/1000 \u001b[0m                      \n","\n","                       Computation: 59065 steps/s (collection: 1.846s, learning 1.483s)\n","               Value function loss: 0.0020\n","                    Surrogate loss: 0.0218\n","             Mean action noise std: 0.04\n","                       Mean reward: 113.82\n","               Mean episode length: 938.22\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 153747456\n","                    Iteration time: 3.33s\n","                        Total time: 2605.62s\n","                               ETA: 729.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 782/1000 \u001b[0m                      \n","\n","                       Computation: 60961 steps/s (collection: 1.847s, learning 1.378s)\n","               Value function loss: 0.0014\n","                    Surrogate loss: 0.0256\n","             Mean action noise std: 0.04\n","                       Mean reward: 110.26\n","               Mean episode length: 913.02\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 153944064\n","                    Iteration time: 3.23s\n","                        Total time: 2608.85s\n","                               ETA: 726.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 783/1000 \u001b[0m                      \n","\n","                       Computation: 61122 steps/s (collection: 1.843s, learning 1.374s)\n","               Value function loss: 0.0018\n","                    Surrogate loss: 0.0198\n","             Mean action noise std: 0.04\n","                       Mean reward: 118.90\n","               Mean episode length: 978.93\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 154140672\n","                    Iteration time: 3.22s\n","                        Total time: 2612.06s\n","                               ETA: 723.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 784/1000 \u001b[0m                      \n","\n","                       Computation: 61443 steps/s (collection: 1.834s, learning 1.366s)\n","               Value function loss: 0.0025\n","                    Surrogate loss: 0.0223\n","             Mean action noise std: 0.04\n","                       Mean reward: 106.32\n","               Mean episode length: 878.45\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 154337280\n","                    Iteration time: 3.20s\n","                        Total time: 2615.26s\n","                               ETA: 719.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 785/1000 \u001b[0m                      \n","\n","                       Computation: 58020 steps/s (collection: 1.847s, learning 1.542s)\n","               Value function loss: 0.0026\n","                    Surrogate loss: 0.0220\n","             Mean action noise std: 0.04\n","                       Mean reward: 111.47\n","               Mean episode length: 927.81\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 154533888\n","                    Iteration time: 3.39s\n","                        Total time: 2618.65s\n","                               ETA: 716.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 786/1000 \u001b[0m                      \n","\n","                       Computation: 61032 steps/s (collection: 1.844s, learning 1.377s)\n","               Value function loss: 0.0017\n","                    Surrogate loss: 0.0230\n","             Mean action noise std: 0.04\n","                       Mean reward: 107.39\n","               Mean episode length: 897.89\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 154730496\n","                    Iteration time: 3.22s\n","                        Total time: 2621.87s\n","                               ETA: 712.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 787/1000 \u001b[0m                      \n","\n","                       Computation: 60931 steps/s (collection: 1.840s, learning 1.387s)\n","               Value function loss: 0.0008\n","                    Surrogate loss: 0.0233\n","             Mean action noise std: 0.04\n","                       Mean reward: 109.92\n","               Mean episode length: 907.28\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 154927104\n","                    Iteration time: 3.23s\n","                        Total time: 2625.10s\n","                               ETA: 709.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 788/1000 \u001b[0m                      \n","\n","                       Computation: 61298 steps/s (collection: 1.835s, learning 1.372s)\n","               Value function loss: 0.0014\n","                    Surrogate loss: 0.0184\n","             Mean action noise std: 0.04\n","                       Mean reward: 107.11\n","               Mean episode length: 900.64\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 155123712\n","                    Iteration time: 3.21s\n","                        Total time: 2628.31s\n","                               ETA: 706.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 789/1000 \u001b[0m                      \n","\n","                       Computation: 58417 steps/s (collection: 1.862s, learning 1.504s)\n","               Value function loss: 0.0019\n","                    Surrogate loss: 0.0225\n","             Mean action noise std: 0.04\n","                       Mean reward: 106.31\n","               Mean episode length: 904.54\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 155320320\n","                    Iteration time: 3.37s\n","                        Total time: 2631.67s\n","                               ETA: 702.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 790/1000 \u001b[0m                      \n","\n","                       Computation: 61187 steps/s (collection: 1.845s, learning 1.368s)\n","               Value function loss: 0.0054\n","                    Surrogate loss: 0.0344\n","             Mean action noise std: 0.04\n","                       Mean reward: 117.21\n","               Mean episode length: 963.29\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 155516928\n","                    Iteration time: 3.21s\n","                        Total time: 2634.89s\n","                               ETA: 699.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 791/1000 \u001b[0m                      \n","\n","                       Computation: 61267 steps/s (collection: 1.840s, learning 1.369s)\n","               Value function loss: 0.0082\n","                    Surrogate loss: 0.0717\n","             Mean action noise std: 0.06\n","                       Mean reward: 106.90\n","               Mean episode length: 893.69\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 155713536\n","                    Iteration time: 3.21s\n","                        Total time: 2638.10s\n","                               ETA: 696.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 792/1000 \u001b[0m                      \n","\n","                       Computation: 61048 steps/s (collection: 1.837s, learning 1.383s)\n","               Value function loss: 0.0210\n","                    Surrogate loss: 0.0363\n","             Mean action noise std: 0.06\n","                       Mean reward: 99.88\n","               Mean episode length: 841.43\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 155910144\n","                    Iteration time: 3.22s\n","                        Total time: 2641.32s\n","                               ETA: 692.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 793/1000 \u001b[0m                      \n","\n","                       Computation: 58270 steps/s (collection: 1.864s, learning 1.510s)\n","               Value function loss: 0.0194\n","                    Surrogate loss: 0.0374\n","             Mean action noise std: 0.06\n","                       Mean reward: 101.09\n","               Mean episode length: 851.59\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 156106752\n","                    Iteration time: 3.37s\n","                        Total time: 2644.69s\n","                               ETA: 689.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 794/1000 \u001b[0m                      \n","\n","                       Computation: 61073 steps/s (collection: 1.836s, learning 1.383s)\n","               Value function loss: 0.0693\n","                    Surrogate loss: 0.1155\n","             Mean action noise std: 0.06\n","                       Mean reward: 99.48\n","               Mean episode length: 845.19\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 156303360\n","                    Iteration time: 3.22s\n","                        Total time: 2647.91s\n","                               ETA: 686.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 795/1000 \u001b[0m                      \n","\n","                       Computation: 61368 steps/s (collection: 1.837s, learning 1.367s)\n","               Value function loss: 0.5069\n","                    Surrogate loss: 0.0625\n","             Mean action noise std: 0.06\n","                       Mean reward: 80.97\n","               Mean episode length: 700.59\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 156499968\n","                    Iteration time: 3.20s\n","                        Total time: 2651.11s\n","                               ETA: 682.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 796/1000 \u001b[0m                      \n","\n","                       Computation: 61492 steps/s (collection: 1.827s, learning 1.370s)\n","               Value function loss: 0.2799\n","                    Surrogate loss: 0.0931\n","             Mean action noise std: 0.06\n","                       Mean reward: 83.13\n","               Mean episode length: 722.28\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 156696576\n","                    Iteration time: 3.20s\n","                        Total time: 2654.31s\n","                               ETA: 679.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 797/1000 \u001b[0m                      \n","\n","                       Computation: 58690 steps/s (collection: 1.873s, learning 1.477s)\n","               Value function loss: 0.1991\n","                    Surrogate loss: 0.0807\n","             Mean action noise std: 0.06\n","                       Mean reward: 61.29\n","               Mean episode length: 559.13\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 156893184\n","                    Iteration time: 3.35s\n","                        Total time: 2657.66s\n","                               ETA: 676.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 798/1000 \u001b[0m                      \n","\n","                       Computation: 61261 steps/s (collection: 1.833s, learning 1.376s)\n","               Value function loss: 0.1415\n","                    Surrogate loss: 0.0533\n","             Mean action noise std: 0.06\n","                       Mean reward: 50.80\n","               Mean episode length: 473.12\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 157089792\n","                    Iteration time: 3.21s\n","                        Total time: 2660.87s\n","                               ETA: 672.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 799/1000 \u001b[0m                      \n","\n","                       Computation: 61216 steps/s (collection: 1.838s, learning 1.373s)\n","               Value function loss: 0.1216\n","                    Surrogate loss: 0.0921\n","             Mean action noise std: 0.07\n","                       Mean reward: 43.62\n","               Mean episode length: 417.76\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 157286400\n","                    Iteration time: 3.21s\n","                        Total time: 2664.08s\n","                               ETA: 669.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 800/1000 \u001b[0m                      \n","\n","                       Computation: 60424 steps/s (collection: 1.836s, learning 1.418s)\n","               Value function loss: 0.0946\n","                    Surrogate loss: 0.0443\n","             Mean action noise std: 0.07\n","                       Mean reward: 36.21\n","               Mean episode length: 356.59\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 157483008\n","                    Iteration time: 3.25s\n","                        Total time: 2667.33s\n","                               ETA: 666.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 801/1000 \u001b[0m                      \n","\n","                       Computation: 59149 steps/s (collection: 1.858s, learning 1.466s)\n","               Value function loss: 0.0860\n","                    Surrogate loss: 0.0346\n","             Mean action noise std: 0.06\n","                       Mean reward: 37.11\n","               Mean episode length: 371.30\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 157679616\n","                    Iteration time: 3.32s\n","                        Total time: 2670.66s\n","                               ETA: 662.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 802/1000 \u001b[0m                      \n","\n","                       Computation: 60710 steps/s (collection: 1.846s, learning 1.393s)\n","               Value function loss: 0.0634\n","                    Surrogate loss: 0.0243\n","             Mean action noise std: 0.07\n","                       Mean reward: 39.72\n","               Mean episode length: 397.48\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 157876224\n","                    Iteration time: 3.24s\n","                        Total time: 2673.90s\n","                               ETA: 659.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 803/1000 \u001b[0m                      \n","\n","                       Computation: 61153 steps/s (collection: 1.844s, learning 1.371s)\n","               Value function loss: 0.0566\n","                    Surrogate loss: 0.0219\n","             Mean action noise std: 0.07\n","                       Mean reward: 49.35\n","               Mean episode length: 473.20\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 158072832\n","                    Iteration time: 3.21s\n","                        Total time: 2677.11s\n","                               ETA: 656.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 804/1000 \u001b[0m                      \n","\n","                       Computation: 60203 steps/s (collection: 1.831s, learning 1.434s)\n","               Value function loss: 0.0448\n","                    Surrogate loss: 0.0210\n","             Mean action noise std: 0.07\n","                       Mean reward: 42.97\n","               Mean episode length: 418.99\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 158269440\n","                    Iteration time: 3.27s\n","                        Total time: 2680.38s\n","                               ETA: 652.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 805/1000 \u001b[0m                      \n","\n","                       Computation: 59739 steps/s (collection: 1.863s, learning 1.428s)\n","               Value function loss: 0.0329\n","                    Surrogate loss: 0.0166\n","             Mean action noise std: 0.06\n","                       Mean reward: 56.15\n","               Mean episode length: 544.51\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 158466048\n","                    Iteration time: 3.29s\n","                        Total time: 2683.67s\n","                               ETA: 649.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 806/1000 \u001b[0m                      \n","\n","                       Computation: 61208 steps/s (collection: 1.837s, learning 1.375s)\n","               Value function loss: 0.0264\n","                    Surrogate loss: 0.0117\n","             Mean action noise std: 0.06\n","                       Mean reward: 53.72\n","               Mean episode length: 534.58\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 158662656\n","                    Iteration time: 3.21s\n","                        Total time: 2686.88s\n","                               ETA: 645.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 807/1000 \u001b[0m                      \n","\n","                       Computation: 61213 steps/s (collection: 1.840s, learning 1.371s)\n","               Value function loss: 0.0218\n","                    Surrogate loss: 0.0129\n","             Mean action noise std: 0.06\n","                       Mean reward: 62.61\n","               Mean episode length: 591.06\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 158859264\n","                    Iteration time: 3.21s\n","                        Total time: 2690.09s\n","                               ETA: 642.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 808/1000 \u001b[0m                      \n","\n","                       Computation: 59748 steps/s (collection: 1.829s, learning 1.462s)\n","               Value function loss: 0.0179\n","                    Surrogate loss: 0.0142\n","             Mean action noise std: 0.06\n","                       Mean reward: 64.80\n","               Mean episode length: 605.48\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 159055872\n","                    Iteration time: 3.29s\n","                        Total time: 2693.38s\n","                               ETA: 639.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 809/1000 \u001b[0m                      \n","\n","                       Computation: 60670 steps/s (collection: 1.861s, learning 1.380s)\n","               Value function loss: 0.0169\n","                    Surrogate loss: 0.0135\n","             Mean action noise std: 0.06\n","                       Mean reward: 70.83\n","               Mean episode length: 684.52\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 159252480\n","                    Iteration time: 3.24s\n","                        Total time: 2696.62s\n","                               ETA: 635.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 810/1000 \u001b[0m                      \n","\n","                       Computation: 61113 steps/s (collection: 1.835s, learning 1.382s)\n","               Value function loss: 0.0133\n","                    Surrogate loss: 0.0165\n","             Mean action noise std: 0.06\n","                       Mean reward: 63.85\n","               Mean episode length: 674.61\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 159449088\n","                    Iteration time: 3.22s\n","                        Total time: 2699.84s\n","                               ETA: 632.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 811/1000 \u001b[0m                      \n","\n","                       Computation: 60997 steps/s (collection: 1.843s, learning 1.380s)\n","               Value function loss: 0.0124\n","                    Surrogate loss: 0.0082\n","             Mean action noise std: 0.06\n","                       Mean reward: 81.36\n","               Mean episode length: 800.62\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 159645696\n","                    Iteration time: 3.22s\n","                        Total time: 2703.06s\n","                               ETA: 629.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 812/1000 \u001b[0m                      \n","\n","                       Computation: 59288 steps/s (collection: 1.832s, learning 1.484s)\n","               Value function loss: 0.0108\n","                    Surrogate loss: 0.0111\n","             Mean action noise std: 0.06\n","                       Mean reward: 76.18\n","               Mean episode length: 790.51\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 159842304\n","                    Iteration time: 3.32s\n","                        Total time: 2706.38s\n","                               ETA: 625.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 813/1000 \u001b[0m                      \n","\n","                       Computation: 60716 steps/s (collection: 1.856s, learning 1.382s)\n","               Value function loss: 0.0105\n","                    Surrogate loss: 0.0122\n","             Mean action noise std: 0.06\n","                       Mean reward: 78.41\n","               Mean episode length: 793.37\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 160038912\n","                    Iteration time: 3.24s\n","                        Total time: 2709.62s\n","                               ETA: 622.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 814/1000 \u001b[0m                      \n","\n","                       Computation: 61318 steps/s (collection: 1.832s, learning 1.374s)\n","               Value function loss: 0.0086\n","                    Surrogate loss: 0.0117\n","             Mean action noise std: 0.06\n","                       Mean reward: 76.53\n","               Mean episode length: 778.96\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 160235520\n","                    Iteration time: 3.21s\n","                        Total time: 2712.83s\n","                               ETA: 619.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 815/1000 \u001b[0m                      \n","\n","                       Computation: 61215 steps/s (collection: 1.838s, learning 1.374s)\n","               Value function loss: 0.0072\n","                    Surrogate loss: 0.0127\n","             Mean action noise std: 0.06\n","                       Mean reward: 89.85\n","               Mean episode length: 870.14\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 160432128\n","                    Iteration time: 3.21s\n","                        Total time: 2716.04s\n","                               ETA: 615.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 816/1000 \u001b[0m                      \n","\n","                       Computation: 58906 steps/s (collection: 1.831s, learning 1.507s)\n","               Value function loss: 0.0065\n","                    Surrogate loss: 0.0143\n","             Mean action noise std: 0.06\n","                       Mean reward: 93.88\n","               Mean episode length: 902.53\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 160628736\n","                    Iteration time: 3.34s\n","                        Total time: 2719.37s\n","                               ETA: 612.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 817/1000 \u001b[0m                      \n","\n","                       Computation: 60880 steps/s (collection: 1.854s, learning 1.376s)\n","               Value function loss: 0.0058\n","                    Surrogate loss: 0.0116\n","             Mean action noise std: 0.06\n","                       Mean reward: 96.53\n","               Mean episode length: 906.84\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 160825344\n","                    Iteration time: 3.23s\n","                        Total time: 2722.60s\n","                               ETA: 609.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 818/1000 \u001b[0m                      \n","\n","                       Computation: 61242 steps/s (collection: 1.832s, learning 1.378s)\n","               Value function loss: 0.0076\n","                    Surrogate loss: 0.0236\n","             Mean action noise std: 0.06\n","                       Mean reward: 90.98\n","               Mean episode length: 885.30\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 161021952\n","                    Iteration time: 3.21s\n","                        Total time: 2725.81s\n","                               ETA: 605.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 819/1000 \u001b[0m                      \n","\n","                       Computation: 61252 steps/s (collection: 1.836s, learning 1.374s)\n","               Value function loss: 0.0067\n","                    Surrogate loss: 0.0165\n","             Mean action noise std: 0.06\n","                       Mean reward: 83.44\n","               Mean episode length: 832.08\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 161218560\n","                    Iteration time: 3.21s\n","                        Total time: 2729.02s\n","                               ETA: 602.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 820/1000 \u001b[0m                      \n","\n","                       Computation: 59022 steps/s (collection: 1.835s, learning 1.496s)\n","               Value function loss: 0.0051\n","                    Surrogate loss: 0.0131\n","             Mean action noise std: 0.06\n","                       Mean reward: 77.02\n","               Mean episode length: 790.01\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 161415168\n","                    Iteration time: 3.33s\n","                        Total time: 2732.36s\n","                               ETA: 599.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 821/1000 \u001b[0m                      \n","\n","                       Computation: 61077 steps/s (collection: 1.852s, learning 1.367s)\n","               Value function loss: 0.0056\n","                    Surrogate loss: 0.0126\n","             Mean action noise std: 0.06\n","                       Mean reward: 75.91\n","               Mean episode length: 781.18\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 161611776\n","                    Iteration time: 3.22s\n","                        Total time: 2735.57s\n","                               ETA: 595.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 822/1000 \u001b[0m                      \n","\n","                       Computation: 61375 steps/s (collection: 1.832s, learning 1.372s)\n","               Value function loss: 0.0055\n","                    Surrogate loss: 0.0191\n","             Mean action noise std: 0.06\n","                       Mean reward: 79.06\n","               Mean episode length: 834.17\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 161808384\n","                    Iteration time: 3.20s\n","                        Total time: 2738.78s\n","                               ETA: 592.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 823/1000 \u001b[0m                      \n","\n","                       Computation: 61098 steps/s (collection: 1.840s, learning 1.378s)\n","               Value function loss: 0.0041\n","                    Surrogate loss: 0.0169\n","             Mean action noise std: 0.06\n","                       Mean reward: 77.94\n","               Mean episode length: 800.34\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 162004992\n","                    Iteration time: 3.22s\n","                        Total time: 2742.00s\n","                               ETA: 589.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 824/1000 \u001b[0m                      \n","\n","                       Computation: 58491 steps/s (collection: 1.857s, learning 1.505s)\n","               Value function loss: 0.0041\n","                    Surrogate loss: 0.0132\n","             Mean action noise std: 0.06\n","                       Mean reward: 83.57\n","               Mean episode length: 808.71\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 162201600\n","                    Iteration time: 3.36s\n","                        Total time: 2745.36s\n","                               ETA: 585.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 825/1000 \u001b[0m                      \n","\n","                       Computation: 61026 steps/s (collection: 1.845s, learning 1.377s)\n","               Value function loss: 0.0034\n","                    Surrogate loss: 0.0109\n","             Mean action noise std: 0.06\n","                       Mean reward: 80.78\n","               Mean episode length: 794.26\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 162398208\n","                    Iteration time: 3.22s\n","                        Total time: 2748.58s\n","                               ETA: 582.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 826/1000 \u001b[0m                      \n","\n","                       Computation: 61063 steps/s (collection: 1.832s, learning 1.387s)\n","               Value function loss: 0.0038\n","                    Surrogate loss: 0.0199\n","             Mean action noise std: 0.06\n","                       Mean reward: 76.81\n","               Mean episode length: 805.02\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 162594816\n","                    Iteration time: 3.22s\n","                        Total time: 2751.80s\n","                               ETA: 579.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 827/1000 \u001b[0m                      \n","\n","                       Computation: 60876 steps/s (collection: 1.842s, learning 1.387s)\n","               Value function loss: 0.0023\n","                    Surrogate loss: 0.0162\n","             Mean action noise std: 0.06\n","                       Mean reward: 76.56\n","               Mean episode length: 802.39\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 162791424\n","                    Iteration time: 3.23s\n","                        Total time: 2755.03s\n","                               ETA: 575.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 828/1000 \u001b[0m                      \n","\n","                       Computation: 58793 steps/s (collection: 1.855s, learning 1.489s)\n","               Value function loss: 0.0032\n","                    Surrogate loss: 0.0122\n","             Mean action noise std: 0.06\n","                       Mean reward: 74.05\n","               Mean episode length: 783.60\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 162988032\n","                    Iteration time: 3.34s\n","                        Total time: 2758.37s\n","                               ETA: 572.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 829/1000 \u001b[0m                      \n","\n","                       Computation: 61271 steps/s (collection: 1.840s, learning 1.369s)\n","               Value function loss: 0.0050\n","                    Surrogate loss: 0.0182\n","             Mean action noise std: 0.06\n","                       Mean reward: 72.97\n","               Mean episode length: 744.36\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 163184640\n","                    Iteration time: 3.21s\n","                        Total time: 2761.58s\n","                               ETA: 569.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 830/1000 \u001b[0m                      \n","\n","                       Computation: 60911 steps/s (collection: 1.836s, learning 1.392s)\n","               Value function loss: 0.0039\n","                    Surrogate loss: 0.0189\n","             Mean action noise std: 0.06\n","                       Mean reward: 68.22\n","               Mean episode length: 699.67\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 163381248\n","                    Iteration time: 3.23s\n","                        Total time: 2764.81s\n","                               ETA: 565.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 831/1000 \u001b[0m                      \n","\n","                       Computation: 61125 steps/s (collection: 1.837s, learning 1.380s)\n","               Value function loss: 0.0041\n","                    Surrogate loss: 0.0215\n","             Mean action noise std: 0.05\n","                       Mean reward: 68.50\n","               Mean episode length: 735.49\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 163577856\n","                    Iteration time: 3.22s\n","                        Total time: 2768.02s\n","                               ETA: 562.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 832/1000 \u001b[0m                      \n","\n","                       Computation: 57941 steps/s (collection: 1.859s, learning 1.534s)\n","               Value function loss: 0.0042\n","                    Surrogate loss: 0.0157\n","             Mean action noise std: 0.05\n","                       Mean reward: 77.30\n","               Mean episode length: 822.44\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 163774464\n","                    Iteration time: 3.39s\n","                        Total time: 2771.42s\n","                               ETA: 558.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 833/1000 \u001b[0m                      \n","\n","                       Computation: 61432 steps/s (collection: 1.832s, learning 1.368s)\n","               Value function loss: 0.0065\n","                    Surrogate loss: 0.0157\n","             Mean action noise std: 0.05\n","                       Mean reward: 78.21\n","               Mean episode length: 814.31\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 163971072\n","                    Iteration time: 3.20s\n","                        Total time: 2774.62s\n","                               ETA: 555.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 834/1000 \u001b[0m                      \n","\n","                       Computation: 61340 steps/s (collection: 1.833s, learning 1.372s)\n","               Value function loss: 0.0078\n","                    Surrogate loss: 0.0176\n","             Mean action noise std: 0.05\n","                       Mean reward: 70.38\n","               Mean episode length: 729.60\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 164167680\n","                    Iteration time: 3.21s\n","                        Total time: 2777.82s\n","                               ETA: 552.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 835/1000 \u001b[0m                      \n","\n","                       Computation: 61348 steps/s (collection: 1.832s, learning 1.373s)\n","               Value function loss: 0.0038\n","                    Surrogate loss: 0.0215\n","             Mean action noise std: 0.05\n","                       Mean reward: 73.46\n","               Mean episode length: 771.48\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 164364288\n","                    Iteration time: 3.20s\n","                        Total time: 2781.03s\n","                               ETA: 548.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 836/1000 \u001b[0m                      \n","\n","                       Computation: 58839 steps/s (collection: 1.864s, learning 1.478s)\n","               Value function loss: 0.0036\n","                    Surrogate loss: 0.0105\n","             Mean action noise std: 0.05\n","                       Mean reward: 75.53\n","               Mean episode length: 863.15\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 164560896\n","                    Iteration time: 3.34s\n","                        Total time: 2784.37s\n","                               ETA: 545.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 837/1000 \u001b[0m                      \n","\n","                       Computation: 61535 steps/s (collection: 1.827s, learning 1.368s)\n","               Value function loss: 0.0051\n","                    Surrogate loss: 0.0225\n","             Mean action noise std: 0.05\n","                       Mean reward: 83.19\n","               Mean episode length: 972.06\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 164757504\n","                    Iteration time: 3.20s\n","                        Total time: 2787.56s\n","                               ETA: 542.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 838/1000 \u001b[0m                      \n","\n","                       Computation: 61344 steps/s (collection: 1.834s, learning 1.371s)\n","               Value function loss: 0.0126\n","                    Surrogate loss: 0.0332\n","             Mean action noise std: 0.05\n","                       Mean reward: 102.51\n","               Mean episode length: 979.45\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 164954112\n","                    Iteration time: 3.20s\n","                        Total time: 2790.77s\n","                               ETA: 538.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 839/1000 \u001b[0m                      \n","\n","                       Computation: 60927 steps/s (collection: 1.838s, learning 1.389s)\n","               Value function loss: 0.0199\n","                    Surrogate loss: 0.0408\n","             Mean action noise std: 0.05\n","                       Mean reward: 102.85\n","               Mean episode length: 970.94\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 165150720\n","                    Iteration time: 3.23s\n","                        Total time: 2794.00s\n","                               ETA: 535.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 840/1000 \u001b[0m                      \n","\n","                       Computation: 58770 steps/s (collection: 1.868s, learning 1.477s)\n","               Value function loss: 0.0111\n","                    Surrogate loss: 0.0385\n","             Mean action noise std: 0.05\n","                       Mean reward: 99.96\n","               Mean episode length: 913.71\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 165347328\n","                    Iteration time: 3.35s\n","                        Total time: 2797.34s\n","                               ETA: 532.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 841/1000 \u001b[0m                      \n","\n","                       Computation: 61401 steps/s (collection: 1.833s, learning 1.369s)\n","               Value function loss: 0.0086\n","                    Surrogate loss: 0.0226\n","             Mean action noise std: 0.05\n","                       Mean reward: 103.87\n","               Mean episode length: 924.16\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 165543936\n","                    Iteration time: 3.20s\n","                        Total time: 2800.54s\n","                               ETA: 528.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 842/1000 \u001b[0m                      \n","\n","                       Computation: 61414 steps/s (collection: 1.832s, learning 1.369s)\n","               Value function loss: 0.0074\n","                    Surrogate loss: 0.0262\n","             Mean action noise std: 0.05\n","                       Mean reward: 105.36\n","               Mean episode length: 923.80\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 165740544\n","                    Iteration time: 3.20s\n","                        Total time: 2803.75s\n","                               ETA: 525.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 843/1000 \u001b[0m                      \n","\n","                       Computation: 60579 steps/s (collection: 1.835s, learning 1.411s)\n","               Value function loss: 0.0061\n","                    Surrogate loss: 0.0375\n","             Mean action noise std: 0.05\n","                       Mean reward: 105.81\n","               Mean episode length: 924.19\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 165937152\n","                    Iteration time: 3.25s\n","                        Total time: 2806.99s\n","                               ETA: 522.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 844/1000 \u001b[0m                      \n","\n","                       Computation: 59306 steps/s (collection: 1.866s, learning 1.449s)\n","               Value function loss: 0.0066\n","                    Surrogate loss: 0.0364\n","             Mean action noise std: 0.05\n","                       Mean reward: 108.34\n","               Mean episode length: 942.64\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 166133760\n","                    Iteration time: 3.32s\n","                        Total time: 2810.31s\n","                               ETA: 518.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 845/1000 \u001b[0m                      \n","\n","                       Computation: 61357 steps/s (collection: 1.840s, learning 1.364s)\n","               Value function loss: 0.0070\n","                    Surrogate loss: 0.0276\n","             Mean action noise std: 0.05\n","                       Mean reward: 108.54\n","               Mean episode length: 943.21\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 166330368\n","                    Iteration time: 3.20s\n","                        Total time: 2813.51s\n","                               ETA: 515.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 846/1000 \u001b[0m                      \n","\n","                       Computation: 61418 steps/s (collection: 1.831s, learning 1.370s)\n","               Value function loss: 0.0054\n","                    Surrogate loss: 0.0215\n","             Mean action noise std: 0.05\n","                       Mean reward: 102.23\n","               Mean episode length: 884.45\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 166526976\n","                    Iteration time: 3.20s\n","                        Total time: 2816.71s\n","                               ETA: 512.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 847/1000 \u001b[0m                      \n","\n","                       Computation: 60402 steps/s (collection: 1.835s, learning 1.420s)\n","               Value function loss: 0.0070\n","                    Surrogate loss: 0.0209\n","             Mean action noise std: 0.05\n","                       Mean reward: 99.13\n","               Mean episode length: 860.77\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 166723584\n","                    Iteration time: 3.25s\n","                        Total time: 2819.97s\n","                               ETA: 508.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 848/1000 \u001b[0m                      \n","\n","                       Computation: 59162 steps/s (collection: 1.867s, learning 1.456s)\n","               Value function loss: 0.0080\n","                    Surrogate loss: 0.0289\n","             Mean action noise std: 0.05\n","                       Mean reward: 94.17\n","               Mean episode length: 826.91\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 166920192\n","                    Iteration time: 3.32s\n","                        Total time: 2823.29s\n","                               ETA: 505.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 849/1000 \u001b[0m                      \n","\n","                       Computation: 61184 steps/s (collection: 1.844s, learning 1.369s)\n","               Value function loss: 0.0096\n","                    Surrogate loss: 0.0274\n","             Mean action noise std: 0.05\n","                       Mean reward: 95.95\n","               Mean episode length: 831.36\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 167116800\n","                    Iteration time: 3.21s\n","                        Total time: 2826.50s\n","                               ETA: 502.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 850/1000 \u001b[0m                      \n","\n","                       Computation: 61520 steps/s (collection: 1.829s, learning 1.367s)\n","               Value function loss: 0.0220\n","                    Surrogate loss: 0.0785\n","             Mean action noise std: 0.06\n","                       Mean reward: 93.77\n","               Mean episode length: 799.54\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 167313408\n","                    Iteration time: 3.20s\n","                        Total time: 2829.70s\n","                               ETA: 498.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 851/1000 \u001b[0m                      \n","\n","                       Computation: 60494 steps/s (collection: 1.831s, learning 1.419s)\n","               Value function loss: 1.4262\n","                    Surrogate loss: 0.0452\n","             Mean action noise std: 0.06\n","                       Mean reward: 72.45\n","               Mean episode length: 635.83\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 167510016\n","                    Iteration time: 3.25s\n","                        Total time: 2832.95s\n","                               ETA: 495.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 852/1000 \u001b[0m                      \n","\n","                       Computation: 60479 steps/s (collection: 1.869s, learning 1.382s)\n","               Value function loss: 0.2422\n","                    Surrogate loss: 0.1540\n","             Mean action noise std: 0.06\n","                       Mean reward: 41.02\n","               Mean episode length: 370.24\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 167706624\n","                    Iteration time: 3.25s\n","                        Total time: 2836.20s\n","                               ETA: 492.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 853/1000 \u001b[0m                      \n","\n","                       Computation: 61572 steps/s (collection: 1.832s, learning 1.361s)\n","               Value function loss: 0.4390\n","                    Surrogate loss: 0.0753\n","             Mean action noise std: 0.06\n","                       Mean reward: 25.42\n","               Mean episode length: 252.37\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 167903232\n","                    Iteration time: 3.19s\n","                        Total time: 2839.39s\n","                               ETA: 488.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 854/1000 \u001b[0m                      \n","\n","                       Computation: 61356 steps/s (collection: 1.832s, learning 1.373s)\n","               Value function loss: 0.2300\n","                    Surrogate loss: 0.0853\n","             Mean action noise std: 0.06\n","                       Mean reward: 7.46\n","               Mean episode length: 98.48\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 168099840\n","                    Iteration time: 3.20s\n","                        Total time: 2842.60s\n","                               ETA: 485.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 855/1000 \u001b[0m                      \n","\n","                       Computation: 59481 steps/s (collection: 1.836s, learning 1.469s)\n","               Value function loss: 0.1932\n","                    Surrogate loss: 0.0703\n","             Mean action noise std: 0.06\n","                       Mean reward: 4.33\n","               Mean episode length: 77.73\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 168296448\n","                    Iteration time: 3.31s\n","                        Total time: 2845.90s\n","                               ETA: 482.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 856/1000 \u001b[0m                      \n","\n","                       Computation: 60897 steps/s (collection: 1.864s, learning 1.364s)\n","               Value function loss: 0.1369\n","                    Surrogate loss: 0.0618\n","             Mean action noise std: 0.06\n","                       Mean reward: 4.98\n","               Mean episode length: 86.37\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 168493056\n","                    Iteration time: 3.23s\n","                        Total time: 2849.13s\n","                               ETA: 478.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 857/1000 \u001b[0m                      \n","\n","                       Computation: 61376 steps/s (collection: 1.832s, learning 1.371s)\n","               Value function loss: 0.1087\n","                    Surrogate loss: 0.0668\n","             Mean action noise std: 0.06\n","                       Mean reward: 4.68\n","               Mean episode length: 87.73\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 168689664\n","                    Iteration time: 3.20s\n","                        Total time: 2852.33s\n","                               ETA: 475.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 858/1000 \u001b[0m                      \n","\n","                       Computation: 60797 steps/s (collection: 1.830s, learning 1.404s)\n","               Value function loss: 0.0819\n","                    Surrogate loss: 0.0574\n","             Mean action noise std: 0.06\n","                       Mean reward: 2.54\n","               Mean episode length: 66.04\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 168886272\n","                    Iteration time: 3.23s\n","                        Total time: 2855.57s\n","                               ETA: 472.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 859/1000 \u001b[0m                      \n","\n","                       Computation: 59056 steps/s (collection: 1.836s, learning 1.493s)\n","               Value function loss: 0.0713\n","                    Surrogate loss: 0.0638\n","             Mean action noise std: 0.06\n","                       Mean reward: 3.96\n","               Mean episode length: 80.36\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 169082880\n","                    Iteration time: 3.33s\n","                        Total time: 2858.90s\n","                               ETA: 468.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 860/1000 \u001b[0m                      \n","\n","                       Computation: 61124 steps/s (collection: 1.854s, learning 1.363s)\n","               Value function loss: 0.0645\n","                    Surrogate loss: 0.0488\n","             Mean action noise std: 0.06\n","                       Mean reward: 3.23\n","               Mean episode length: 73.86\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 169279488\n","                    Iteration time: 3.22s\n","                        Total time: 2862.11s\n","                               ETA: 465.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 861/1000 \u001b[0m                      \n","\n","                       Computation: 61562 steps/s (collection: 1.832s, learning 1.361s)\n","               Value function loss: 0.0599\n","                    Surrogate loss: 0.0375\n","             Mean action noise std: 0.06\n","                       Mean reward: 3.10\n","               Mean episode length: 75.24\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 169476096\n","                    Iteration time: 3.19s\n","                        Total time: 2865.31s\n","                               ETA: 462.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 862/1000 \u001b[0m                      \n","\n","                       Computation: 61337 steps/s (collection: 1.838s, learning 1.367s)\n","               Value function loss: 0.0534\n","                    Surrogate loss: 0.0384\n","             Mean action noise std: 0.06\n","                       Mean reward: 3.18\n","               Mean episode length: 75.12\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 169672704\n","                    Iteration time: 3.21s\n","                        Total time: 2868.51s\n","                               ETA: 458.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 863/1000 \u001b[0m                      \n","\n","                       Computation: 59240 steps/s (collection: 1.837s, learning 1.482s)\n","               Value function loss: 0.0527\n","                    Surrogate loss: 0.0412\n","             Mean action noise std: 0.06\n","                       Mean reward: 3.54\n","               Mean episode length: 76.64\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 169869312\n","                    Iteration time: 3.32s\n","                        Total time: 2871.83s\n","                               ETA: 455.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 864/1000 \u001b[0m                      \n","\n","                       Computation: 61193 steps/s (collection: 1.851s, learning 1.362s)\n","               Value function loss: 0.0491\n","                    Surrogate loss: 0.0399\n","             Mean action noise std: 0.06\n","                       Mean reward: 3.69\n","               Mean episode length: 83.07\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 170065920\n","                    Iteration time: 3.21s\n","                        Total time: 2875.04s\n","                               ETA: 452.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 865/1000 \u001b[0m                      \n","\n","                       Computation: 61373 steps/s (collection: 1.835s, learning 1.369s)\n","               Value function loss: 0.0512\n","                    Surrogate loss: 0.0347\n","             Mean action noise std: 0.06\n","                       Mean reward: 4.33\n","               Mean episode length: 88.28\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 170262528\n","                    Iteration time: 3.20s\n","                        Total time: 2878.25s\n","                               ETA: 448.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 866/1000 \u001b[0m                      \n","\n","                       Computation: 61402 steps/s (collection: 1.833s, learning 1.369s)\n","               Value function loss: 0.0488\n","                    Surrogate loss: 0.0300\n","             Mean action noise std: 0.06\n","                       Mean reward: 4.26\n","               Mean episode length: 81.98\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 170459136\n","                    Iteration time: 3.20s\n","                        Total time: 2881.45s\n","                               ETA: 445.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 867/1000 \u001b[0m                      \n","\n","                       Computation: 58450 steps/s (collection: 1.845s, learning 1.519s)\n","               Value function loss: 0.0479\n","                    Surrogate loss: 0.0421\n","             Mean action noise std: 0.06\n","                       Mean reward: 3.42\n","               Mean episode length: 71.28\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 170655744\n","                    Iteration time: 3.36s\n","                        Total time: 2884.81s\n","                               ETA: 442.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 868/1000 \u001b[0m                      \n","\n","                       Computation: 61395 steps/s (collection: 1.845s, learning 1.357s)\n","               Value function loss: 0.0450\n","                    Surrogate loss: 0.0368\n","             Mean action noise std: 0.06\n","                       Mean reward: 3.78\n","               Mean episode length: 77.27\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 170852352\n","                    Iteration time: 3.20s\n","                        Total time: 2888.02s\n","                               ETA: 438.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 869/1000 \u001b[0m                      \n","\n","                       Computation: 61412 steps/s (collection: 1.839s, learning 1.362s)\n","               Value function loss: 0.0444\n","                    Surrogate loss: 0.0238\n","             Mean action noise std: 0.06\n","                       Mean reward: 4.98\n","               Mean episode length: 93.73\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 171048960\n","                    Iteration time: 3.20s\n","                        Total time: 2891.22s\n","                               ETA: 435.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 870/1000 \u001b[0m                      \n","\n","                       Computation: 61439 steps/s (collection: 1.837s, learning 1.363s)\n","               Value function loss: 0.0423\n","                    Surrogate loss: 0.0234\n","             Mean action noise std: 0.06\n","                       Mean reward: 5.93\n","               Mean episode length: 108.42\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 171245568\n","                    Iteration time: 3.20s\n","                        Total time: 2894.42s\n","                               ETA: 432.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 871/1000 \u001b[0m                      \n","\n","                       Computation: 59334 steps/s (collection: 1.846s, learning 1.467s)\n","               Value function loss: 0.0412\n","                    Surrogate loss: 0.0285\n","             Mean action noise std: 0.06\n","                       Mean reward: 5.37\n","               Mean episode length: 96.65\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 171442176\n","                    Iteration time: 3.31s\n","                        Total time: 2897.73s\n","                               ETA: 428.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 872/1000 \u001b[0m                      \n","\n","                       Computation: 61164 steps/s (collection: 1.851s, learning 1.363s)\n","               Value function loss: 0.0409\n","                    Surrogate loss: 0.0245\n","             Mean action noise std: 0.06\n","                       Mean reward: 4.92\n","               Mean episode length: 85.82\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 171638784\n","                    Iteration time: 3.21s\n","                        Total time: 2900.95s\n","                               ETA: 425.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 873/1000 \u001b[0m                      \n","\n","                       Computation: 61584 steps/s (collection: 1.831s, learning 1.362s)\n","               Value function loss: 0.0394\n","                    Surrogate loss: 0.0178\n","             Mean action noise std: 0.06\n","                       Mean reward: 6.69\n","               Mean episode length: 106.99\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 171835392\n","                    Iteration time: 3.19s\n","                        Total time: 2904.14s\n","                               ETA: 422.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 874/1000 \u001b[0m                      \n","\n","                       Computation: 61343 steps/s (collection: 1.839s, learning 1.366s)\n","               Value function loss: 0.0393\n","                    Surrogate loss: 0.0227\n","             Mean action noise std: 0.06\n","                       Mean reward: 6.54\n","               Mean episode length: 110.97\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 172032000\n","                    Iteration time: 3.21s\n","                        Total time: 2907.34s\n","                               ETA: 418.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 875/1000 \u001b[0m                      \n","\n","                       Computation: 58922 steps/s (collection: 1.851s, learning 1.486s)\n","               Value function loss: 0.0384\n","                    Surrogate loss: 0.0239\n","             Mean action noise std: 0.06\n","                       Mean reward: 6.21\n","               Mean episode length: 121.99\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 172228608\n","                    Iteration time: 3.34s\n","                        Total time: 2910.68s\n","                               ETA: 415.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 876/1000 \u001b[0m                      \n","\n","                       Computation: 61443 steps/s (collection: 1.848s, learning 1.352s)\n","               Value function loss: 0.0374\n","                    Surrogate loss: 0.0179\n","             Mean action noise std: 0.06\n","                       Mean reward: 6.31\n","               Mean episode length: 108.08\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 172425216\n","                    Iteration time: 3.20s\n","                        Total time: 2913.88s\n","                               ETA: 412.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 877/1000 \u001b[0m                      \n","\n","                       Computation: 61561 steps/s (collection: 1.835s, learning 1.358s)\n","               Value function loss: 0.0335\n","                    Surrogate loss: 0.0200\n","             Mean action noise std: 0.06\n","                       Mean reward: 5.33\n","               Mean episode length: 94.82\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 172621824\n","                    Iteration time: 3.19s\n","                        Total time: 2917.07s\n","                               ETA: 408.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 878/1000 \u001b[0m                      \n","\n","                       Computation: 61585 steps/s (collection: 1.831s, learning 1.361s)\n","               Value function loss: 0.0329\n","                    Surrogate loss: 0.0199\n","             Mean action noise std: 0.06\n","                       Mean reward: 6.28\n","               Mean episode length: 113.18\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 172818432\n","                    Iteration time: 3.19s\n","                        Total time: 2920.27s\n","                               ETA: 405.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 879/1000 \u001b[0m                      \n","\n","                       Computation: 58289 steps/s (collection: 1.865s, learning 1.508s)\n","               Value function loss: 0.0327\n","                    Surrogate loss: 0.0186\n","             Mean action noise std: 0.06\n","                       Mean reward: 8.06\n","               Mean episode length: 219.61\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 173015040\n","                    Iteration time: 3.37s\n","                        Total time: 2923.64s\n","                               ETA: 402.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 880/1000 \u001b[0m                      \n","\n","                       Computation: 61879 steps/s (collection: 1.831s, learning 1.346s)\n","               Value function loss: 0.0307\n","                    Surrogate loss: 0.0223\n","             Mean action noise std: 0.06\n","                       Mean reward: 8.14\n","               Mean episode length: 290.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 173211648\n","                    Iteration time: 3.18s\n","                        Total time: 2926.82s\n","                               ETA: 398.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 881/1000 \u001b[0m                      \n","\n","                       Computation: 61652 steps/s (collection: 1.831s, learning 1.358s)\n","               Value function loss: 0.0276\n","                    Surrogate loss: 0.0208\n","             Mean action noise std: 0.06\n","                       Mean reward: 7.31\n","               Mean episode length: 273.66\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 173408256\n","                    Iteration time: 3.19s\n","                        Total time: 2930.00s\n","                               ETA: 395.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 882/1000 \u001b[0m                      \n","\n","                       Computation: 61413 steps/s (collection: 1.833s, learning 1.369s)\n","               Value function loss: 0.0266\n","                    Surrogate loss: 0.0241\n","             Mean action noise std: 0.06\n","                       Mean reward: 5.76\n","               Mean episode length: 319.06\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 173604864\n","                    Iteration time: 3.20s\n","                        Total time: 2933.21s\n","                               ETA: 392.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 883/1000 \u001b[0m                      \n","\n","                       Computation: 58565 steps/s (collection: 1.865s, learning 1.492s)\n","               Value function loss: 0.0252\n","                    Surrogate loss: 0.0272\n","             Mean action noise std: 0.06\n","                       Mean reward: 7.04\n","               Mean episode length: 284.90\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 173801472\n","                    Iteration time: 3.36s\n","                        Total time: 2936.56s\n","                               ETA: 388.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 884/1000 \u001b[0m                      \n","\n","                       Computation: 61558 steps/s (collection: 1.831s, learning 1.363s)\n","               Value function loss: 0.0238\n","                    Surrogate loss: 0.0289\n","             Mean action noise std: 0.06\n","                       Mean reward: 8.00\n","               Mean episode length: 222.72\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 173998080\n","                    Iteration time: 3.19s\n","                        Total time: 2939.76s\n","                               ETA: 385.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 885/1000 \u001b[0m                      \n","\n","                       Computation: 61053 steps/s (collection: 1.840s, learning 1.381s)\n","               Value function loss: 0.0192\n","                    Surrogate loss: 0.0155\n","             Mean action noise std: 0.06\n","                       Mean reward: 7.78\n","               Mean episode length: 208.49\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 174194688\n","                    Iteration time: 3.22s\n","                        Total time: 2942.98s\n","                               ETA: 382.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 886/1000 \u001b[0m                      \n","\n","                       Computation: 61074 steps/s (collection: 1.855s, learning 1.364s)\n","               Value function loss: 0.0186\n","                    Surrogate loss: 0.0164\n","             Mean action noise std: 0.06\n","                       Mean reward: 7.01\n","               Mean episode length: 216.27\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 174391296\n","                    Iteration time: 3.22s\n","                        Total time: 2946.20s\n","                               ETA: 378.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 887/1000 \u001b[0m                      \n","\n","                       Computation: 58798 steps/s (collection: 1.877s, learning 1.466s)\n","               Value function loss: 0.0172\n","                    Surrogate loss: 0.0204\n","             Mean action noise std: 0.06\n","                       Mean reward: 8.58\n","               Mean episode length: 215.78\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 174587904\n","                    Iteration time: 3.34s\n","                        Total time: 2949.54s\n","                               ETA: 375.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 888/1000 \u001b[0m                      \n","\n","                       Computation: 61709 steps/s (collection: 1.830s, learning 1.356s)\n","               Value function loss: 0.0156\n","                    Surrogate loss: 0.0215\n","             Mean action noise std: 0.06\n","                       Mean reward: 7.09\n","               Mean episode length: 172.52\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 174784512\n","                    Iteration time: 3.19s\n","                        Total time: 2952.73s\n","                               ETA: 372.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 889/1000 \u001b[0m                      \n","\n","                       Computation: 61710 steps/s (collection: 1.830s, learning 1.356s)\n","               Value function loss: 0.0136\n","                    Surrogate loss: 0.0160\n","             Mean action noise std: 0.06\n","                       Mean reward: 7.41\n","               Mean episode length: 187.82\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 174981120\n","                    Iteration time: 3.19s\n","                        Total time: 2955.91s\n","                               ETA: 368.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 890/1000 \u001b[0m                      \n","\n","                       Computation: 61348 steps/s (collection: 1.838s, learning 1.366s)\n","               Value function loss: 0.0120\n","                    Surrogate loss: 0.0192\n","             Mean action noise std: 0.06\n","                       Mean reward: 6.81\n","               Mean episode length: 193.79\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 175177728\n","                    Iteration time: 3.20s\n","                        Total time: 2959.12s\n","                               ETA: 365.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 891/1000 \u001b[0m                      \n","\n","                       Computation: 58818 steps/s (collection: 1.871s, learning 1.471s)\n","               Value function loss: 0.0109\n","                    Surrogate loss: 0.0186\n","             Mean action noise std: 0.06\n","                       Mean reward: 7.52\n","               Mean episode length: 266.43\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 175374336\n","                    Iteration time: 3.34s\n","                        Total time: 2962.46s\n","                               ETA: 362.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 892/1000 \u001b[0m                      \n","\n","                       Computation: 61560 steps/s (collection: 1.827s, learning 1.367s)\n","               Value function loss: 0.0100\n","                    Surrogate loss: 0.0195\n","             Mean action noise std: 0.06\n","                       Mean reward: 6.56\n","               Mean episode length: 250.66\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 175570944\n","                    Iteration time: 3.19s\n","                        Total time: 2965.65s\n","                               ETA: 358.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 893/1000 \u001b[0m                      \n","\n","                       Computation: 61486 steps/s (collection: 1.833s, learning 1.365s)\n","               Value function loss: 0.0105\n","                    Surrogate loss: 0.0185\n","             Mean action noise std: 0.06\n","                       Mean reward: 5.25\n","               Mean episode length: 556.12\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 175767552\n","                    Iteration time: 3.20s\n","                        Total time: 2968.85s\n","                               ETA: 355.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 894/1000 \u001b[0m                      \n","\n","                       Computation: 60578 steps/s (collection: 1.837s, learning 1.409s)\n","               Value function loss: 0.0129\n","                    Surrogate loss: 0.0228\n","             Mean action noise std: 0.06\n","                       Mean reward: 3.55\n","               Mean episode length: 822.71\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 175964160\n","                    Iteration time: 3.25s\n","                        Total time: 2972.10s\n","                               ETA: 352.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 895/1000 \u001b[0m                      \n","\n","                       Computation: 59128 steps/s (collection: 1.874s, learning 1.451s)\n","               Value function loss: 0.0092\n","                    Surrogate loss: 0.0183\n","             Mean action noise std: 0.06\n","                       Mean reward: 2.88\n","               Mean episode length: 553.65\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 176160768\n","                    Iteration time: 3.33s\n","                        Total time: 2975.42s\n","                               ETA: 348.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 896/1000 \u001b[0m                      \n","\n","                       Computation: 61536 steps/s (collection: 1.836s, learning 1.359s)\n","               Value function loss: 0.0099\n","                    Surrogate loss: 0.0190\n","             Mean action noise std: 0.06\n","                       Mean reward: 2.35\n","               Mean episode length: 661.55\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 176357376\n","                    Iteration time: 3.19s\n","                        Total time: 2978.62s\n","                               ETA: 345.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 897/1000 \u001b[0m                      \n","\n","                       Computation: 61515 steps/s (collection: 1.838s, learning 1.358s)\n","               Value function loss: 0.0079\n","                    Surrogate loss: 0.0199\n","             Mean action noise std: 0.06\n","                       Mean reward: 4.72\n","               Mean episode length: 580.11\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 176553984\n","                    Iteration time: 3.20s\n","                        Total time: 2981.81s\n","                               ETA: 342.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 898/1000 \u001b[0m                      \n","\n","                       Computation: 60398 steps/s (collection: 1.833s, learning 1.422s)\n","               Value function loss: 0.0074\n","                    Surrogate loss: 0.0191\n","             Mean action noise std: 0.06\n","                       Mean reward: 2.67\n","               Mean episode length: 535.98\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 176750592\n","                    Iteration time: 3.26s\n","                        Total time: 2985.07s\n","                               ETA: 338.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 899/1000 \u001b[0m                      \n","\n","                       Computation: 60080 steps/s (collection: 1.871s, learning 1.402s)\n","               Value function loss: 0.0083\n","                    Surrogate loss: 0.0162\n","             Mean action noise std: 0.06\n","                       Mean reward: 7.83\n","               Mean episode length: 619.34\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 176947200\n","                    Iteration time: 3.27s\n","                        Total time: 2988.34s\n","                               ETA: 335.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 900/1000 \u001b[0m                      \n","\n","                       Computation: 61602 steps/s (collection: 1.832s, learning 1.360s)\n","               Value function loss: 0.0094\n","                    Surrogate loss: 0.0196\n","             Mean action noise std: 0.06\n","                       Mean reward: 3.85\n","               Mean episode length: 545.51\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 177143808\n","                    Iteration time: 3.19s\n","                        Total time: 2991.53s\n","                               ETA: 332.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 901/1000 \u001b[0m                      \n","\n","                       Computation: 61542 steps/s (collection: 1.832s, learning 1.363s)\n","               Value function loss: 0.0073\n","                    Surrogate loss: 0.0295\n","             Mean action noise std: 0.06\n","                       Mean reward: 7.47\n","               Mean episode length: 619.44\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 177340416\n","                    Iteration time: 3.19s\n","                        Total time: 2994.73s\n","                               ETA: 328.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 902/1000 \u001b[0m                      \n","\n","                       Computation: 60109 steps/s (collection: 1.831s, learning 1.440s)\n","               Value function loss: 0.0077\n","                    Surrogate loss: 0.0181\n","             Mean action noise std: 0.06\n","                       Mean reward: 10.85\n","               Mean episode length: 640.01\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 177537024\n","                    Iteration time: 3.27s\n","                        Total time: 2998.00s\n","                               ETA: 325.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 903/1000 \u001b[0m                      \n","\n","                       Computation: 60773 steps/s (collection: 1.874s, learning 1.361s)\n","               Value function loss: 0.0085\n","                    Surrogate loss: 0.0190\n","             Mean action noise std: 0.06\n","                       Mean reward: 16.48\n","               Mean episode length: 717.69\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 177733632\n","                    Iteration time: 3.24s\n","                        Total time: 3001.23s\n","                               ETA: 322.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 904/1000 \u001b[0m                      \n","\n","                       Computation: 61350 steps/s (collection: 1.843s, learning 1.362s)\n","               Value function loss: 0.0078\n","                    Surrogate loss: 0.0157\n","             Mean action noise std: 0.06\n","                       Mean reward: 24.46\n","               Mean episode length: 679.55\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 177930240\n","                    Iteration time: 3.20s\n","                        Total time: 3004.44s\n","                               ETA: 318.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 905/1000 \u001b[0m                      \n","\n","                       Computation: 61670 steps/s (collection: 1.832s, learning 1.356s)\n","               Value function loss: 0.0074\n","                    Surrogate loss: 0.0173\n","             Mean action noise std: 0.06\n","                       Mean reward: 24.45\n","               Mean episode length: 730.31\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 178126848\n","                    Iteration time: 3.19s\n","                        Total time: 3007.62s\n","                               ETA: 315.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 906/1000 \u001b[0m                      \n","\n","                       Computation: 59516 steps/s (collection: 1.834s, learning 1.470s)\n","               Value function loss: 0.0078\n","                    Surrogate loss: 0.0196\n","             Mean action noise std: 0.06\n","                       Mean reward: 21.97\n","               Mean episode length: 653.35\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 178323456\n","                    Iteration time: 3.30s\n","                        Total time: 3010.93s\n","                               ETA: 312.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 907/1000 \u001b[0m                      \n","\n","                       Computation: 60776 steps/s (collection: 1.867s, learning 1.368s)\n","               Value function loss: 0.0082\n","                    Surrogate loss: 0.0199\n","             Mean action noise std: 0.06\n","                       Mean reward: 35.75\n","               Mean episode length: 705.42\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 178520064\n","                    Iteration time: 3.23s\n","                        Total time: 3014.16s\n","                               ETA: 308.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 908/1000 \u001b[0m                      \n","\n","                       Computation: 61576 steps/s (collection: 1.840s, learning 1.353s)\n","               Value function loss: 0.0087\n","                    Surrogate loss: 0.0219\n","             Mean action noise std: 0.06\n","                       Mean reward: 34.06\n","               Mean episode length: 723.40\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 178716672\n","                    Iteration time: 3.19s\n","                        Total time: 3017.36s\n","                               ETA: 305.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 909/1000 \u001b[0m                      \n","\n","                       Computation: 61585 steps/s (collection: 1.833s, learning 1.360s)\n","               Value function loss: 0.0075\n","                    Surrogate loss: 0.0184\n","             Mean action noise std: 0.06\n","                       Mean reward: 35.00\n","               Mean episode length: 705.85\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 178913280\n","                    Iteration time: 3.19s\n","                        Total time: 3020.55s\n","                               ETA: 302.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 910/1000 \u001b[0m                      \n","\n","                       Computation: 58857 steps/s (collection: 1.831s, learning 1.510s)\n","               Value function loss: 0.0079\n","                    Surrogate loss: 0.0220\n","             Mean action noise std: 0.06\n","                       Mean reward: 36.19\n","               Mean episode length: 763.10\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 179109888\n","                    Iteration time: 3.34s\n","                        Total time: 3023.89s\n","                               ETA: 298.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 911/1000 \u001b[0m                      \n","\n","                       Computation: 60978 steps/s (collection: 1.865s, learning 1.360s)\n","               Value function loss: 0.0085\n","                    Surrogate loss: 0.0192\n","             Mean action noise std: 0.06\n","                       Mean reward: 44.99\n","               Mean episode length: 740.54\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 179306496\n","                    Iteration time: 3.22s\n","                        Total time: 3027.11s\n","                               ETA: 295.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 912/1000 \u001b[0m                      \n","\n","                       Computation: 61652 steps/s (collection: 1.834s, learning 1.355s)\n","               Value function loss: 0.0079\n","                    Surrogate loss: 0.0202\n","             Mean action noise std: 0.06\n","                       Mean reward: 56.51\n","               Mean episode length: 764.79\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 179503104\n","                    Iteration time: 3.19s\n","                        Total time: 3030.30s\n","                               ETA: 292.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 913/1000 \u001b[0m                      \n","\n","                       Computation: 61791 steps/s (collection: 1.827s, learning 1.355s)\n","               Value function loss: 0.0082\n","                    Surrogate loss: 0.0230\n","             Mean action noise std: 0.06\n","                       Mean reward: 58.08\n","               Mean episode length: 766.78\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 179699712\n","                    Iteration time: 3.18s\n","                        Total time: 3033.48s\n","                               ETA: 288.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 914/1000 \u001b[0m                      \n","\n","                       Computation: 59130 steps/s (collection: 1.839s, learning 1.486s)\n","               Value function loss: 0.0086\n","                    Surrogate loss: 0.0268\n","             Mean action noise std: 0.06\n","                       Mean reward: 53.76\n","               Mean episode length: 677.60\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 179896320\n","                    Iteration time: 3.33s\n","                        Total time: 3036.81s\n","                               ETA: 285.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 915/1000 \u001b[0m                      \n","\n","                       Computation: 61072 steps/s (collection: 1.857s, learning 1.362s)\n","               Value function loss: 0.0083\n","                    Surrogate loss: 0.0203\n","             Mean action noise std: 0.06\n","                       Mean reward: 51.12\n","               Mean episode length: 710.78\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 180092928\n","                    Iteration time: 3.22s\n","                        Total time: 3040.03s\n","                               ETA: 282.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 916/1000 \u001b[0m                      \n","\n","                       Computation: 61558 steps/s (collection: 1.833s, learning 1.361s)\n","               Value function loss: 0.0068\n","                    Surrogate loss: 0.0161\n","             Mean action noise std: 0.06\n","                       Mean reward: 65.13\n","               Mean episode length: 764.82\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 180289536\n","                    Iteration time: 3.19s\n","                        Total time: 3043.22s\n","                               ETA: 278.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 917/1000 \u001b[0m                      \n","\n","                       Computation: 61536 steps/s (collection: 1.833s, learning 1.362s)\n","               Value function loss: 0.0078\n","                    Surrogate loss: 0.0228\n","             Mean action noise std: 0.06\n","                       Mean reward: 52.02\n","               Mean episode length: 693.82\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 180486144\n","                    Iteration time: 3.20s\n","                        Total time: 3046.42s\n","                               ETA: 275.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 918/1000 \u001b[0m                      \n","\n","                       Computation: 59201 steps/s (collection: 1.848s, learning 1.473s)\n","               Value function loss: 0.0063\n","                    Surrogate loss: 0.0184\n","             Mean action noise std: 0.06\n","                       Mean reward: 51.50\n","               Mean episode length: 598.12\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 180682752\n","                    Iteration time: 3.32s\n","                        Total time: 3049.74s\n","                               ETA: 272.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 919/1000 \u001b[0m                      \n","\n","                       Computation: 61175 steps/s (collection: 1.854s, learning 1.360s)\n","               Value function loss: 0.0055\n","                    Surrogate loss: 0.0173\n","             Mean action noise std: 0.06\n","                       Mean reward: 51.46\n","               Mean episode length: 656.99\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 180879360\n","                    Iteration time: 3.21s\n","                        Total time: 3052.95s\n","                               ETA: 268.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 920/1000 \u001b[0m                      \n","\n","                       Computation: 61377 steps/s (collection: 1.841s, learning 1.362s)\n","               Value function loss: 0.0064\n","                    Surrogate loss: 0.0233\n","             Mean action noise std: 0.06\n","                       Mean reward: 53.73\n","               Mean episode length: 668.04\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 181075968\n","                    Iteration time: 3.20s\n","                        Total time: 3056.16s\n","                               ETA: 265.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 921/1000 \u001b[0m                      \n","\n","                       Computation: 61389 steps/s (collection: 1.840s, learning 1.362s)\n","               Value function loss: 0.0059\n","                    Surrogate loss: 0.0228\n","             Mean action noise std: 0.06\n","                       Mean reward: 41.50\n","               Mean episode length: 731.34\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 181272576\n","                    Iteration time: 3.20s\n","                        Total time: 3059.36s\n","                               ETA: 262.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 922/1000 \u001b[0m                      \n","\n","                       Computation: 59102 steps/s (collection: 1.850s, learning 1.477s)\n","               Value function loss: 0.0056\n","                    Surrogate loss: 0.0184\n","             Mean action noise std: 0.06\n","                       Mean reward: 42.06\n","               Mean episode length: 722.16\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 181469184\n","                    Iteration time: 3.33s\n","                        Total time: 3062.68s\n","                               ETA: 258.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 923/1000 \u001b[0m                      \n","\n","                       Computation: 60982 steps/s (collection: 1.848s, learning 1.376s)\n","               Value function loss: 0.0070\n","                    Surrogate loss: 0.0226\n","             Mean action noise std: 0.06\n","                       Mean reward: 45.42\n","               Mean episode length: 738.05\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 181665792\n","                    Iteration time: 3.22s\n","                        Total time: 3065.91s\n","                               ETA: 255.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 924/1000 \u001b[0m                      \n","\n","                       Computation: 61209 steps/s (collection: 1.844s, learning 1.368s)\n","               Value function loss: 0.0058\n","                    Surrogate loss: 0.0217\n","             Mean action noise std: 0.06\n","                       Mean reward: 40.13\n","               Mean episode length: 593.51\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 181862400\n","                    Iteration time: 3.21s\n","                        Total time: 3069.12s\n","                               ETA: 252.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 925/1000 \u001b[0m                      \n","\n","                       Computation: 61263 steps/s (collection: 1.841s, learning 1.369s)\n","               Value function loss: 0.0050\n","                    Surrogate loss: 0.0157\n","             Mean action noise std: 0.06\n","                       Mean reward: 41.91\n","               Mean episode length: 583.35\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 182059008\n","                    Iteration time: 3.21s\n","                        Total time: 3072.33s\n","                               ETA: 248.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 926/1000 \u001b[0m                      \n","\n","                       Computation: 58581 steps/s (collection: 1.850s, learning 1.506s)\n","               Value function loss: 0.0047\n","                    Surrogate loss: 0.0211\n","             Mean action noise std: 0.06\n","                       Mean reward: 47.13\n","               Mean episode length: 640.15\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 182255616\n","                    Iteration time: 3.36s\n","                        Total time: 3075.69s\n","                               ETA: 245.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 927/1000 \u001b[0m                      \n","\n","                       Computation: 61380 steps/s (collection: 1.843s, learning 1.360s)\n","               Value function loss: 0.0040\n","                    Surrogate loss: 0.0185\n","             Mean action noise std: 0.06\n","                       Mean reward: 31.41\n","               Mean episode length: 464.43\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 182452224\n","                    Iteration time: 3.20s\n","                        Total time: 3078.89s\n","                               ETA: 242.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 928/1000 \u001b[0m                      \n","\n","                       Computation: 61569 steps/s (collection: 1.834s, learning 1.360s)\n","               Value function loss: 0.0044\n","                    Surrogate loss: 0.0144\n","             Mean action noise std: 0.06\n","                       Mean reward: 33.22\n","               Mean episode length: 459.31\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 182648832\n","                    Iteration time: 3.19s\n","                        Total time: 3082.08s\n","                               ETA: 238.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 929/1000 \u001b[0m                      \n","\n","                       Computation: 61566 steps/s (collection: 1.837s, learning 1.356s)\n","               Value function loss: 0.0036\n","                    Surrogate loss: 0.0164\n","             Mean action noise std: 0.06\n","                       Mean reward: 30.36\n","               Mean episode length: 412.72\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 182845440\n","                    Iteration time: 3.19s\n","                        Total time: 3085.28s\n","                               ETA: 235.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 930/1000 \u001b[0m                      \n","\n","                       Computation: 58449 steps/s (collection: 1.862s, learning 1.501s)\n","               Value function loss: 0.0030\n","                    Surrogate loss: 0.0108\n","             Mean action noise std: 0.06\n","                       Mean reward: 35.38\n","               Mean episode length: 449.39\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 183042048\n","                    Iteration time: 3.36s\n","                        Total time: 3088.64s\n","                               ETA: 232.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 931/1000 \u001b[0m                      \n","\n","                       Computation: 61565 steps/s (collection: 1.837s, learning 1.356s)\n","               Value function loss: 0.0029\n","                    Surrogate loss: 0.0175\n","             Mean action noise std: 0.06\n","                       Mean reward: 25.70\n","               Mean episode length: 405.11\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 183238656\n","                    Iteration time: 3.19s\n","                        Total time: 3091.83s\n","                               ETA: 228.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 932/1000 \u001b[0m                      \n","\n","                       Computation: 61593 steps/s (collection: 1.833s, learning 1.359s)\n","               Value function loss: 0.0028\n","                    Surrogate loss: 0.0145\n","             Mean action noise std: 0.06\n","                       Mean reward: 33.52\n","               Mean episode length: 466.95\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 183435264\n","                    Iteration time: 3.19s\n","                        Total time: 3095.02s\n","                               ETA: 225.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 933/1000 \u001b[0m                      \n","\n","                       Computation: 61523 steps/s (collection: 1.839s, learning 1.356s)\n","               Value function loss: 0.0030\n","                    Surrogate loss: 0.0170\n","             Mean action noise std: 0.06\n","                       Mean reward: 19.48\n","               Mean episode length: 290.68\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 183631872\n","                    Iteration time: 3.20s\n","                        Total time: 3098.22s\n","                               ETA: 222.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 934/1000 \u001b[0m                      \n","\n","                       Computation: 58638 steps/s (collection: 1.859s, learning 1.494s)\n","               Value function loss: 0.0017\n","                    Surrogate loss: 0.0260\n","             Mean action noise std: 0.06\n","                       Mean reward: 16.49\n","               Mean episode length: 336.70\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 183828480\n","                    Iteration time: 3.35s\n","                        Total time: 3101.57s\n","                               ETA: 218.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 935/1000 \u001b[0m                      \n","\n","                       Computation: 61326 steps/s (collection: 1.843s, learning 1.363s)\n","               Value function loss: 0.0034\n","                    Surrogate loss: 0.0151\n","             Mean action noise std: 0.06\n","                       Mean reward: 24.00\n","               Mean episode length: 837.25\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 184025088\n","                    Iteration time: 3.21s\n","                        Total time: 3104.78s\n","                               ETA: 215.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 936/1000 \u001b[0m                      \n","\n","                       Computation: 61436 steps/s (collection: 1.830s, learning 1.370s)\n","               Value function loss: 0.0042\n","                    Surrogate loss: 0.0141\n","             Mean action noise std: 0.06\n","                       Mean reward: 20.47\n","               Mean episode length: 654.15\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 184221696\n","                    Iteration time: 3.20s\n","                        Total time: 3107.98s\n","                               ETA: 212.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 937/1000 \u001b[0m                      \n","\n","                       Computation: 61503 steps/s (collection: 1.835s, learning 1.361s)\n","               Value function loss: 0.0030\n","                    Surrogate loss: 0.0201\n","             Mean action noise std: 0.06\n","                       Mean reward: 25.18\n","               Mean episode length: 750.77\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 184418304\n","                    Iteration time: 3.20s\n","                        Total time: 3111.18s\n","                               ETA: 209.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 938/1000 \u001b[0m                      \n","\n","                       Computation: 58592 steps/s (collection: 1.863s, learning 1.493s)\n","               Value function loss: 0.0045\n","                    Surrogate loss: 0.0212\n","             Mean action noise std: 0.06\n","                       Mean reward: 34.99\n","               Mean episode length: 721.53\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 184614912\n","                    Iteration time: 3.36s\n","                        Total time: 3114.53s\n","                               ETA: 205.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 939/1000 \u001b[0m                      \n","\n","                       Computation: 61513 steps/s (collection: 1.836s, learning 1.360s)\n","               Value function loss: 0.0043\n","                    Surrogate loss: 0.0173\n","             Mean action noise std: 0.06\n","                       Mean reward: 22.88\n","               Mean episode length: 626.25\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 184811520\n","                    Iteration time: 3.20s\n","                        Total time: 3117.73s\n","                               ETA: 202.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 940/1000 \u001b[0m                      \n","\n","                       Computation: 61656 steps/s (collection: 1.833s, learning 1.356s)\n","               Value function loss: 0.0026\n","                    Surrogate loss: 0.0149\n","             Mean action noise std: 0.06\n","                       Mean reward: 25.04\n","               Mean episode length: 616.90\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 185008128\n","                    Iteration time: 3.19s\n","                        Total time: 3120.92s\n","                               ETA: 199.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 941/1000 \u001b[0m                      \n","\n","                       Computation: 61405 steps/s (collection: 1.837s, learning 1.365s)\n","               Value function loss: 0.0027\n","                    Surrogate loss: 0.0124\n","             Mean action noise std: 0.05\n","                       Mean reward: 19.06\n","               Mean episode length: 731.85\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 185204736\n","                    Iteration time: 3.20s\n","                        Total time: 3124.12s\n","                               ETA: 195.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 942/1000 \u001b[0m                      \n","\n","                       Computation: 58709 steps/s (collection: 1.882s, learning 1.467s)\n","               Value function loss: 0.0024\n","                    Surrogate loss: 0.0164\n","             Mean action noise std: 0.05\n","                       Mean reward: 17.42\n","               Mean episode length: 626.16\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 185401344\n","                    Iteration time: 3.35s\n","                        Total time: 3127.47s\n","                               ETA: 192.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 943/1000 \u001b[0m                      \n","\n","                       Computation: 61452 steps/s (collection: 1.835s, learning 1.364s)\n","               Value function loss: 0.0031\n","                    Surrogate loss: 0.0164\n","             Mean action noise std: 0.05\n","                       Mean reward: 20.85\n","               Mean episode length: 618.32\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 185597952\n","                    Iteration time: 3.20s\n","                        Total time: 3130.67s\n","                               ETA: 189.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 944/1000 \u001b[0m                      \n","\n","                       Computation: 61563 steps/s (collection: 1.829s, learning 1.364s)\n","               Value function loss: 0.0058\n","                    Surrogate loss: 0.0215\n","             Mean action noise std: 0.05\n","                       Mean reward: 18.78\n","               Mean episode length: 704.90\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 185794560\n","                    Iteration time: 3.19s\n","                        Total time: 3133.86s\n","                               ETA: 185.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 945/1000 \u001b[0m                      \n","\n","                       Computation: 61034 steps/s (collection: 1.828s, learning 1.393s)\n","               Value function loss: 0.0040\n","                    Surrogate loss: 0.0174\n","             Mean action noise std: 0.05\n","                       Mean reward: 17.78\n","               Mean episode length: 742.36\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 185991168\n","                    Iteration time: 3.22s\n","                        Total time: 3137.08s\n","                               ETA: 182.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 946/1000 \u001b[0m                      \n","\n","                       Computation: 59418 steps/s (collection: 1.862s, learning 1.447s)\n","               Value function loss: 0.0073\n","                    Surrogate loss: 0.0275\n","             Mean action noise std: 0.05\n","                       Mean reward: 22.59\n","               Mean episode length: 751.54\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 186187776\n","                    Iteration time: 3.31s\n","                        Total time: 3140.39s\n","                               ETA: 179.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 947/1000 \u001b[0m                      \n","\n","                       Computation: 61618 steps/s (collection: 1.836s, learning 1.355s)\n","               Value function loss: 0.0075\n","                    Surrogate loss: 0.0293\n","             Mean action noise std: 0.05\n","                       Mean reward: 22.80\n","               Mean episode length: 827.80\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 186384384\n","                    Iteration time: 3.19s\n","                        Total time: 3143.58s\n","                               ETA: 175.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 948/1000 \u001b[0m                      \n","\n","                       Computation: 61718 steps/s (collection: 1.830s, learning 1.356s)\n","               Value function loss: 0.0108\n","                    Surrogate loss: 0.0196\n","             Mean action noise std: 0.05\n","                       Mean reward: 32.88\n","               Mean episode length: 747.89\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 186580992\n","                    Iteration time: 3.19s\n","                        Total time: 3146.77s\n","                               ETA: 172.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 949/1000 \u001b[0m                      \n","\n","                       Computation: 60574 steps/s (collection: 1.842s, learning 1.404s)\n","               Value function loss: 0.0087\n","                    Surrogate loss: 0.0325\n","             Mean action noise std: 0.06\n","                       Mean reward: 50.06\n","               Mean episode length: 790.14\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 186777600\n","                    Iteration time: 3.25s\n","                        Total time: 3150.01s\n","                               ETA: 169.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 950/1000 \u001b[0m                      \n","\n","                       Computation: 59747 steps/s (collection: 1.865s, learning 1.425s)\n","               Value function loss: 0.0079\n","                    Surrogate loss: 0.0289\n","             Mean action noise std: 0.06\n","                       Mean reward: 34.28\n","               Mean episode length: 673.38\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 186974208\n","                    Iteration time: 3.29s\n","                        Total time: 3153.30s\n","                               ETA: 165.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 951/1000 \u001b[0m                      \n","\n","                       Computation: 61145 steps/s (collection: 1.831s, learning 1.385s)\n","               Value function loss: 0.0052\n","                    Surrogate loss: 0.0242\n","             Mean action noise std: 0.06\n","                       Mean reward: 52.31\n","               Mean episode length: 741.71\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 187170816\n","                    Iteration time: 3.22s\n","                        Total time: 3156.52s\n","                               ETA: 162.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 952/1000 \u001b[0m                      \n","\n","                       Computation: 61487 steps/s (collection: 1.833s, learning 1.364s)\n","               Value function loss: 0.0062\n","                    Surrogate loss: 0.0221\n","             Mean action noise std: 0.06\n","                       Mean reward: 58.49\n","               Mean episode length: 799.30\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 187367424\n","                    Iteration time: 3.20s\n","                        Total time: 3159.72s\n","                               ETA: 159.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 953/1000 \u001b[0m                      \n","\n","                       Computation: 60146 steps/s (collection: 1.830s, learning 1.438s)\n","               Value function loss: 0.0047\n","                    Surrogate loss: 0.0261\n","             Mean action noise std: 0.06\n","                       Mean reward: 72.97\n","               Mean episode length: 819.09\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 187564032\n","                    Iteration time: 3.27s\n","                        Total time: 3162.98s\n","                               ETA: 155.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 954/1000 \u001b[0m                      \n","\n","                       Computation: 60478 steps/s (collection: 1.875s, learning 1.376s)\n","               Value function loss: 0.0069\n","                    Surrogate loss: 0.0258\n","             Mean action noise std: 0.05\n","                       Mean reward: 72.95\n","               Mean episode length: 828.91\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 187760640\n","                    Iteration time: 3.25s\n","                        Total time: 3166.24s\n","                               ETA: 152.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 955/1000 \u001b[0m                      \n","\n","                       Computation: 61504 steps/s (collection: 1.838s, learning 1.359s)\n","               Value function loss: 0.0061\n","                    Surrogate loss: 0.0292\n","             Mean action noise std: 0.06\n","                       Mean reward: 63.85\n","               Mean episode length: 791.98\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 187957248\n","                    Iteration time: 3.20s\n","                        Total time: 3169.43s\n","                               ETA: 149.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 956/1000 \u001b[0m                      \n","\n","                       Computation: 61458 steps/s (collection: 1.839s, learning 1.360s)\n","               Value function loss: 0.0065\n","                    Surrogate loss: 0.0333\n","             Mean action noise std: 0.06\n","                       Mean reward: 74.53\n","               Mean episode length: 848.05\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 188153856\n","                    Iteration time: 3.20s\n","                        Total time: 3172.63s\n","                               ETA: 145.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 957/1000 \u001b[0m                      \n","\n","                       Computation: 60035 steps/s (collection: 1.827s, learning 1.447s)\n","               Value function loss: 0.0058\n","                    Surrogate loss: 0.0247\n","             Mean action noise std: 0.05\n","                       Mean reward: 75.79\n","               Mean episode length: 833.18\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 188350464\n","                    Iteration time: 3.27s\n","                        Total time: 3175.91s\n","                               ETA: 142.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 958/1000 \u001b[0m                      \n","\n","                       Computation: 60806 steps/s (collection: 1.870s, learning 1.363s)\n","               Value function loss: 0.0043\n","                    Surrogate loss: 0.0239\n","             Mean action noise std: 0.05\n","                       Mean reward: 77.30\n","               Mean episode length: 879.61\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 188547072\n","                    Iteration time: 3.23s\n","                        Total time: 3179.14s\n","                               ETA: 139.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 959/1000 \u001b[0m                      \n","\n","                       Computation: 61225 steps/s (collection: 1.842s, learning 1.370s)\n","               Value function loss: 0.0039\n","                    Surrogate loss: 0.0181\n","             Mean action noise std: 0.05\n","                       Mean reward: 78.51\n","               Mean episode length: 819.72\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 188743680\n","                    Iteration time: 3.21s\n","                        Total time: 3182.35s\n","                               ETA: 135.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 960/1000 \u001b[0m                      \n","\n","                       Computation: 61486 steps/s (collection: 1.828s, learning 1.370s)\n","               Value function loss: 0.0039\n","                    Surrogate loss: 0.0171\n","             Mean action noise std: 0.05\n","                       Mean reward: 73.56\n","               Mean episode length: 810.43\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 188940288\n","                    Iteration time: 3.20s\n","                        Total time: 3185.55s\n","                               ETA: 132.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 961/1000 \u001b[0m                      \n","\n","                       Computation: 59122 steps/s (collection: 1.834s, learning 1.492s)\n","               Value function loss: 0.0037\n","                    Surrogate loss: 0.0246\n","             Mean action noise std: 0.05\n","                       Mean reward: 74.22\n","               Mean episode length: 853.48\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 189136896\n","                    Iteration time: 3.33s\n","                        Total time: 3188.87s\n","                               ETA: 129.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 962/1000 \u001b[0m                      \n","\n","                       Computation: 60901 steps/s (collection: 1.869s, learning 1.359s)\n","               Value function loss: 0.0043\n","                    Surrogate loss: 0.0200\n","             Mean action noise std: 0.05\n","                       Mean reward: 64.84\n","               Mean episode length: 836.48\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 189333504\n","                    Iteration time: 3.23s\n","                        Total time: 3192.10s\n","                               ETA: 126.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 963/1000 \u001b[0m                      \n","\n","                       Computation: 61378 steps/s (collection: 1.847s, learning 1.357s)\n","               Value function loss: 0.0053\n","                    Surrogate loss: 0.0212\n","             Mean action noise std: 0.05\n","                       Mean reward: 61.77\n","               Mean episode length: 875.97\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 189530112\n","                    Iteration time: 3.20s\n","                        Total time: 3195.31s\n","                               ETA: 122.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 964/1000 \u001b[0m                      \n","\n","                       Computation: 61600 steps/s (collection: 1.834s, learning 1.357s)\n","               Value function loss: 0.0051\n","                    Surrogate loss: 0.0234\n","             Mean action noise std: 0.05\n","                       Mean reward: 74.32\n","               Mean episode length: 842.50\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 189726720\n","                    Iteration time: 3.19s\n","                        Total time: 3198.50s\n","                               ETA: 119.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 965/1000 \u001b[0m                      \n","\n","                       Computation: 59028 steps/s (collection: 1.834s, learning 1.497s)\n","               Value function loss: 0.0042\n","                    Surrogate loss: 0.0215\n","             Mean action noise std: 0.05\n","                       Mean reward: 62.10\n","               Mean episode length: 897.58\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 189923328\n","                    Iteration time: 3.33s\n","                        Total time: 3201.83s\n","                               ETA: 116.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 966/1000 \u001b[0m                      \n","\n","                       Computation: 60887 steps/s (collection: 1.856s, learning 1.373s)\n","               Value function loss: 0.0049\n","                    Surrogate loss: 0.0284\n","             Mean action noise std: 0.05\n","                       Mean reward: 74.66\n","               Mean episode length: 866.60\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 190119936\n","                    Iteration time: 3.23s\n","                        Total time: 3205.06s\n","                               ETA: 112.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 967/1000 \u001b[0m                      \n","\n","                       Computation: 61386 steps/s (collection: 1.841s, learning 1.361s)\n","               Value function loss: 0.0043\n","                    Surrogate loss: 0.0197\n","             Mean action noise std: 0.05\n","                       Mean reward: 68.94\n","               Mean episode length: 848.10\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 190316544\n","                    Iteration time: 3.20s\n","                        Total time: 3208.26s\n","                               ETA: 109.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 968/1000 \u001b[0m                      \n","\n","                       Computation: 61418 steps/s (collection: 1.833s, learning 1.368s)\n","               Value function loss: 0.0041\n","                    Surrogate loss: 0.0219\n","             Mean action noise std: 0.05\n","                       Mean reward: 63.49\n","               Mean episode length: 802.60\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 190513152\n","                    Iteration time: 3.20s\n","                        Total time: 3211.46s\n","                               ETA: 106.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 969/1000 \u001b[0m                      \n","\n","                       Computation: 58980 steps/s (collection: 1.839s, learning 1.495s)\n","               Value function loss: 0.0025\n","                    Surrogate loss: 0.0226\n","             Mean action noise std: 0.05\n","                       Mean reward: 62.66\n","               Mean episode length: 841.99\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 190709760\n","                    Iteration time: 3.33s\n","                        Total time: 3214.79s\n","                               ETA: 102.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 970/1000 \u001b[0m                      \n","\n","                       Computation: 61040 steps/s (collection: 1.859s, learning 1.362s)\n","               Value function loss: 0.0025\n","                    Surrogate loss: 0.0180\n","             Mean action noise std: 0.05\n","                       Mean reward: 58.73\n","               Mean episode length: 716.27\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 190906368\n","                    Iteration time: 3.22s\n","                        Total time: 3218.01s\n","                               ETA: 99.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 971/1000 \u001b[0m                      \n","\n","                       Computation: 61416 steps/s (collection: 1.839s, learning 1.362s)\n","               Value function loss: 0.0023\n","                    Surrogate loss: 0.0162\n","             Mean action noise std: 0.05\n","                       Mean reward: 65.51\n","               Mean episode length: 781.18\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 191102976\n","                    Iteration time: 3.20s\n","                        Total time: 3221.22s\n","                               ETA: 96.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 972/1000 \u001b[0m                      \n","\n","                       Computation: 61460 steps/s (collection: 1.834s, learning 1.365s)\n","               Value function loss: 0.0025\n","                    Surrogate loss: 0.0160\n","             Mean action noise std: 0.05\n","                       Mean reward: 68.11\n","               Mean episode length: 742.76\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 191299584\n","                    Iteration time: 3.20s\n","                        Total time: 3224.41s\n","                               ETA: 92.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 973/1000 \u001b[0m                      \n","\n","                       Computation: 59184 steps/s (collection: 1.843s, learning 1.479s)\n","               Value function loss: 0.0024\n","                    Surrogate loss: 0.0173\n","             Mean action noise std: 0.05\n","                       Mean reward: 53.73\n","               Mean episode length: 724.06\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 191496192\n","                    Iteration time: 3.32s\n","                        Total time: 3227.74s\n","                               ETA: 89.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 974/1000 \u001b[0m                      \n","\n","                       Computation: 60902 steps/s (collection: 1.858s, learning 1.370s)\n","               Value function loss: 0.0020\n","                    Surrogate loss: 0.0154\n","             Mean action noise std: 0.05\n","                       Mean reward: 54.98\n","               Mean episode length: 697.89\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 191692800\n","                    Iteration time: 3.23s\n","                        Total time: 3230.97s\n","                               ETA: 86.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 975/1000 \u001b[0m                      \n","\n","                       Computation: 61588 steps/s (collection: 1.837s, learning 1.355s)\n","               Value function loss: 0.0025\n","                    Surrogate loss: 0.0164\n","             Mean action noise std: 0.05\n","                       Mean reward: 40.57\n","               Mean episode length: 638.40\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 191889408\n","                    Iteration time: 3.19s\n","                        Total time: 3234.16s\n","                               ETA: 82.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 976/1000 \u001b[0m                      \n","\n","                       Computation: 61346 steps/s (collection: 1.835s, learning 1.370s)\n","               Value function loss: 0.0017\n","                    Surrogate loss: 0.0157\n","             Mean action noise std: 0.05\n","                       Mean reward: 36.40\n","               Mean episode length: 615.02\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 192086016\n","                    Iteration time: 3.20s\n","                        Total time: 3237.36s\n","                               ETA: 79.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 977/1000 \u001b[0m                      \n","\n","                       Computation: 58826 steps/s (collection: 1.844s, learning 1.498s)\n","               Value function loss: 0.0075\n","                    Surrogate loss: 0.0192\n","             Mean action noise std: 0.05\n","                       Mean reward: 24.23\n","               Mean episode length: 957.92\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 192282624\n","                    Iteration time: 3.34s\n","                        Total time: 3240.70s\n","                               ETA: 76.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 978/1000 \u001b[0m                      \n","\n","                       Computation: 61111 steps/s (collection: 1.852s, learning 1.365s)\n","               Value function loss: 0.0066\n","                    Surrogate loss: 0.0179\n","             Mean action noise std: 0.05\n","                       Mean reward: 19.22\n","               Mean episode length: 833.86\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 192479232\n","                    Iteration time: 3.22s\n","                        Total time: 3243.92s\n","                               ETA: 72.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 979/1000 \u001b[0m                      \n","\n","                       Computation: 61210 steps/s (collection: 1.838s, learning 1.374s)\n","               Value function loss: 0.0062\n","                    Surrogate loss: 0.0200\n","             Mean action noise std: 0.05\n","                       Mean reward: 41.16\n","               Mean episode length: 890.50\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 192675840\n","                    Iteration time: 3.21s\n","                        Total time: 3247.13s\n","                               ETA: 69.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 980/1000 \u001b[0m                      \n","\n","                       Computation: 61377 steps/s (collection: 1.842s, learning 1.362s)\n","               Value function loss: 0.0059\n","                    Surrogate loss: 0.0224\n","             Mean action noise std: 0.05\n","                       Mean reward: 19.77\n","               Mean episode length: 855.92\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 192872448\n","                    Iteration time: 3.20s\n","                        Total time: 3250.34s\n","                               ETA: 66.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 981/1000 \u001b[0m                      \n","\n","                       Computation: 59127 steps/s (collection: 1.850s, learning 1.475s)\n","               Value function loss: 0.0045\n","                    Surrogate loss: 0.0328\n","             Mean action noise std: 0.05\n","                       Mean reward: 27.69\n","               Mean episode length: 849.94\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 193069056\n","                    Iteration time: 3.33s\n","                        Total time: 3253.66s\n","                               ETA: 63.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 982/1000 \u001b[0m                      \n","\n","                       Computation: 61344 steps/s (collection: 1.844s, learning 1.361s)\n","               Value function loss: 0.0039\n","                    Surrogate loss: 0.0309\n","             Mean action noise std: 0.05\n","                       Mean reward: 30.28\n","               Mean episode length: 880.62\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 193265664\n","                    Iteration time: 3.20s\n","                        Total time: 3256.87s\n","                               ETA: 59.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 983/1000 \u001b[0m                      \n","\n","                       Computation: 61293 steps/s (collection: 1.837s, learning 1.371s)\n","               Value function loss: 0.0039\n","                    Surrogate loss: 0.0313\n","             Mean action noise std: 0.05\n","                       Mean reward: 22.86\n","               Mean episode length: 850.72\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 193462272\n","                    Iteration time: 3.21s\n","                        Total time: 3260.07s\n","                               ETA: 56.3s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 984/1000 \u001b[0m                      \n","\n","                       Computation: 61148 steps/s (collection: 1.843s, learning 1.372s)\n","               Value function loss: 0.0035\n","                    Surrogate loss: 0.0196\n","             Mean action noise std: 0.05\n","                       Mean reward: 35.65\n","               Mean episode length: 819.25\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 193658880\n","                    Iteration time: 3.22s\n","                        Total time: 3263.29s\n","                               ETA: 53.0s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 985/1000 \u001b[0m                      \n","\n","                       Computation: 58320 steps/s (collection: 1.855s, learning 1.516s)\n","               Value function loss: 0.0075\n","                    Surrogate loss: 0.0288\n","             Mean action noise std: 0.05\n","                       Mean reward: 34.52\n","               Mean episode length: 818.39\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 193855488\n","                    Iteration time: 3.37s\n","                        Total time: 3266.66s\n","                               ETA: 49.7s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 986/1000 \u001b[0m                      \n","\n","                       Computation: 61302 steps/s (collection: 1.841s, learning 1.367s)\n","               Value function loss: 0.0051\n","                    Surrogate loss: 0.0243\n","             Mean action noise std: 0.05\n","                       Mean reward: 34.60\n","               Mean episode length: 816.68\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 194052096\n","                    Iteration time: 3.21s\n","                        Total time: 3269.87s\n","                               ETA: 46.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 987/1000 \u001b[0m                      \n","\n","                       Computation: 61351 steps/s (collection: 1.837s, learning 1.367s)\n","               Value function loss: 0.0082\n","                    Surrogate loss: 0.0218\n","             Mean action noise std: 0.05\n","                       Mean reward: 45.78\n","               Mean episode length: 847.64\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 194248704\n","                    Iteration time: 3.20s\n","                        Total time: 3273.07s\n","                               ETA: 43.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 988/1000 \u001b[0m                      \n","\n","                       Computation: 61176 steps/s (collection: 1.832s, learning 1.382s)\n","               Value function loss: 0.0076\n","                    Surrogate loss: 0.0280\n","             Mean action noise std: 0.05\n","                       Mean reward: 54.91\n","               Mean episode length: 857.95\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 194445312\n","                    Iteration time: 3.21s\n","                        Total time: 3276.29s\n","                               ETA: 39.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 989/1000 \u001b[0m                      \n","\n","                       Computation: 58931 steps/s (collection: 1.856s, learning 1.480s)\n","               Value function loss: 0.0094\n","                    Surrogate loss: 0.0331\n","             Mean action noise std: 0.05\n","                       Mean reward: 33.44\n","               Mean episode length: 684.02\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 194641920\n","                    Iteration time: 3.34s\n","                        Total time: 3279.62s\n","                               ETA: 36.4s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 990/1000 \u001b[0m                      \n","\n","                       Computation: 61392 steps/s (collection: 1.839s, learning 1.364s)\n","               Value function loss: 0.0061\n","                    Surrogate loss: 0.0241\n","             Mean action noise std: 0.05\n","                       Mean reward: 51.14\n","               Mean episode length: 781.67\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 194838528\n","                    Iteration time: 3.20s\n","                        Total time: 3282.83s\n","                               ETA: 33.1s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 991/1000 \u001b[0m                      \n","\n","                       Computation: 61520 steps/s (collection: 1.835s, learning 1.361s)\n","               Value function loss: 0.0059\n","                    Surrogate loss: 0.0331\n","             Mean action noise std: 0.05\n","                       Mean reward: 70.30\n","               Mean episode length: 843.21\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 195035136\n","                    Iteration time: 3.20s\n","                        Total time: 3286.02s\n","                               ETA: 29.8s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 992/1000 \u001b[0m                      \n","\n","                       Computation: 61337 steps/s (collection: 1.835s, learning 1.370s)\n","               Value function loss: 0.0073\n","                    Surrogate loss: 0.0302\n","             Mean action noise std: 0.05\n","                       Mean reward: 64.09\n","               Mean episode length: 813.53\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 195231744\n","                    Iteration time: 3.21s\n","                        Total time: 3289.23s\n","                               ETA: 26.5s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 993/1000 \u001b[0m                      \n","\n","                       Computation: 59103 steps/s (collection: 1.867s, learning 1.460s)\n","               Value function loss: 0.0072\n","                    Surrogate loss: 0.0492\n","             Mean action noise std: 0.05\n","                       Mean reward: 55.78\n","               Mean episode length: 786.88\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 195428352\n","                    Iteration time: 3.33s\n","                        Total time: 3292.55s\n","                               ETA: 23.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 994/1000 \u001b[0m                      \n","\n","                       Computation: 61167 steps/s (collection: 1.838s, learning 1.376s)\n","               Value function loss: 0.0102\n","                    Surrogate loss: 0.0363\n","             Mean action noise std: 0.05\n","                       Mean reward: 74.08\n","               Mean episode length: 832.34\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 195624960\n","                    Iteration time: 3.21s\n","                        Total time: 3295.77s\n","                               ETA: 19.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 995/1000 \u001b[0m                      \n","\n","                       Computation: 61474 steps/s (collection: 1.830s, learning 1.368s)\n","               Value function loss: 0.0086\n","                    Surrogate loss: 0.0403\n","             Mean action noise std: 0.05\n","                       Mean reward: 73.76\n","               Mean episode length: 810.73\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 195821568\n","                    Iteration time: 3.20s\n","                        Total time: 3298.97s\n","                               ETA: 16.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 996/1000 \u001b[0m                      \n","\n","                       Computation: 60593 steps/s (collection: 1.836s, learning 1.408s)\n","               Value function loss: 0.0042\n","                    Surrogate loss: 0.0284\n","             Mean action noise std: 0.05\n","                       Mean reward: 94.28\n","               Mean episode length: 930.20\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 196018176\n","                    Iteration time: 3.24s\n","                        Total time: 3302.21s\n","                               ETA: 13.2s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 997/1000 \u001b[0m                      \n","\n","                       Computation: 59134 steps/s (collection: 1.865s, learning 1.459s)\n","               Value function loss: 0.0048\n","                    Surrogate loss: 0.0252\n","             Mean action noise std: 0.05\n","                       Mean reward: 96.04\n","               Mean episode length: 971.33\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 196214784\n","                    Iteration time: 3.32s\n","                        Total time: 3305.54s\n","                               ETA: 9.9s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 998/1000 \u001b[0m                      \n","\n","                       Computation: 61237 steps/s (collection: 1.842s, learning 1.369s)\n","               Value function loss: 0.0056\n","                    Surrogate loss: 0.0280\n","             Mean action noise std: 0.05\n","                       Mean reward: 102.43\n","               Mean episode length: 943.31\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 196411392\n","                    Iteration time: 3.21s\n","                        Total time: 3308.75s\n","                               ETA: 6.6s\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 999/1000 \u001b[0m                      \n","\n","                       Computation: 61568 steps/s (collection: 1.832s, learning 1.362s)\n","               Value function loss: 0.0078\n","                    Surrogate loss: 0.0325\n","             Mean action noise std: 0.05\n","                       Mean reward: 101.04\n","               Mean episode length: 952.49\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 196608000\n","                    Iteration time: 3.19s\n","                        Total time: 3311.94s\n","                               ETA: 3.3s\n","\n","Done training.\n","/content/drive/MyDrive/CSE_598/lab7-rma-phase1/logs/RMA-20251016-183430-None\n"]}],"source":["runner.learn(\n","    num_learning_iterations=1000,\n","    init_at_random_ep_len=False,\n",")\n","print(\"Done training.\")\n","print(logdir)"]},{"cell_type":"markdown","id":"5ec46b0e","metadata":{"id":"5ec46b0e"},"source":["# Analyze the Learned Environment Latent $z_t$ (optional)\n","One thing you might be curious about is what the envirionment latent $z_t$ actually captures. One technique for doing this would be to grab the ActorCriticLatent policy that you trained, and pass in hand-made environment observations vectors to see how it affects $z_t$. Alternatively, you could also collect $z_t$ values while rolling out the policy in randomized environments. Since $z_t$ is likely > 2 dimensions, you may find it useful to visualize it using visualization / latent space projection techniques such as tSNE. (Check out the `./media` folder)\n","\n","I encourage you to explore this if you're curious and have the time!"]},{"cell_type":"code","execution_count":23,"id":"f67bd687","metadata":{"id":"f67bd687","executionInfo":{"status":"ok","timestamp":1760643306268,"user_tz":240,"elapsed":110,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["# Your Code Here"]},{"cell_type":"markdown","id":"1f9c2ab3","metadata":{"id":"1f9c2ab3"},"source":["## What to Turn In\n","\n","`#TODO(student):` Please zip the following files and turn them into the assignment on gradescope:\n","1. this `07_lab_student.ipynb` file. Please make sure to fill our your name and umich ID in the first cell\n","2. the `ActorCriticLatent.py` file\n","\n","Please ensure all cell outputs (videos, plots, etc) are in tact when you download the .ipynb file."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}