{"cells":[{"cell_type":"markdown","id":"0052d469","metadata":{"id":"0052d469"},"source":["# EECS 598 Lab 6: Mujoco Playground Suite & External RL Libraries\n","\n","![lab6_poster](media/lab06_poster.png)"]},{"cell_type":"markdown","id":"35fe0d94","metadata":{"id":"35fe0d94"},"source":["This notebook is worth **80 points**. Your score will be calculated as `score = min(score, 80)`.\n","Questions and implementations are marked with relevent `#TODO(student)` markers.\n","\n","Before starting the assignment, please put your name and UMID in the following format:\n","\n","Firstname LASTNAME, #00000000 (ex. Drew SCHEFFER #31415926)"]},{"cell_type":"markdown","id":"80e2a8b0","metadata":{"id":"80e2a8b0"},"source":["**YOUR ANSWER**\n","\n","SHIVAM UDESHI, 87841376"]},{"cell_type":"markdown","id":"051985ae","metadata":{"id":"051985ae"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"id":"7b4d4e9d","metadata":{"id":"7b4d4e9d","executionInfo":{"status":"ok","timestamp":1760054184697,"user_tz":240,"elapsed":24,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["import sys, types, importlib\n","\n","# Create a tiny fake 'imp' module exposing only 'reload'\n","_imp = types.ModuleType(\"imp\")\n","_imp.reload = importlib.reload\n","sys.modules[\"imp\"] = _imp\n","\n","# load autoreload\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":2,"id":"b241b29a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b241b29a","executionInfo":{"status":"ok","timestamp":1760054184718,"user_tz":240,"elapsed":17,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"16ec8a2d-c6b1-4c6c-8e7c-1124138dd241"},"outputs":[{"output_type":"stream","name":"stdout","text":["Setting environment variable to use GPU rendering:\n","env: MUJOCO_GL=egl\n","env: XLA_PYTHON_CLIENT_PREALLOCATE=false\n"]}],"source":["print('Setting environment variable to use GPU rendering:')\n","%env MUJOCO_GL=egl\n","%env XLA_PYTHON_CLIENT_PREALLOCATE=false"]},{"cell_type":"code","execution_count":3,"id":"e71b5ee4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e71b5ee4","executionInfo":{"status":"ok","timestamp":1760054197805,"user_tz":240,"elapsed":13086,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"d9531741-e962-46d3-df9a-979a065aeecf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Installing mediapy:\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["#@title Import packages for plotting and creating graphics\n","import time\n","import itertools\n","import numpy as np\n","from typing import Callable, NamedTuple, Optional, Union, List\n","\n","# Graphics and plotting.\n","print('Installing mediapy:')\n","!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n","!pip install -q mediapy\n","import mediapy as media\n","import matplotlib.pyplot as plt\n","\n","# More legible printing from numpy.\n","np.set_printoptions(precision=3, suppress=True, linewidth=100)"]},{"cell_type":"markdown","id":"8ee8f44b","metadata":{"id":"8ee8f44b"},"source":["### Google Colab Setup"]},{"cell_type":"markdown","id":"ee71ce41","metadata":{"id":"ee71ce41"},"source":["Next, we'll run a few commands to set up the environment on Google Colab. If you are running this notebook locally you can skip this section"]},{"cell_type":"markdown","id":"a16e6299","metadata":{"id":"a16e6299"},"source":["Run the following to mount this notebook to your Google Drive. Follow the link and sign into the Google account following the prompts. Use the same Google account that you used to store this notebook."]},{"cell_type":"code","execution_count":4,"id":"547f0b4e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"547f0b4e","executionInfo":{"status":"ok","timestamp":1760054228360,"user_tz":240,"elapsed":30551,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"1dd35041-4d65-41dd-ff69-01cbc8f3d075"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","id":"4786706d","metadata":{"id":"4786706d"},"source":["Now update the path below to point to the folder in your Google Drive where you uploaded this notebook. If everything worked correctly you should see the following filenames at least: [`06_lab_student.ipynb`, `CSE598RSLWrapper.py`]"]},{"cell_type":"code","execution_count":5,"id":"3f2f3840","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3f2f3840","executionInfo":{"status":"ok","timestamp":1760054230060,"user_tz":240,"elapsed":1688,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"4e5bb747-77fc-44ab-b89f-d499c7a42992"},"outputs":[{"output_type":"stream","name":"stdout","text":["['custom_domain_randomize.py', '.DS_Store', 'CSE598RSLWrapper.py', 'custom_go1_locomote.py', 'checkpoints', 'media', '06_lab_student.ipynb']\n"]}],"source":["import os\n","\n","# TODO: Fill in the Google Drive path where you uploaded project 2\n","# Example: If you create a 2025FA folder and put all the files under Lab6, then '2025FA/Lab6'\n","# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '2025FA/Lab6'\n","\n","GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = '/content/drive/MyDrive/CSE_598/lab6-playground-rsl/lab6-playground-rsl'\n","GOOGLE_DRIVE_PATH_LAB6 = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n","\n","print(os.listdir(GOOGLE_DRIVE_PATH_LAB6))\n","\n","# Add to path and change directory for good measure\n","sys.path.append(GOOGLE_DRIVE_PATH_LAB6)\n","os.chdir(GOOGLE_DRIVE_PATH_LAB6)"]},{"cell_type":"code","execution_count":6,"id":"85dfee08","metadata":{"id":"85dfee08","executionInfo":{"status":"ok","timestamp":1760054230305,"user_tz":240,"elapsed":244,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["from google.colab import files\n","\n","import distutils.util\n","import os\n","import subprocess\n","if subprocess.run('nvidia-smi').returncode:\n","  raise RuntimeError(\n","      'Cannot communicate with GPU. '\n","      'Make sure you are using a GPU Colab runtime. '\n","      'Go to the Runtime menu and select Choose runtime type.')\n","\n","# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n","# This is usually installed as part of an Nvidia driver package, but the Colab\n","# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n","# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n","NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n","if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n","  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n","    f.write(\"\"\"{\n","    \"file_format_version\" : \"1.0.0\",\n","    \"ICD\" : {\n","        \"library_path\" : \"libEGL_nvidia.so.0\"\n","    }\n","}\n","\"\"\")"]},{"cell_type":"markdown","id":"1cc4b0ba","metadata":{"id":"1cc4b0ba"},"source":["## Mujoco, JAX, MJX, BRAX, and Playground Setup & Imports"]},{"cell_type":"code","execution_count":7,"id":"65316e07","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65316e07","executionInfo":{"status":"ok","timestamp":1760054260526,"user_tz":240,"elapsed":30202,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"ab785820-1cfa-49ea-8d71-7e604d8be8f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mujoco\n","  Downloading mujoco-3.3.6-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (41 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.4.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco) (1.13.0)\n","Collecting glfw (from mujoco)\n","  Downloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mujoco) (2.0.2)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco) (3.1.10)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (2025.3.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (6.5.2)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (4.15.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco) (3.23.0)\n","Downloading mujoco-3.3.6-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: glfw, mujoco\n","Successfully installed glfw-2.10.0 mujoco-3.3.6\n","Collecting mujoco_mjx\n","  Downloading mujoco_mjx-3.3.6-py3-none-any.whl.metadata (3.4 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco_mjx) (1.4.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco_mjx) (1.13.0)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mujoco_mjx) (0.5.3)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mujoco_mjx) (0.5.3)\n","Requirement already satisfied: mujoco>=3.3.6.dev0 in /usr/local/lib/python3.12/dist-packages (from mujoco_mjx) (3.3.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from mujoco_mjx) (1.16.2)\n","Collecting trimesh (from mujoco_mjx)\n","  Downloading trimesh-4.8.3-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: glfw in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.3.6.dev0->mujoco_mjx) (2.10.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.3.6.dev0->mujoco_mjx) (2.0.2)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.3.6.dev0->mujoco_mjx) (3.1.10)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco_mjx) (2025.3.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco_mjx) (6.5.2)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco_mjx) (4.15.0)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco_mjx) (3.23.0)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mujoco_mjx) (0.5.3)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mujoco_mjx) (3.4.0)\n","Downloading mujoco_mjx-3.3.6-py3-none-any.whl (7.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading trimesh-4.8.3-py3-none-any.whl (735 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.5/735.5 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: trimesh, mujoco_mjx\n","Successfully installed mujoco_mjx-3.3.6 trimesh-4.8.3\n","Collecting brax\n","  Downloading brax-0.13.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from brax) (1.4.0)\n","Requirement already satisfied: etils in /usr/local/lib/python3.12/dist-packages (from brax) (1.13.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (from brax) (3.1.2)\n","Collecting flask-cors (from brax)\n","  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n","Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (from brax) (0.10.6)\n","Requirement already satisfied: jax>=0.4.6 in /usr/local/lib/python3.12/dist-packages (from brax) (0.5.3)\n","Requirement already satisfied: jaxlib>=0.4.6 in /usr/local/lib/python3.12/dist-packages (from brax) (0.5.3)\n","Collecting jaxopt (from brax)\n","  Downloading jaxopt-0.8.5-py3-none-any.whl.metadata (3.3 kB)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from brax) (3.1.6)\n","Collecting ml-collections (from brax)\n","  Downloading ml_collections-1.1.0-py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: mujoco in /usr/local/lib/python3.12/dist-packages (from brax) (3.3.6)\n","Requirement already satisfied: mujoco-mjx in /usr/local/lib/python3.12/dist-packages (from brax) (3.3.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from brax) (2.0.2)\n","Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from brax) (0.2.6)\n","Requirement already satisfied: orbax-checkpoint>=0.11.22 in /usr/local/lib/python3.12/dist-packages (from brax) (0.11.24)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from brax) (11.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from brax) (1.16.2)\n","Collecting tensorboardx (from brax)\n","  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n","Requirement already satisfied: trimesh in /usr/local/lib/python3.12/dist-packages (from brax) (4.8.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from brax) (4.15.0)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.6->brax) (0.5.3)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.6->brax) (3.4.0)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (1.1.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (6.0.3)\n","Requirement already satisfied: tensorstore>=0.1.71 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (0.1.77)\n","Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (1.6.0)\n","Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (24.1.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (5.29.5)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (4.13.0)\n","Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax) (3.20.2)\n","Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask->brax) (1.9.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask->brax) (8.3.0)\n","Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask->brax) (2.2.0)\n","Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask->brax) (3.0.3)\n","Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask->brax) (3.1.3)\n","Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax->brax) (13.9.4)\n","Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax->brax) (0.1.10)\n","Requirement already satisfied: glfw in /usr/local/lib/python3.12/dist-packages (from mujoco->brax) (2.10.0)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco->brax) (3.1.10)\n","Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->brax) (0.1.90)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboardx->brax) (25.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->brax) (75.2.0)\n","Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->brax) (0.12.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->brax) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->brax) (2.19.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint>=0.11.22->brax) (2025.3.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint>=0.11.22->brax) (6.5.2)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath,epy]->orbax-checkpoint>=0.11.22->brax) (3.23.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->brax) (0.1.2)\n","Downloading brax-0.13.0-py3-none-any.whl (344 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n","Downloading jaxopt-0.8.5-py3-none-any.whl (172 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.4/172.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ml_collections-1.1.0-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tensorboardx, ml-collections, flask-cors, jaxopt, brax\n","Successfully installed brax-0.13.0 flask-cors-6.0.1 jaxopt-0.8.5 ml-collections-1.1.0 tensorboardx-2.6.4\n","Collecting noise\n","  Downloading noise-1.2.2.zip (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: noise\n","  Building wheel for noise (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for noise: filename=noise-1.2.2-cp312-cp312-linux_x86_64.whl size=56631 sha256=4d66042e8aa9a99353fb4f577baffa79598272d055eae9816786b47357214317\n","  Stored in directory: /root/.cache/pip/wheels/78/71/a2/47a0c6acdeb8f7a2f4e69067d3c737219e36414136441a1ef8\n","Successfully built noise\n","Installing collected packages: noise\n","Successfully installed noise-1.2.2\n"]}],"source":["!pip install mujoco\n","!pip install mujoco_mjx\n","!pip install brax\n","!pip install noise\n","\n","# TODO(student): If you're running this locally, make sure to install cuda enabled jax via something like:\n","# !pip install \"jax[cuda12]\""]},{"cell_type":"code","execution_count":8,"id":"cee0de70","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cee0de70","executionInfo":{"status":"ok","timestamp":1760054295845,"user_tz":240,"elapsed":35313,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"3a281711-7f96-4324-e904-e18b548abff5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting playground\n","  Downloading playground-0.0.5-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: brax>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from playground) (0.13.0)\n","Requirement already satisfied: etils in /usr/local/lib/python3.12/dist-packages (from playground) (1.13.0)\n","Requirement already satisfied: flax in /usr/local/lib/python3.12/dist-packages (from playground) (0.10.6)\n","Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from playground) (0.5.3)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from playground) (5.4.0)\n","Requirement already satisfied: ml-collections in /usr/local/lib/python3.12/dist-packages (from playground) (1.1.0)\n","Requirement already satisfied: mujoco-mjx>=3.2.7 in /usr/local/lib/python3.12/dist-packages (from playground) (3.3.6)\n","Requirement already satisfied: mujoco>=3.2.7 in /usr/local/lib/python3.12/dist-packages (from playground) (3.3.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from playground) (4.67.1)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (1.4.0)\n","Requirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (3.1.2)\n","Requirement already satisfied: flask-cors in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (6.0.1)\n","Requirement already satisfied: jaxlib>=0.4.6 in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (0.5.3)\n","Requirement already satisfied: jaxopt in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (0.8.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (3.1.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (2.0.2)\n","Requirement already satisfied: optax in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (0.2.6)\n","Requirement already satisfied: orbax-checkpoint>=0.11.22 in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (0.11.24)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (11.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (1.16.2)\n","Requirement already satisfied: tensorboardx in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (2.6.4)\n","Requirement already satisfied: trimesh in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (4.8.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from brax>=0.12.1->playground) (4.15.0)\n","Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->playground) (0.5.3)\n","Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->playground) (3.4.0)\n","Requirement already satisfied: glfw in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.2.7->playground) (2.10.0)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.2.7->playground) (3.1.10)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.12/dist-packages (from flax->playground) (1.1.1)\n","Requirement already satisfied: tensorstore in /usr/local/lib/python3.12/dist-packages (from flax->playground) (0.1.77)\n","Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.12/dist-packages (from flax->playground) (13.9.4)\n","Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.12/dist-packages (from flax->playground) (6.0.3)\n","Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.12/dist-packages (from flax->playground) (0.1.10)\n","Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax>=0.12.1->playground) (1.6.0)\n","Requirement already satisfied: aiofiles in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax>=0.12.1->playground) (24.1.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax>=0.12.1->playground) (5.29.5)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax>=0.12.1->playground) (4.13.0)\n","Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.12/dist-packages (from orbax-checkpoint>=0.11.22->brax>=0.12.1->playground) (3.20.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->playground) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=11.1->flax->playground) (2.19.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=3.2.7->playground) (2025.3.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=3.2.7->playground) (6.5.2)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=3.2.7->playground) (3.23.0)\n","Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask->brax>=0.12.1->playground) (1.9.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask->brax>=0.12.1->playground) (8.3.0)\n","Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask->brax>=0.12.1->playground) (2.2.0)\n","Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask->brax>=0.12.1->playground) (3.0.3)\n","Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask->brax>=0.12.1->playground) (3.1.3)\n","Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.12/dist-packages (from optax->brax>=0.12.1->playground) (0.1.90)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboardx->brax>=0.12.1->playground) (25.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->brax>=0.12.1->playground) (75.2.0)\n","Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chex>=0.1.87->optax->brax>=0.12.1->playground) (0.12.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax->playground) (0.1.2)\n","Downloading playground-0.0.5-py3-none-any.whl (7.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: playground\n","Successfully installed playground-0.0.5\n","Collecting rsl-rl-lib<3.0.0\n","  Downloading rsl_rl_lib-2.3.3-py3-none-any.whl.metadata (5.4 kB)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from rsl-rl-lib<3.0.0) (2.8.0+cu126)\n","Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from rsl-rl-lib<3.0.0) (0.23.0+cu126)\n","Requirement already satisfied: numpy>=1.16.4 in /usr/local/lib/python3.12/dist-packages (from rsl-rl-lib<3.0.0) (2.0.2)\n","Requirement already satisfied: GitPython in /usr/local/lib/python3.12/dist-packages (from rsl-rl-lib<3.0.0) (3.1.45)\n","Collecting onnx (from rsl-rl-lib<3.0.0)\n","  Downloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (3.19.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10.0->rsl-rl-lib<3.0.0) (3.4.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision>=0.5.0->rsl-rl-lib<3.0.0) (11.3.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from GitPython->rsl-rl-lib<3.0.0) (4.0.12)\n","Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx->rsl-rl-lib<3.0.0) (5.29.5)\n","Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnx->rsl-rl-lib<3.0.0) (0.5.3)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->GitPython->rsl-rl-lib<3.0.0) (5.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10.0->rsl-rl-lib<3.0.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10.0->rsl-rl-lib<3.0.0) (3.0.3)\n","Downloading rsl_rl_lib-2.3.3-py3-none-any.whl (40 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnx-1.19.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: onnx, rsl-rl-lib\n","Successfully installed onnx-1.19.0 rsl-rl-lib-2.3.3\n","Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.1)\n","Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n","Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.9)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.39.0)\n","Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.75.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.9)\n","Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n","Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n"]}],"source":["!pip install playground\n","!pip install \"rsl-rl-lib<3.0.0\"\n","!pip install wandb\n","!pip install tensorboard"]},{"cell_type":"code","execution_count":9,"id":"72dd4342","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72dd4342","executionInfo":{"status":"ok","timestamp":1760054296237,"user_tz":240,"elapsed":389,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"3705c1df-91af-4f0e-8352-c05df79516fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking that the installation succeeded:\n","Installation successful.\n"]}],"source":["import os\n","\n","try:\n","  print('Checking that the installation succeeded:')\n","  import mujoco\n","  mujoco.MjModel.from_xml_string('<mujoco/>')\n","except Exception as e:\n","  raise e from RuntimeError(\n","      'Something went wrong during installation. Check the shell output above '\n","      'for more information.\\n'\n","      'If using a hosted Colab runtime, make sure you enable GPU acceleration '\n","      'by going to the Runtime menu and selecting \"Choose runtime type\".')\n","\n","print('Installation successful.')\n","\n","# Tell XLA to use Triton GEMM, this improves steps/sec by ~30% on some GPUs\n","xla_flags = os.environ.get('XLA_FLAGS', '')\n","xla_flags += ' --xla_gpu_triton_gemm_any=True'\n","os.environ['XLA_FLAGS'] = xla_flags"]},{"cell_type":"markdown","id":"01f742de","metadata":{"id":"01f742de"},"source":["Ensure that the output of the following cell is `[CudaDevice(id=0)]`"]},{"cell_type":"code","execution_count":10,"id":"1519cc79","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1519cc79","executionInfo":{"status":"ok","timestamp":1760054297523,"user_tz":240,"elapsed":1285,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"5b71d3e0-0bcf-4fad-cfec-c9a5bf5b4994"},"outputs":[{"output_type":"stream","name":"stdout","text":["[CudaDevice(id=0)]\n"]}],"source":["import jax\n","print(jax.devices())"]},{"cell_type":"code","execution_count":11,"id":"9538916c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9538916c","executionInfo":{"status":"ok","timestamp":1760054305135,"user_tz":240,"elapsed":7578,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"14e250bb-0f54-4a7d-a80c-e520ba4e5252"},"outputs":[{"output_type":"stream","name":"stdout","text":["Failed to import warp: No module named 'warp'\n","Failed to import mujoco.mjx.third_party.mujoco_warp as mujoco_warp: No module named 'warp'\n"]}],"source":["#@title Import MuJoCo, MJX, and Brax\n","from datetime import datetime\n","from etils import epath\n","import functools\n","from IPython.display import HTML\n","from typing import Any, Dict, Sequence, Tuple, Union\n","import os\n","from ml_collections import config_dict\n","\n","\n","import jax\n","from jax import numpy as jp\n","import numpy as np\n","from flax.training import orbax_utils\n","from flax import struct\n","from matplotlib import pyplot as plt\n","import mediapy as media\n","from orbax import checkpoint as ocp\n","\n","import mujoco\n","from mujoco import mjx\n","\n","from brax import base\n","from brax import envs\n","from brax import math\n","from brax.base import Base, Motion, Transform\n","from brax.base import State as PipelineState\n","from brax.envs.base import Env, PipelineEnv, State\n","from brax.mjx.base import State as MjxState\n","from brax.training.agents.ppo import train as ppo\n","from brax.training.agents.ppo import networks as ppo_networks\n","from brax.io import html, mjcf, model"]},{"cell_type":"code","execution_count":12,"id":"b4218c78","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4218c78","executionInfo":{"status":"ok","timestamp":1760054356672,"user_tz":240,"elapsed":51533,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"13f9e0cf-b636-44cf-a57f-f84e8f714b38"},"outputs":[{"output_type":"stream","name":"stdout","text":["mujoco_menagerie not found. Downloading...\n"]},{"output_type":"stream","name":"stderr","text":["Cloning mujoco_menagerie: ██████████| 100/100 [00:43<00:00]\n"]},{"output_type":"stream","name":"stdout","text":["Checking out commit 14ceccf557cc47240202f2354d684eca58ff8de4\n","Successfully downloaded mujoco_menagerie\n"]}],"source":["\n","import os\n","\n","xla_flags = os.environ.get(\"XLA_FLAGS\", \"\")\n","xla_flags += \" --xla_gpu_triton_gemm_any=True\"\n","os.environ[\"XLA_FLAGS\"] = xla_flags\n","os.environ[\"MUJOCO_GL\"] = \"egl\"\n","\n","from datetime import datetime\n","import json\n","\n","from absl import app\n","from absl import flags\n","from absl import logging\n","import jax\n","import mediapy as media\n","from ml_collections import config_dict\n","import mujoco\n","from rsl_rl.runners import OnPolicyRunner\n","import torch\n","# import wandb\n","\n","import mujoco_playground\n","from mujoco_playground import registry\n","from mujoco_playground import wrapper_torch\n","from mujoco_playground import wrapper\n","from mujoco_playground.config import locomotion_params\n","from mujoco_playground.config import manipulation_params\n","from mujoco_playground.config import dm_control_suite_params\n","\n","# Suppress logs if you want\n","logging.set_verbosity(logging.WARNING)"]},{"cell_type":"markdown","id":"5593089f","metadata":{"id":"5593089f"},"source":["# Playing around with the Full MujocoPlayground Stack!\n","Up until this point, we actually haven't used the full MujocoPlayground software stack. We have been implementing our environments and training loops entirely in the BRAX ecosystem. Playground offers a slightly different environment structure which is consistent among all of the environments that they impliment. Typically, however, they still use the JAX-based BRAX RL abstractions to train their agents. In this lab, we'll get more experience working with Mujoco Playground and interfacing it with a popular 3rd party RL library.\n","\n","Below lists all the default environments that are implimented. Check out their [Github](https://github.com/google-deepmind/mujoco_playground/tree/main/mujoco_playground/_src/locomotion) to see the implementations. The code should look very similar to what you've seen in previous labs."]},{"cell_type":"code","execution_count":13,"id":"31e1c584","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"31e1c584","executionInfo":{"status":"ok","timestamp":1760054356709,"user_tz":240,"elapsed":33,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"5cc8c0f2-be39-4e9b-9116-690c8153d507"},"outputs":[{"output_type":"stream","name":"stdout","text":["These are all of the implemented playground envs: \n"," AcrobotSwingup, AcrobotSwingupSparse, BallInCup, CartpoleBalance, CartpoleBalanceSparse, CartpoleSwingup, CartpoleSwingupSparse, CheetahRun, FingerSpin, FingerTurnEasy, FingerTurnHard, FishSwim, HopperHop, HopperStand, HumanoidStand, HumanoidWalk, HumanoidRun, PendulumSwingup, PointMass, ReacherEasy, ReacherHard, SwimmerSwimmer6, WalkerRun, WalkerStand, WalkerWalk, ApolloJoystickFlatTerrain, BarkourJoystick, BerkeleyHumanoidJoystickFlatTerrain, BerkeleyHumanoidJoystickRoughTerrain, G1JoystickFlatTerrain, G1JoystickRoughTerrain, Go1JoystickFlatTerrain, Go1JoystickRoughTerrain, Go1Getup, Go1Handstand, Go1Footstand, H1InplaceGaitTracking, H1JoystickGaitTracking, Op3Joystick, SpotFlatTerrainJoystick, SpotGetup, SpotJoystickGaitTracking, T1JoystickFlatTerrain, T1JoystickRoughTerrain, AlohaHandOver, AlohaSinglePegInsertion, PandaPickCube, PandaPickCubeOrientation, PandaPickCubeCartesian, PandaOpenCabinet, PandaRobotiqPushCube, LeapCubeReorient, LeapCubeRotateZAxis\n"]}],"source":["print(\"These are all of the implemented playground envs: \\n\", \", \".join(registry.ALL_ENVS))"]},{"cell_type":"markdown","id":"e98ec916","metadata":{"id":"e98ec916"},"source":["### Inspecting the environment implementations\n","\n","`TODO(student):` Describe one similarity and one difference with how the environments are implemented in MujocoPlayground compared to our previous BRAX-based implementations. **(10 points)**\n","\n","**Answer Here**\n","BRAX environments are implemented in pure JAX, which makes them differentiable and allows for massive parallelization on accelerators (TPUs/GPUs). The entire simulation is vectorized and JIT-compiled.\n","\n","MujocoPlayground uses the MuJoCo physics engine as the backend. This is a compiled C++ engine wrapped in Python, prioritizing physical accuracy and stability, but not differentiable. Parallelization is done differently (via multiple processes or vectorized environments, not JAX compilation).\n","\n","\n"]},{"cell_type":"markdown","id":"b175cda8","metadata":{"id":"b175cda8"},"source":["# Customizing an Environment / Task\n","\n","Often times, when both learning about and doing research in reinforcement learning, one of the most effective ways to build understanding and develop new ideas is to start from a baseline that already works.\n","From there, making small, deliberate modifications (i.e. adjusting rewards, alterning the learning algorithm, or changing which state variables the agent observes) can uncover a great deal of insight.\n","In my experience, these incremental tweaks not only help highlight the role of the design choices, but also help develop intuition about what drives performance, potentially leading to breakthroughs.\n","\n","In this section of the lab, I hope that you get a chance to experiment with this kind of play/debugging. Specifically, I'll ask you to build on top of at least one environment implemented in Playground, and\n","compare it to the default environment. Basically... modify something!\n","\n","Here's a non-exhaustive list of some example modifications:\n","- Add / Delete / change a reward term\n","- Give the policy an observations that that it may not have access to in the real world (e.g. terrain friction, depth of terrain, etc)\n","- Remove or periodically stiffen one of the legs (fault recovery)\n","- Change the direction / magnitude of the gravity vector\n","- Change the noise level of the observations\n","- Simulate Sensor Latency\n","- Change what's randomized during training"]},{"cell_type":"markdown","id":"604150e8","metadata":{"id":"604150e8"},"source":["`TODO(student):` Describe the MujocoPlayground environment that you intend to work with, and the environment modification that you intend to make. Give a hypothesis for the effect of your modification on the learned policy. How could you measure / test that hypothesis (Is it something that is evident in a video rollout? Do you need to plot anything)?  **(40 points)**\n","\n","**\\[Answer Here\\]**\n","\n","The environment I am using is Go1JoystickFlatTerrain, where a simulated quadruped robot must follow joystick commands while maintaining stable locomotion on flat terrain. The agent receives proprioceptive observations including joint positions, velocities, body orientation, and linear and angular velocities, and the reward encourages accurate command tracking, energy efficiency, and proper foot placement. The modification I implemented is to vary the observation noise level, which adds Gaussian noise to sensors such as joint positions, velocities, and gravity, simulating imperfect real-world measurements. Policies trained without noise are expected to perform well under clean observations but may degrade when tested with higher sensor noise, resulting in poorer command tracking and less stable gaits. By evaluating the baseline policy under three noise conditions—low, medium, and high—we can measure how sensitive it is to observational uncertainty. Metrics include cumulative episode reward across multiple rollouts, as well as reward component breakdowns to identify which aspects of performance are most affected. Visualization includes line plots of mean reward ± standard deviation across noise levels and horizontal stacked bar plots showing reward contributions per component. The goal is to assess whether the baseline policy can generalize under different levels of observation noise and identify which reward components are most impacted by increased sensor uncertainty."]},{"cell_type":"markdown","id":"ecc7ae6b","metadata":{"id":"ecc7ae6b"},"source":["## Training / Testing a Baseline Policy"]},{"cell_type":"markdown","id":"343b938b","metadata":{"id":"343b938b"},"source":["First, get the default environment configuration for your selected environment. This specifies things like reward constants, noise values, PD values, etc."]},{"cell_type":"code","execution_count":42,"id":"822fe759","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"822fe759","executionInfo":{"status":"ok","timestamp":1760057666184,"user_tz":240,"elapsed":460,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"2e5945bc-1ab6-4e67-bee5-d979e38aa6e9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Kd: 0.5\n","Kp: 35.0\n","action_repeat: 1\n","action_scale: 0.5\n","command_config:\n","  a:\n","  - 1.5\n","  - 0.8\n","  - 1.2\n","  b:\n","  - 0.9\n","  - 0.25\n","  - 0.5\n","ctrl_dt: 0.02\n","episode_length: 1000\n","history_len: 1\n","noise_config:\n","  level: 1.0\n","  scales:\n","    gravity: 0.05\n","    gyro: 0.2\n","    joint_pos: 0.03\n","    joint_vel: 1.5\n","    linvel: 0.1\n","pert_config:\n","  enable: false\n","  kick_durations:\n","  - 0.05\n","  - 0.2\n","  kick_wait_times:\n","  - 1.0\n","  - 3.0\n","  velocity_kick:\n","  - 0.0\n","  - 3.0\n","reward_config:\n","  max_foot_height: 0.1\n","  scales:\n","    action_rate: -0.01\n","    ang_vel_xy: -0.05\n","    dof_pos_limits: -1.0\n","    energy: -0.001\n","    feet_air_time: 0.1\n","    feet_clearance: -2.0\n","    feet_height: -0.2\n","    feet_slip: -0.1\n","    lin_vel_z: -0.5\n","    orientation: -5.0\n","    pose: 0.5\n","    stand_still: -1.0\n","    termination: -1.0\n","    torques: -0.0002\n","    tracking_ang_vel: 0.5\n","    tracking_lin_vel: 1.0\n","  tracking_sigma: 0.25\n","sim_dt: 0.004\n","soft_joint_pos_limit_factor: 0.95"]},"metadata":{},"execution_count":42}],"source":[" #TODO(student): Change this to the playground environment you want to work with. Pretrained model checkpoints are available for the \"Go1Getup\" and \"Go1JoystickFlatTerrain\" environments for your convenience\n","env_name = \"Go1JoystickFlatTerrain\"\n","\n","env = registry.load(env_name)\n","env_cfg = registry.get_default_config(env_name)\n","env_cfg"]},{"cell_type":"markdown","id":"b008eb84","metadata":{"id":"b008eb84"},"source":["Next, get the standard BRAX RL configuration for an MLP policy trained with PPO."]},{"cell_type":"code","execution_count":43,"id":"0d2496c4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0d2496c4","executionInfo":{"status":"ok","timestamp":1760057669154,"user_tz":240,"elapsed":61,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"d232ff10-01fc-4c7a-c61b-44dd1c6b6e03"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["action_repeat: 1\n","batch_size: 256\n","discounting: 0.97\n","entropy_cost: 0.01\n","episode_length: 1000\n","learning_rate: 0.0003\n","max_grad_norm: 1.0\n","network_factory:\n","  policy_hidden_layer_sizes: &id001 !!python/tuple\n","  - 512\n","  - 256\n","  - 128\n","  policy_obs_key: state\n","  value_hidden_layer_sizes: *id001\n","  value_obs_key: privileged_state\n","normalize_observations: true\n","num_envs: 8192\n","num_evals: 10\n","num_minibatches: 32\n","num_resets_per_eval: 1\n","num_timesteps: 200000000\n","num_updates_per_batch: 4\n","reward_scaling: 1.0\n","unroll_length: 20"]},"metadata":{},"execution_count":43}],"source":["\n","from mujoco_playground.config import locomotion_params\n","ppo_params = locomotion_params.brax_ppo_config(env_name)\n","ppo_params.num_timesteps = 200_000_000\n","\n","ppo_params"]},{"cell_type":"markdown","id":"d80deb3a","metadata":{"id":"d80deb3a"},"source":["Get the domain randomizer... Very similar to what you implemented in `Lab04`. This domain randomization function randomizes over friction, armature, center of mass of the torso, and link masses, amongst other simulation parameters. See the [implementation here](https://github.com/google-deepmind/mujoco_playground/blob/main/mujoco_playground/_src/locomotion/go1/randomize.py)."]},{"cell_type":"code","execution_count":44,"id":"99f5ec98","metadata":{"id":"99f5ec98","executionInfo":{"status":"ok","timestamp":1760057672544,"user_tz":240,"elapsed":44,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["# from custom_domain_randomize import custom_domain_randomize\n","# randomizer = custom_domain_randomize # Custom randomizer\n","\n","randomizer = registry.get_domain_randomizer(env_name) # Default randomizer"]},{"cell_type":"markdown","id":"b351f2eb","metadata":{"id":"b351f2eb"},"source":["Set up the brax training function. This should look almost identical to what was implemented in `Lab05`, however note the additional `wrapper.wrap_for_brax_training`."]},{"cell_type":"code","execution_count":45,"id":"5a45ed99","metadata":{"id":"5a45ed99","executionInfo":{"status":"ok","timestamp":1760057673920,"user_tz":240,"elapsed":52,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["from IPython.display import HTML, clear_output\n","\n","x_data, y_data, y_dataerr = [], [], []\n","times = [datetime.now()]\n","\n","\n","def progress_callback(num_steps, metrics):\n","  clear_output(wait=True)\n","\n","  times.append(datetime.now())\n","  x_data.append(num_steps)\n","  y_data.append(metrics[\"eval/episode_reward\"])\n","  y_dataerr.append(metrics[\"eval/episode_reward_std\"])\n","\n","  plt.xlim([0, ppo_params[\"num_timesteps\"] * 1.25])\n","  plt.xlabel(\"# environment steps\")\n","  plt.ylabel(\"reward per episode\")\n","  plt.title(f\"y={y_data[-1]:.3f}\")\n","  plt.errorbar(x_data, y_data, yerr=y_dataerr, color=\"blue\")\n","\n","  display(plt.gcf())\n","\n","ppo_training_params = dict(ppo_params)\n","network_factory = ppo_networks.make_ppo_networks\n","if \"network_factory\" in ppo_params:\n","  del ppo_training_params[\"network_factory\"]\n","  network_factory = functools.partial(\n","      ppo_networks.make_ppo_networks,\n","      **ppo_params.network_factory\n","  )\n","\n","train_fn = functools.partial(\n","    ppo.train, **dict(ppo_training_params),\n","    network_factory=network_factory,\n","    randomization_fn=randomizer,\n","    progress_fn=progress_callback,\n","    # TODO(student): Note the below change from previous labs.\n","    # The env we're working with isn't BRAX, so we have to wrap it to be compatible with brax RL training...\n","    wrap_env_fn=wrapper.wrap_for_brax_training,\n","    seed=598\n",")"]},{"cell_type":"markdown","id":"68b73afe","metadata":{"id":"68b73afe"},"source":["In the cell below, you'll either train your BRAX policy from scratch or load from a checkpoint. Given COLAB usage limits, I've provided checkpoints as baselines in the `./checkpoints` directory along with their corresponding training plots."]},{"cell_type":"code","execution_count":46,"id":"1d2bb176","metadata":{"id":"1d2bb176","executionInfo":{"status":"ok","timestamp":1760057720516,"user_tz":240,"elapsed":41698,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["\n","TRAIN_POLICY = False\n","\n","if TRAIN_POLICY:\n","    # Running the below will TRAIN the policy from scratch\n","\n","    make_inference_fn, params, metrics = train_fn(\n","        environment=env,\n","        eval_env=registry.load(env_name, config=env_cfg),\n","    )\n","    print(f\"time to jit: {times[1] - times[0]}\")\n","    print(f\"time to train: {times[-1] - times[1]}\")\n","\n","    # Save the model\n","\n","    # TODO(student): You may have to change the checkpoint below based on which environment you used\n","    model_path = './checkpoints/go1joystickflat_custom'\n","    model.save_params(model_path, params)\n","    params = model.load_params(model_path)\n","else:\n","    # Running the below will load a pretrained policy from a checkpoint\n","    make_inference_fn, params, metrics = train_fn(\n","        environment=env,\n","        eval_env=registry.load(env_name, config=env_cfg),\n","        wrap_env_fn=wrapper.wrap_for_brax_training,\n","        num_timesteps = 0,\n","    )\n","\n","    # TODO(student): You may have to change the checkpoint below based on which environment you used\n","    model_path = './checkpoints/go1joystickflat_default'\n","    params = model.load_params(model_path)\n"]},{"cell_type":"markdown","id":"7773d684","metadata":{"id":"7773d684"},"source":["##### Rollout & render the baseline policy"]},{"cell_type":"code","execution_count":47,"id":"0ebc45fd","metadata":{"id":"0ebc45fd","executionInfo":{"status":"ok","timestamp":1760057720942,"user_tz":240,"elapsed":415,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["# Perform the jitting\n","env_cfg = registry.get_default_config(env_name)\n","eval_env = registry.load(env_name, config=env_cfg)\n","\n","jit_reset = jax.jit(eval_env.reset)\n","jit_step = jax.jit(eval_env.step)\n","jit_inference_fn = jax.jit(make_inference_fn(params, deterministic=True))\n","rand_idx = 12"]},{"cell_type":"code","execution_count":48,"id":"e69e0b4d","metadata":{"id":"e69e0b4d","executionInfo":{"status":"ok","timestamp":1760057831271,"user_tz":240,"elapsed":110297,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["# Do a rollout\n","from mujoco_playground._src.gait import draw_joystick_command\n","\n","x_vel = 1.0\n","y_vel = 0.0\n","yaw_vel = 0\n","\n","\n","rand_idx = rand_idx + 1\n","rng = jax.random.PRNGKey(rand_idx)\n","rollout = []\n","modify_scene_fns = []\n","\n","rewards = []\n","foot_vel = []\n","rews = []\n","contact = []\n","command = jp.array([x_vel, y_vel, yaw_vel])\n","\n","state = jit_reset(rng)\n","\n","for i in range(env_cfg.episode_length):\n","    act_rng, rng = jax.random.split(rng)\n","    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n","    state = jit_step(state, ctrl)\n","\n","    rews.append(\n","        {k: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n","    )\n","    rollout.append(state)\n","    rewards.append(\n","        {k[7:]: v for k, v in state.metrics.items() if k.startswith(\"reward/\")}\n","    )\n","\n","    # Render the joystick command as an arrow, if applicable.\n","    if \"joystick\" in env_name.lower():\n","        state.info[\"command\"] = command\n","        xyz = np.array(state.data.xpos[env._torso_body_id])\n","        xyz += np.array([0, 0, 0.2])\n","        x_axis = state.data.xmat[env._torso_body_id, 0]\n","        yaw = -np.arctan2(x_axis[1], x_axis[0])\n","        modify_scene_fns.append(\n","            functools.partial(\n","                draw_joystick_command,\n","                cmd=state.info[\"command\"],\n","                xyz=xyz,\n","                theta=yaw,\n","                scl=abs(state.info[\"command\"][0])\n","                / env_cfg.command_config.a[0],\n","            )\n","        )"]},{"cell_type":"code","execution_count":49,"id":"48053c64","metadata":{"id":"48053c64","colab":{"base_uri":"https://localhost:8080/","height":521,"output_embedded_package_id":"1d5G97Yr7oxF9obWn4RbqwUR7eDEjNNd5"},"executionInfo":{"status":"ok","timestamp":1760057847240,"user_tz":240,"elapsed":15974,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"fdba6a2a-e930-4434-9e90-87b2087b78fe"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Render the rollout\n","\n","render_every = 2\n","fps = 1.0 / eval_env.dt / render_every\n","traj = rollout[::render_every]\n","\n","if \"joystick\" in env_name.lower():\n","    mod_fns = modify_scene_fns[::render_every]\n","else:\n","    mod_fns = None\n","\n","scene_option = mujoco.MjvOption()\n","scene_option.geomgroup[2] = True\n","scene_option.geomgroup[3] = False\n","scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTPOINT] = True\n","scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = False\n","scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = False\n","\n","frames = eval_env.render(\n","    traj,\n","    camera=\"track\",\n","    scene_option=scene_option,\n","    width=640,\n","    height=480,\n","    modify_scene_fns=mod_fns,\n",")\n","media.show_video(frames, fps=fps, loop=False)"]},{"cell_type":"markdown","id":"7f98de3a","metadata":{"id":"7f98de3a"},"source":["## Environment Modification Code\n","\n","`TODO(student):` Here, write any code you need to to compare your \"modified environment\" to a baseline environment above. Create some visualization about what changed/didn't change. Below, write a brief explaination of the effect your modification had, and if it corresponds to your hypothesis **(40 points)**\n","\n"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import jax\n","\n","print(\"=\"*60)\n","print(\"ENV MODIFICATION: Noise Level Experiment (Go1JoystickFlatTerrain)\")\n","print(\"=\"*60)\n","\n","# --- Experiment parameters ---\n","noise_levels = [0.0, 0.5, 1.0]   # low, medium, high\n","num_rollouts = 3\n","episode_length = 100  # shorter for fast testing\n","\n","baseline_rewards = {n: [] for n in noise_levels}\n","\n","for noise in noise_levels:\n","    # --- create environment with specific noise ---\n","    test_cfg = registry.get_default_config(\"Go1JoystickFlatTerrain\")\n","    test_cfg.noise_config.level = noise\n","    test_cfg.episode_length = episode_length\n","    test_env = registry.load(\"Go1JoystickFlatTerrain\", config=test_cfg)\n","\n","    jit_reset = jax.jit(test_env.reset)\n","    jit_step  = jax.jit(test_env.step)\n","\n","    for rollout_idx in range(num_rollouts):\n","        rng = jax.random.PRNGKey(rollout_idx + int(noise*1000))\n","        state = jit_reset(rng)\n","\n","        total_reward = 0.0\n","        for step_idx in range(episode_length):\n","            act_rng, rng = jax.random.split(rng)\n","            ctrl, _ = jit_inference_fn(state.obs, act_rng)\n","            state = jit_step(state, ctrl)\n","            total_reward += float(state.reward)\n","\n","        baseline_rewards[noise].append(total_reward)\n","\n","    mean_r = np.mean(baseline_rewards[noise])\n","    std_r  = np.std(baseline_rewards[noise])\n","    print(f\"Noise Level {noise}: Mean Reward = {mean_r:.2f} ± {std_r:.2f}\")\n","\n","# --- Visualization ---\n","fig, ax = plt.subplots(figsize=(7,5))\n","means = [np.mean(baseline_rewards[n]) for n in noise_levels]\n","stds  = [np.std(baseline_rewards[n]) for n in noise_levels]\n","\n","ax.errorbar(noise_levels, means, yerr=stds, fmt='-o', capsize=5, color='purple')\n","ax.set_xlabel(\"Noise Level\", fontsize=12)\n","ax.set_ylabel(\"Episode Reward\", fontsize=12)\n","ax.set_title(\"Policy Performance vs Noise Level\", fontsize=14, fontweight='bold')\n","ax.grid(True)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":596},"id":"iygWaRVEaN9P","executionInfo":{"status":"ok","timestamp":1760058630888,"user_tz":240,"elapsed":293209,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"a4b5cdf3-4d97-45e9-904d-3c722bec0770"},"id":"iygWaRVEaN9P","execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","ENV MODIFICATION: Noise Level Experiment (Go1JoystickFlatTerrain)\n","============================================================\n","Noise Level 0.0: Mean Reward = 2.50 ± 0.21\n","Noise Level 0.5: Mean Reward = 2.91 ± 0.12\n","Noise Level 1.0: Mean Reward = 2.82 ± 0.35\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 700x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","id":"395c1932","metadata":{"id":"395c1932"},"source":["# Using Other External RL Libraries (rsl_rl)\n","\n","In this section, we'll investigate how to interface with *different* RL libraries beyond the BRAX RL implimenations we've experimented with in mujoco_playground. Specificially, we'll focus on the popular [RSL_RL](https://github.com/leggedrobotics/rsl_rl) library, a fast and simple implimentation of RL algorithms designed to run fully on the GPU. It also provides nice logging/visualization. It's built to be small, modular, and relatively easy to read. In the next lab, we'll extend on the codebase to enable some more complicated learning architectures.\n","\n","The `RSL_RL` reinforcment learning library is built entirely using [PyTorch](https://pytorch.org/). Notably this is different than the [JAX](https://github.com/jax-ml/jax)-based RL library bundled with mujoco-playground. To use RSL_RL, we'll need to make sure our environment conforms to the interface RSL_RL expects. We'll make an environment *wrapper* (parent class) that exposes observations, rewards, and resets as PyTorch tensors, supports batched execution for many environments in parallel, and keeps all data on the GPU to avoid unnecessary slowdowns. Additionally, we'll change some necessary configuration files.\n"]},{"cell_type":"markdown","id":"df98a5e4","metadata":{"id":"df98a5e4"},"source":["We'll train on the `CheetahRun` environment from the `DeepMindControlSuite`. Below, we setup some global variables"]},{"cell_type":"code","execution_count":66,"id":"4c83367d","metadata":{"id":"4c83367d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760059290570,"user_tz":240,"elapsed":61,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"3ecfb18f-f8fa-4882-e36a-cf29235e18a9"},"outputs":[{"output_type":"stream","name":"stdout","text":[" All Envs in MujocoPlayground: AcrobotSwingup, AcrobotSwingupSparse, BallInCup, CartpoleBalance, CartpoleBalanceSparse, CartpoleSwingup, CartpoleSwingupSparse, CheetahRun, FingerSpin, FingerTurnEasy, FingerTurnHard, FishSwim, HopperHop, HopperStand, HumanoidStand, HumanoidWalk, HumanoidRun, PendulumSwingup, PointMass, ReacherEasy, ReacherHard, SwimmerSwimmer6, WalkerRun, WalkerStand, WalkerWalk, ApolloJoystickFlatTerrain, BarkourJoystick, BerkeleyHumanoidJoystickFlatTerrain, BerkeleyHumanoidJoystickRoughTerrain, G1JoystickFlatTerrain, G1JoystickRoughTerrain, Go1JoystickFlatTerrain, Go1JoystickRoughTerrain, Go1Getup, Go1Handstand, Go1Footstand, H1InplaceGaitTracking, H1JoystickGaitTracking, Op3Joystick, SpotFlatTerrainJoystick, SpotGetup, SpotJoystickGaitTracking, T1JoystickFlatTerrain, T1JoystickRoughTerrain, AlohaHandOver, AlohaSinglePegInsertion, PandaPickCube, PandaPickCubeOrientation, PandaPickCubeCartesian, PandaOpenCabinet, PandaRobotiqPushCube, LeapCubeReorient, LeapCubeRotateZAxis\n"]}],"source":["# Setup some global variables\n","_ENV_NAME = \"CheetahRun\" # from deepmind_control_suite\n","_LOAD_RUN_NAME = None\n","_CHECKPOINT_NUM = -1\n","_PLAY_ONLY = False\n","_USE_WANDB = False\n","_SUFFIX = None\n","_SEED = 1\n","_NUM_ENVS = 4096\n","_DEVICE = \"cuda:0\"\n","\n","device=_DEVICE\n","\n","print(f\" All Envs in MujocoPlayground: {', '.join(mujoco_playground.registry.ALL_ENVS)}\")"]},{"cell_type":"markdown","id":"375c94d2","metadata":{"id":"375c94d2"},"source":["### Modify Some Configuration Files"]},{"cell_type":"markdown","id":"fdd8e7a1","metadata":{"id":"fdd8e7a1"},"source":["First, we'll define a configuration file for our CheetahRun Environment. We'll set up an `ActorCritic` policy using the `PPO` algorithm.\n","\n","`TODO(student):` Fill in the TODOs below. To find out what parameters are necessary do some sleuthing around the [rsl_rl repo](https://github.com/leggedrobotics/rsl_rl/tree/750e84566d91877a8bbabc7971578be24429bca8)"]},{"cell_type":"code","execution_count":67,"id":"82701722","metadata":{"id":"82701722","executionInfo":{"status":"ok","timestamp":1760059330204,"user_tz":240,"elapsed":118,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}}},"outputs":[],"source":["def rsl_rl_cheetah_config() -> config_dict.ConfigDict:\n","  \"\"\"Returns tuned RSL-RL PPO config for the CheetahRun environment.\"\"\"\n","\n","  rl_config = config_dict.create(\n","      seed=1,\n","      runner_class_name=\"OnPolicyRunner\",\n","      policy=config_dict.create(\n","          class_name='ActorCritic',\n","          init_noise_std=1.0,\n","          actor_hidden_dims=[256, 256, 256],\n","          critic_hidden_dims=[256, 256, 256],\n","          activation='elu',\n","      ),\n","      algorithm=config_dict.create(\n","          class_name='PPO',  # <-- THIS WAS MISSING!\n","          value_loss_coef=1.0,\n","          use_clipped_value_loss=True,\n","          clip_param=0.2,\n","          entropy_coef=0.01,\n","          num_learning_epochs=5,\n","          num_mini_batches=4,\n","          learning_rate=1.0e-3,\n","          schedule='adaptive',\n","          gamma=0.99,\n","          lam=0.95,\n","          desired_kl=0.01,\n","          max_grad_norm=1.0,\n","      ),\n","      num_steps_per_env=24,  # per iteration\n","      max_iterations=100000,  # number of policy updates\n","      empirical_normalization=True,\n","\n","      # logging\n","      save_interval=50,  # check for potential saves every this many iterations\n","      experiment_name=\"eecs598_dm_control\",\n","      run_name=\"\",\n","\n","      # load and resume\n","      resume=False,\n","      load_run=\"-1\",  # -1 = last run\n","      checkpoint=-1,  # -1 = last saved model\n","      resume_path=None,  # updated from load_run and chkpt\n","  )\n","\n","  return rl_config"]},{"cell_type":"markdown","id":"a29e2f95","metadata":{"id":"a29e2f95"},"source":["Inspect the Environment Config and the RSL_RL Training Config."]},{"cell_type":"code","execution_count":68,"id":"a80ee484","metadata":{"id":"a80ee484","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760059332245,"user_tz":240,"elapsed":93,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"15533c14-9daf-4324-a31b-4ec50fb82ab9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Experiment name: CheetahRun-20251010-012212\n","-------- Environment config --------\n","action_repeat: 1\n","ctrl_dt: 0.01\n","episode_length: 1000\n","sim_dt: 0.01\n","vision: false\n","\n","-------- RSL_RL config --------\n","algorithm:\n","  class_name: PPO\n","  clip_param: 0.2\n","  desired_kl: 0.01\n","  entropy_coef: 0.01\n","  gamma: 0.99\n","  lam: 0.95\n","  learning_rate: 0.001\n","  max_grad_norm: 1.0\n","  num_learning_epochs: 5\n","  num_mini_batches: 4\n","  schedule: adaptive\n","  use_clipped_value_loss: true\n","  value_loss_coef: 1.0\n","checkpoint: -1\n","empirical_normalization: true\n","experiment_name: eecs598_dm_control\n","load_run: '-1'\n","max_iterations: 100000\n","num_steps_per_env: 24\n","policy:\n","  activation: elu\n","  actor_hidden_dims:\n","  - 256\n","  - 256\n","  - 256\n","  class_name: ActorCritic\n","  critic_hidden_dims:\n","  - 256\n","  - 256\n","  - 256\n","  init_noise_std: 1.0\n","resume: false\n","resume_path: null\n","run_name: CheetahRun-20251010-012212\n","runner_class_name: OnPolicyRunner\n","save_interval: 50\n","seed: 1\n","\n"]}],"source":["# 1. Come up with an experiment name...\n","now = datetime.now()\n","timestamp = now.strftime(\"%Y%m%d-%H%M%S\")\n","exp_name = f\"{_ENV_NAME}-{timestamp}\"\n","if _SUFFIX is not None:\n","    exp_name += f\"-{_SUFFIX}\"\n","print(f\"Experiment name: {exp_name}\")\n","\n","\n","# 2. Load default CheetahRun environment config from registry\n","env_cfg = registry.get_default_config(_ENV_NAME)\n","print(f\"-------- Environment config --------\\n{env_cfg}\")\n","\n","# 3. Load the RSL_RL Training Config\n","train_cfg = rsl_rl_cheetah_config()\n","\n","train_cfg.seed = _SEED\n","train_cfg.run_name = exp_name\n","train_cfg.resume = _LOAD_RUN_NAME is not None\n","train_cfg.load_run = _LOAD_RUN_NAME if _LOAD_RUN_NAME else \"-1\"\n","train_cfg.checkpoint = _CHECKPOINT_NUM\n","\n","train_cfg_dict = train_cfg.to_dict()\n","print(f\"-------- RSL_RL config --------\\n{train_cfg}\")\n"]},{"cell_type":"markdown","id":"74937ba1","metadata":{"id":"74937ba1"},"source":["First, we'll do some simple setup for logging and checkpoints and load the base environment."]},{"cell_type":"code","execution_count":69,"id":"00b6f529","metadata":{"id":"00b6f529","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760059336859,"user_tz":240,"elapsed":212,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"2e3e546e-80ad-4d20-a771-848ff902cfd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Logs are being stored in: /content/drive/MyDrive/CSE_598/lab6-playground-rsl/lab6-playground-rsl/logs/CheetahRun-20251010-012212\n","Checkpoint path: /content/drive/MyDrive/CSE_598/lab6-playground-rsl/lab6-playground-rsl/logs/CheetahRun-20251010-012212/checkpoints\n"]}],"source":["# Logging directory\n","logdir = os.path.abspath(os.path.join(\"logs\", exp_name))\n","os.makedirs(logdir, exist_ok=True)\n","print(f\"Logs are being stored in: {logdir}\")\n","\n","# Checkpoint directory\n","ckpt_path = os.path.join(logdir, \"checkpoints\")\n","os.makedirs(ckpt_path, exist_ok=True)\n","print(f\"Checkpoint path: {ckpt_path}\")\n","\n","# Save environment config to JSON\n","with open(\n","    os.path.join(ckpt_path, \"config.json\"), \"w\", encoding=\"utf-8\"\n",") as fp:\n","    json.dump(env_cfg.to_dict(), fp, indent=4)\n","\n","\n","# We'll store environment states during rendering\n","render_trajectory = []\n","\n","# Callback to gather states for rendering\n","def render_callback(_, state):\n","    render_trajectory.append(state)\n","\n","# Load the environment\n","raw_env = registry.load(_ENV_NAME, config=env_cfg)"]},{"cell_type":"markdown","id":"5f4e2015","metadata":{"id":"5f4e2015"},"source":["### Create an RSL_RL Wrapper"]},{"cell_type":"markdown","id":"42aac30b","metadata":{"id":"42aac30b"},"source":["The next cell wraps the MujocoPlayground environment using a new `CSE598RSLWrapper`. You can find the (almost complete) implementation of this wrapper in the `CSE598RSLWrapper.py` file.\n","\n","`TODO(student):` Fix the bugs in the `step()` function of `CSE598RSLWrapper.py`.  **(20 points)**\n"]},{"cell_type":"code","execution_count":70,"id":"95b3b14b","metadata":{"id":"95b3b14b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760059342005,"user_tz":240,"elapsed":3066,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"4d7f7fe6-7440-489a-de7d-9b697024405a"},"outputs":[{"output_type":"stream","name":"stdout","text":["obs_shape: 17\n","JITing reset and step\n","Done JITing reset and step\n"]}],"source":["from CSE598RSLWrapper import CSE598RSLWrapper\n","\n","rsl_env = CSE598RSLWrapper(\n","    raw_env,\n","    _NUM_ENVS,\n","    _SEED,\n","    env_cfg.episode_length,\n","    1,\n","    render_callback=render_callback,\n",")"]},{"cell_type":"markdown","id":"98b88cca","metadata":{"id":"98b88cca"},"source":["Create an `OnPolicyRunner`, an abstraction used to learn a policy using OnPolicy algorithms like PPO."]},{"cell_type":"code","execution_count":71,"id":"60c65146","metadata":{"id":"60c65146","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760059393797,"user_tz":240,"elapsed":51722,"user":{"displayName":"Shivam Udeshi","userId":"02020928031667840245"}},"outputId":"adc83fff-75c1-456c-b719-39de7ea15372"},"outputs":[{"output_type":"stream","name":"stdout","text":["Actor MLP: Sequential(\n","  (0): Linear(in_features=17, out_features=256, bias=True)\n","  (1): ELU(alpha=1.0)\n","  (2): Linear(in_features=256, out_features=256, bias=True)\n","  (3): ELU(alpha=1.0)\n","  (4): Linear(in_features=256, out_features=256, bias=True)\n","  (5): ELU(alpha=1.0)\n","  (6): Linear(in_features=256, out_features=6, bias=True)\n",")\n","Critic MLP: Sequential(\n","  (0): Linear(in_features=17, out_features=256, bias=True)\n","  (1): ELU(alpha=1.0)\n","  (2): Linear(in_features=256, out_features=256, bias=True)\n","  (3): ELU(alpha=1.0)\n","  (4): Linear(in_features=256, out_features=256, bias=True)\n","  (5): ELU(alpha=1.0)\n","  (6): Linear(in_features=256, out_features=1, bias=True)\n",")\n"]}],"source":["from rsl_rl.runners import OnPolicyRunner\n","runner = OnPolicyRunner(rsl_env, train_cfg_dict, logdir, device=device)"]},{"cell_type":"code","execution_count":null,"id":"4b0442e7","metadata":{"id":"4b0442e7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3a708284-c5bd-4a0e-f712-c1632171cdce"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","                       Mean reward: 359.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 24084480\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:31\n","                               ETA: 00:10:51\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 245/1000 \u001b[0m                      \n","\n","                       Computation: 155522 steps/s (collection: 0.266s, learning 0.366s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.2358\n","               Mean surrogate loss: 0.0067\n","                 Mean entropy loss: 8.9097\n","                       Mean reward: 359.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 24182784\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:03:31\n","                               ETA: 00:10:50\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 246/1000 \u001b[0m                      \n","\n","                       Computation: 149818 steps/s (collection: 0.290s, learning 0.366s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.1580\n","               Mean surrogate loss: 0.0068\n","                 Mean entropy loss: 8.9102\n","                       Mean reward: 359.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 24281088\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:03:32\n","                               ETA: 00:10:48\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 247/1000 \u001b[0m                      \n","\n","                       Computation: 144843 steps/s (collection: 0.313s, learning 0.365s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.2410\n","               Mean surrogate loss: 0.0069\n","                 Mean entropy loss: 8.9108\n","                       Mean reward: 359.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 24379392\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:03:33\n","                               ETA: 00:10:47\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 248/1000 \u001b[0m                      \n","\n","                       Computation: 145604 steps/s (collection: 0.317s, learning 0.358s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 1.1921\n","               Mean surrogate loss: 0.0082\n","                 Mean entropy loss: 8.9116\n","                       Mean reward: 359.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 24477696\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:03:33\n","                               ETA: 00:10:46\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 249/1000 \u001b[0m                      \n","\n","                       Computation: 146795 steps/s (collection: 0.301s, learning 0.368s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 1.1798\n","               Mean surrogate loss: 0.0062\n","                 Mean entropy loss: 8.9121\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 24576000\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:03:34\n","                               ETA: 00:10:44\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 250/1000 \u001b[0m                      \n","\n","                       Computation: 142893 steps/s (collection: 0.333s, learning 0.355s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.1691\n","               Mean surrogate loss: 0.0006\n","                 Mean entropy loss: 8.9118\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 24674304\n","                    Iteration time: 0.69s\n","                      Time elapsed: 00:03:35\n","                               ETA: 00:10:43\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 251/1000 \u001b[0m                      \n","\n","                       Computation: 157631 steps/s (collection: 0.270s, learning 0.353s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 0.7307\n","               Mean surrogate loss: -0.0010\n","                 Mean entropy loss: 8.9035\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 24772608\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:35\n","                               ETA: 00:10:41\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 252/1000 \u001b[0m                      \n","\n","                       Computation: 158324 steps/s (collection: 0.265s, learning 0.356s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 0.9716\n","               Mean surrogate loss: 0.0175\n","                 Mean entropy loss: 8.8904\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 24870912\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:36\n","                               ETA: 00:10:40\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 253/1000 \u001b[0m                      \n","\n","                       Computation: 159528 steps/s (collection: 0.263s, learning 0.353s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.2862\n","               Mean surrogate loss: 0.0196\n","                 Mean entropy loss: 8.8935\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 24969216\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:37\n","                               ETA: 00:10:38\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 254/1000 \u001b[0m                      \n","\n","                       Computation: 155530 steps/s (collection: 0.269s, learning 0.363s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 0.9045\n","               Mean surrogate loss: -0.0006\n","                 Mean entropy loss: 8.8937\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 25067520\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:03:37\n","                               ETA: 00:10:37\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 255/1000 \u001b[0m                      \n","\n","                       Computation: 156393 steps/s (collection: 0.273s, learning 0.356s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 0.9042\n","               Mean surrogate loss: 0.0022\n","                 Mean entropy loss: 8.8870\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 25165824\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:03:38\n","                               ETA: 00:10:35\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 256/1000 \u001b[0m                      \n","\n","                       Computation: 158815 steps/s (collection: 0.261s, learning 0.358s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.0239\n","               Mean surrogate loss: 0.0058\n","                 Mean entropy loss: 8.8795\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 25264128\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:39\n","                               ETA: 00:10:34\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 257/1000 \u001b[0m                      \n","\n","                       Computation: 160226 steps/s (collection: 0.265s, learning 0.348s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.0377\n","               Mean surrogate loss: 0.0028\n","                 Mean entropy loss: 8.8729\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 25362432\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:03:39\n","                               ETA: 00:10:32\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 258/1000 \u001b[0m                      \n","\n","                       Computation: 156843 steps/s (collection: 0.264s, learning 0.363s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.4437\n","               Mean surrogate loss: 0.0124\n","                 Mean entropy loss: 8.8590\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 25460736\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:03:40\n","                               ETA: 00:10:31\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 259/1000 \u001b[0m                      \n","\n","                       Computation: 158704 steps/s (collection: 0.270s, learning 0.349s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.3258\n","               Mean surrogate loss: 0.0034\n","                 Mean entropy loss: 8.8543\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 25559040\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:40\n","                               ETA: 00:10:29\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 260/1000 \u001b[0m                      \n","\n","                       Computation: 155354 steps/s (collection: 0.273s, learning 0.360s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.5103\n","               Mean surrogate loss: 0.0004\n","                 Mean entropy loss: 8.8458\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 25657344\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:03:41\n","                               ETA: 00:10:28\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 261/1000 \u001b[0m                      \n","\n","                       Computation: 161248 steps/s (collection: 0.263s, learning 0.347s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.9790\n","               Mean surrogate loss: 0.0106\n","                 Mean entropy loss: 8.8215\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 25755648\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:03:42\n","                               ETA: 00:10:26\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 262/1000 \u001b[0m                      \n","\n","                       Computation: 157327 steps/s (collection: 0.268s, learning 0.356s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.6695\n","               Mean surrogate loss: 0.0105\n","                 Mean entropy loss: 8.8201\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 25853952\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:42\n","                               ETA: 00:10:25\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 263/1000 \u001b[0m                      \n","\n","                       Computation: 158590 steps/s (collection: 0.271s, learning 0.349s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.5878\n","               Mean surrogate loss: 0.0078\n","                 Mean entropy loss: 8.8205\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 25952256\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:43\n","                               ETA: 00:10:23\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 264/1000 \u001b[0m                      \n","\n","                       Computation: 157372 steps/s (collection: 0.265s, learning 0.360s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.5172\n","               Mean surrogate loss: 0.0075\n","                 Mean entropy loss: 8.8206\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 26050560\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:43\n","                               ETA: 00:10:22\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 265/1000 \u001b[0m                      \n","\n","                       Computation: 157158 steps/s (collection: 0.274s, learning 0.352s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.6017\n","               Mean surrogate loss: 0.0077\n","                 Mean entropy loss: 8.8201\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 26148864\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:03:44\n","                               ETA: 00:10:20\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 266/1000 \u001b[0m                      \n","\n","                       Computation: 153002 steps/s (collection: 0.271s, learning 0.371s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.6437\n","               Mean surrogate loss: 0.0088\n","                 Mean entropy loss: 8.8198\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 26247168\n","                    Iteration time: 0.64s\n","                      Time elapsed: 00:03:45\n","                               ETA: 00:10:19\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 267/1000 \u001b[0m                      \n","\n","                       Computation: 149456 steps/s (collection: 0.304s, learning 0.354s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.5013\n","               Mean surrogate loss: 0.0070\n","                 Mean entropy loss: 8.8203\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 26345472\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:03:45\n","                               ETA: 00:10:17\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 268/1000 \u001b[0m                      \n","\n","                       Computation: 143866 steps/s (collection: 0.318s, learning 0.365s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.5621\n","               Mean surrogate loss: 0.0089\n","                 Mean entropy loss: 8.8207\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 26443776\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:03:46\n","                               ETA: 00:10:16\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 269/1000 \u001b[0m                      \n","\n","                       Computation: 146572 steps/s (collection: 0.314s, learning 0.356s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.7085\n","               Mean surrogate loss: 0.0080\n","                 Mean entropy loss: 8.8212\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 26542080\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:03:47\n","                               ETA: 00:10:15\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 270/1000 \u001b[0m                      \n","\n","                       Computation: 140487 steps/s (collection: 0.333s, learning 0.367s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.5234\n","               Mean surrogate loss: 0.0091\n","                 Mean entropy loss: 8.8218\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 26640384\n","                    Iteration time: 0.70s\n","                      Time elapsed: 00:03:47\n","                               ETA: 00:10:14\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 271/1000 \u001b[0m                      \n","\n","                       Computation: 154351 steps/s (collection: 0.288s, learning 0.349s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.6164\n","               Mean surrogate loss: 0.0060\n","                 Mean entropy loss: 8.8225\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 26738688\n","                    Iteration time: 0.64s\n","                      Time elapsed: 00:03:48\n","                               ETA: 00:10:12\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 272/1000 \u001b[0m                      \n","\n","                       Computation: 157402 steps/s (collection: 0.270s, learning 0.355s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.5124\n","               Mean surrogate loss: 0.0082\n","                 Mean entropy loss: 8.8232\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 26836992\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:49\n","                               ETA: 00:10:11\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 273/1000 \u001b[0m                      \n","\n","                       Computation: 157021 steps/s (collection: 0.279s, learning 0.347s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.5390\n","               Mean surrogate loss: 0.0071\n","                 Mean entropy loss: 8.8235\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 26935296\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:03:49\n","                               ETA: 00:10:09\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 274/1000 \u001b[0m                      \n","\n","                       Computation: 155941 steps/s (collection: 0.270s, learning 0.361s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.6375\n","               Mean surrogate loss: 0.0055\n","                 Mean entropy loss: 8.8244\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 27033600\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:03:50\n","                               ETA: 00:10:08\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 275/1000 \u001b[0m                      \n","\n","                       Computation: 162386 steps/s (collection: 0.262s, learning 0.344s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.6998\n","               Mean surrogate loss: 0.0003\n","                 Mean entropy loss: 8.8293\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 27131904\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:03:51\n","                               ETA: 00:10:07\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 276/1000 \u001b[0m                      \n","\n","                       Computation: 158127 steps/s (collection: 0.265s, learning 0.357s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 3.5622\n","               Mean surrogate loss: 0.0086\n","                 Mean entropy loss: 8.8361\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 27230208\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:51\n","                               ETA: 00:10:05\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 277/1000 \u001b[0m                      \n","\n","                       Computation: 161287 steps/s (collection: 0.263s, learning 0.347s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 2.1441\n","               Mean surrogate loss: 0.0003\n","                 Mean entropy loss: 8.8348\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 27328512\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:03:52\n","                               ETA: 00:10:04\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 278/1000 \u001b[0m                      \n","\n","                       Computation: 155757 steps/s (collection: 0.273s, learning 0.358s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 2.0954\n","               Mean surrogate loss: 0.0017\n","                 Mean entropy loss: 8.8321\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 27426816\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:03:52\n","                               ETA: 00:10:02\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 279/1000 \u001b[0m                      \n","\n","                       Computation: 159629 steps/s (collection: 0.265s, learning 0.350s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.9506\n","               Mean surrogate loss: 0.0059\n","                 Mean entropy loss: 8.8376\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 27525120\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:53\n","                               ETA: 00:10:01\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 280/1000 \u001b[0m                      \n","\n","                       Computation: 159238 steps/s (collection: 0.269s, learning 0.349s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.8171\n","               Mean surrogate loss: 0.0015\n","                 Mean entropy loss: 8.8364\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 27623424\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:54\n","                               ETA: 00:10:00\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 281/1000 \u001b[0m                      \n","\n","                       Computation: 157136 steps/s (collection: 0.267s, learning 0.359s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.8886\n","               Mean surrogate loss: 0.0051\n","                 Mean entropy loss: 8.8276\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 27721728\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:03:54\n","                               ETA: 00:09:58\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 282/1000 \u001b[0m                      \n","\n","                       Computation: 160825 steps/s (collection: 0.262s, learning 0.350s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.5662\n","               Mean surrogate loss: 0.0011\n","                 Mean entropy loss: 8.8217\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 27820032\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:03:55\n","                               ETA: 00:09:57\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 283/1000 \u001b[0m                      \n","\n","                       Computation: 156939 steps/s (collection: 0.271s, learning 0.355s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 1.7494\n","               Mean surrogate loss: 0.0095\n","                 Mean entropy loss: 8.8170\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 27918336\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:03:56\n","                               ETA: 00:09:55\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 284/1000 \u001b[0m                      \n","\n","                       Computation: 160535 steps/s (collection: 0.264s, learning 0.348s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 2.2525\n","               Mean surrogate loss: 0.0069\n","                 Mean entropy loss: 8.8195\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 28016640\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:03:56\n","                               ETA: 00:09:54\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 285/1000 \u001b[0m                      \n","\n","                       Computation: 159005 steps/s (collection: 0.266s, learning 0.352s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 2.3106\n","               Mean surrogate loss: 0.0191\n","                 Mean entropy loss: 8.8218\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 28114944\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:57\n","                               ETA: 00:09:53\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 286/1000 \u001b[0m                      \n","\n","                       Computation: 158469 steps/s (collection: 0.266s, learning 0.355s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 2.4501\n","               Mean surrogate loss: 0.0102\n","                 Mean entropy loss: 8.8223\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 28213248\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:03:57\n","                               ETA: 00:09:51\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 287/1000 \u001b[0m                      \n","\n","                       Computation: 149777 steps/s (collection: 0.295s, learning 0.361s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 2.1784\n","               Mean surrogate loss: 0.0095\n","                 Mean entropy loss: 8.8230\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 28311552\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:03:58\n","                               ETA: 00:09:50\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 288/1000 \u001b[0m                      \n","\n","                       Computation: 144531 steps/s (collection: 0.320s, learning 0.360s)\n","             Mean action noise std: 1.07\n","          Mean value_function loss: 2.4153\n","               Mean surrogate loss: 0.0038\n","                 Mean entropy loss: 8.8241\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 28409856\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:03:59\n","                               ETA: 00:09:49\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 289/1000 \u001b[0m                      \n","\n","                       Computation: 147059 steps/s (collection: 0.308s, learning 0.360s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 3.3488\n","               Mean surrogate loss: 0.0049\n","                 Mean entropy loss: 8.8268\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 28508160\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:03:59\n","                               ETA: 00:09:48\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 290/1000 \u001b[0m                      \n","\n","                       Computation: 148824 steps/s (collection: 0.304s, learning 0.357s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 3.3971\n","               Mean surrogate loss: 0.0063\n","                 Mean entropy loss: 8.8318\n","                       Mean reward: 386.60\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 28606464\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:04:00\n","                               ETA: 00:09:46\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 291/1000 \u001b[0m                      \n","\n","                       Computation: 143895 steps/s (collection: 0.331s, learning 0.352s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.3322\n","               Mean surrogate loss: 0.0069\n","                 Mean entropy loss: 8.8352\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 28704768\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:04:01\n","                               ETA: 00:09:45\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 292/1000 \u001b[0m                      \n","\n","                       Computation: 157207 steps/s (collection: 0.267s, learning 0.358s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.0724\n","               Mean surrogate loss: 0.0004\n","                 Mean entropy loss: 8.8340\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 28803072\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:01\n","                               ETA: 00:09:44\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 293/1000 \u001b[0m                      \n","\n","                       Computation: 159342 steps/s (collection: 0.269s, learning 0.348s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 1.4595\n","               Mean surrogate loss: 0.0062\n","                 Mean entropy loss: 8.8298\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 28901376\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:02\n","                               ETA: 00:09:43\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 294/1000 \u001b[0m                      \n","\n","                       Computation: 157661 steps/s (collection: 0.265s, learning 0.359s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 3.0082\n","               Mean surrogate loss: 0.0076\n","                 Mean entropy loss: 8.8293\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 28999680\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:03\n","                               ETA: 00:09:41\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 295/1000 \u001b[0m                      \n","\n","                       Computation: 159386 steps/s (collection: 0.269s, learning 0.348s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 2.5836\n","               Mean surrogate loss: 0.0061\n","                 Mean entropy loss: 8.8298\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 29097984\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:03\n","                               ETA: 00:09:40\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 296/1000 \u001b[0m                      \n","\n","                       Computation: 157214 steps/s (collection: 0.271s, learning 0.354s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 2.0448\n","               Mean surrogate loss: 0.0107\n","                 Mean entropy loss: 8.8305\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 29196288\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:04\n","                               ETA: 00:09:39\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 297/1000 \u001b[0m                      \n","\n","                       Computation: 159107 steps/s (collection: 0.269s, learning 0.348s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 2.8567\n","               Mean surrogate loss: 0.0043\n","                 Mean entropy loss: 8.8311\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 29294592\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:04\n","                               ETA: 00:09:37\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 298/1000 \u001b[0m                      \n","\n","                       Computation: 157045 steps/s (collection: 0.269s, learning 0.357s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 3.1536\n","               Mean surrogate loss: 0.0119\n","                 Mean entropy loss: 8.8318\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 29392896\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:05\n","                               ETA: 00:09:36\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 299/1000 \u001b[0m                      \n","\n","                       Computation: 160356 steps/s (collection: 0.261s, learning 0.352s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 3.8146\n","               Mean surrogate loss: 0.0040\n","                 Mean entropy loss: 8.8331\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 29491200\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:04:06\n","                               ETA: 00:09:35\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 300/1000 \u001b[0m                      \n","\n","                       Computation: 159207 steps/s (collection: 0.265s, learning 0.353s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 5.1262\n","               Mean surrogate loss: 0.0035\n","                 Mean entropy loss: 8.8367\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 29589504\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:06\n","                               ETA: 00:09:34\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 301/1000 \u001b[0m                      \n","\n","                       Computation: 158473 steps/s (collection: 0.267s, learning 0.354s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 5.0028\n","               Mean surrogate loss: 0.0069\n","                 Mean entropy loss: 8.8419\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 29687808\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:07\n","                               ETA: 00:09:32\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 302/1000 \u001b[0m                      \n","\n","                       Computation: 157441 steps/s (collection: 0.265s, learning 0.359s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 4.6530\n","               Mean surrogate loss: 0.0050\n","                 Mean entropy loss: 8.8512\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 29786112\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:08\n","                               ETA: 00:09:31\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 303/1000 \u001b[0m                      \n","\n","                       Computation: 155773 steps/s (collection: 0.284s, learning 0.347s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 5.2887\n","               Mean surrogate loss: 0.0047\n","                 Mean entropy loss: 8.8604\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 29884416\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:08\n","                               ETA: 00:09:30\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 304/1000 \u001b[0m                      \n","\n","                       Computation: 157183 steps/s (collection: 0.263s, learning 0.362s)\n","             Mean action noise std: 1.08\n","          Mean value_function loss: 4.6050\n","               Mean surrogate loss: 0.0036\n","                 Mean entropy loss: 8.8698\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 29982720\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:09\n","                               ETA: 00:09:28\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 305/1000 \u001b[0m                      \n","\n","                       Computation: 160500 steps/s (collection: 0.265s, learning 0.348s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 4.1376\n","               Mean surrogate loss: 0.0016\n","                 Mean entropy loss: 8.8761\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 30081024\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:04:09\n","                               ETA: 00:09:27\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 306/1000 \u001b[0m                      \n","\n","                       Computation: 156692 steps/s (collection: 0.274s, learning 0.353s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 4.7364\n","               Mean surrogate loss: 0.0018\n","                 Mean entropy loss: 8.8835\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 30179328\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:10\n","                               ETA: 00:09:26\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 307/1000 \u001b[0m                      \n","\n","                       Computation: 156708 steps/s (collection: 0.264s, learning 0.364s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 3.8684\n","               Mean surrogate loss: 0.0050\n","                 Mean entropy loss: 8.8878\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 30277632\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:11\n","                               ETA: 00:09:25\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 308/1000 \u001b[0m                      \n","\n","                       Computation: 148723 steps/s (collection: 0.306s, learning 0.354s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 3.7099\n","               Mean surrogate loss: 0.0045\n","                 Mean entropy loss: 8.8890\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 30375936\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:04:11\n","                               ETA: 00:09:24\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 309/1000 \u001b[0m                      \n","\n","                       Computation: 143534 steps/s (collection: 0.327s, learning 0.358s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 3.1065\n","               Mean surrogate loss: 0.0069\n","                 Mean entropy loss: 8.8895\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 30474240\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:04:12\n","                               ETA: 00:09:22\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 310/1000 \u001b[0m                      \n","\n","                       Computation: 148178 steps/s (collection: 0.309s, learning 0.355s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 2.5947\n","               Mean surrogate loss: 0.0037\n","                 Mean entropy loss: 8.8904\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 30572544\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:04:13\n","                               ETA: 00:09:21\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 311/1000 \u001b[0m                      \n","\n","                       Computation: 140717 steps/s (collection: 0.331s, learning 0.368s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 2.5278\n","               Mean surrogate loss: 0.0007\n","                 Mean entropy loss: 8.8945\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 30670848\n","                    Iteration time: 0.70s\n","                      Time elapsed: 00:04:13\n","                               ETA: 00:09:20\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 312/1000 \u001b[0m                      \n","\n","                       Computation: 157412 steps/s (collection: 0.267s, learning 0.357s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 3.1122\n","               Mean surrogate loss: 0.0024\n","                 Mean entropy loss: 8.9005\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 30769152\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:14\n","                               ETA: 00:09:19\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 313/1000 \u001b[0m                      \n","\n","                       Computation: 158383 steps/s (collection: 0.271s, learning 0.349s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 2.2542\n","               Mean surrogate loss: 0.0007\n","                 Mean entropy loss: 8.9044\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 30867456\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:15\n","                               ETA: 00:09:18\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 314/1000 \u001b[0m                      \n","\n","                       Computation: 156020 steps/s (collection: 0.270s, learning 0.360s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 2.3088\n","               Mean surrogate loss: -0.0010\n","                 Mean entropy loss: 8.9143\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 30965760\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:15\n","                               ETA: 00:09:17\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 315/1000 \u001b[0m                      \n","\n","                       Computation: 159109 steps/s (collection: 0.266s, learning 0.351s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 2.5238\n","               Mean surrogate loss: 0.0183\n","                 Mean entropy loss: 8.9244\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 31064064\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:16\n","                               ETA: 00:09:15\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 316/1000 \u001b[0m                      \n","\n","                       Computation: 157624 steps/s (collection: 0.269s, learning 0.355s)\n","             Mean action noise std: 1.09\n","          Mean value_function loss: 2.0867\n","               Mean surrogate loss: 0.0023\n","                 Mean entropy loss: 8.9260\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 31162368\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:17\n","                               ETA: 00:09:14\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 317/1000 \u001b[0m                      \n","\n","                       Computation: 158510 steps/s (collection: 0.271s, learning 0.349s)\n","             Mean action noise std: 1.10\n","          Mean value_function loss: 1.8539\n","               Mean surrogate loss: -0.0006\n","                 Mean entropy loss: 8.9305\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 31260672\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:17\n","                               ETA: 00:09:13\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 318/1000 \u001b[0m                      \n","\n","                       Computation: 157355 steps/s (collection: 0.270s, learning 0.355s)\n","             Mean action noise std: 1.10\n","          Mean value_function loss: 2.6054\n","               Mean surrogate loss: 0.0036\n","                 Mean entropy loss: 8.9435\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 31358976\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:18\n","                               ETA: 00:09:12\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 319/1000 \u001b[0m                      \n","\n","                       Computation: 158919 steps/s (collection: 0.272s, learning 0.347s)\n","             Mean action noise std: 1.10\n","          Mean value_function loss: 2.8015\n","               Mean surrogate loss: 0.0017\n","                 Mean entropy loss: 8.9514\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 31457280\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:18\n","                               ETA: 00:09:10\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 320/1000 \u001b[0m                      \n","\n","                       Computation: 158717 steps/s (collection: 0.260s, learning 0.359s)\n","             Mean action noise std: 1.10\n","          Mean value_function loss: 2.2159\n","               Mean surrogate loss: 0.0027\n","                 Mean entropy loss: 8.9550\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 31555584\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:19\n","                               ETA: 00:09:09\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 321/1000 \u001b[0m                      \n","\n","                       Computation: 155407 steps/s (collection: 0.267s, learning 0.365s)\n","             Mean action noise std: 1.10\n","          Mean value_function loss: 1.8409\n","               Mean surrogate loss: 0.0008\n","                 Mean entropy loss: 8.9621\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 31653888\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:20\n","                               ETA: 00:09:08\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 322/1000 \u001b[0m                      \n","\n","                       Computation: 103116 steps/s (collection: 0.492s, learning 0.462s)\n","             Mean action noise std: 1.10\n","          Mean value_function loss: 1.8596\n","               Mean surrogate loss: -0.0002\n","                 Mean entropy loss: 8.9694\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 31752192\n","                    Iteration time: 0.95s\n","                      Time elapsed: 00:04:21\n","                               ETA: 00:09:08\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 323/1000 \u001b[0m                      \n","\n","                       Computation: 124585 steps/s (collection: 0.418s, learning 0.371s)\n","             Mean action noise std: 1.10\n","          Mean value_function loss: 1.9335\n","               Mean surrogate loss: -0.0010\n","                 Mean entropy loss: 8.9721\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 31850496\n","                    Iteration time: 0.79s\n","                      Time elapsed: 00:04:21\n","                               ETA: 00:09:07\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 324/1000 \u001b[0m                      \n","\n","                       Computation: 159048 steps/s (collection: 0.266s, learning 0.352s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 2.0662\n","               Mean surrogate loss: 0.0015\n","                 Mean entropy loss: 8.9764\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 31948800\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:22\n","                               ETA: 00:09:06\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 325/1000 \u001b[0m                      \n","\n","                       Computation: 157942 steps/s (collection: 0.274s, learning 0.349s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 2.4135\n","               Mean surrogate loss: 0.0463\n","                 Mean entropy loss: 8.9796\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 32047104\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:23\n","                               ETA: 00:09:04\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 326/1000 \u001b[0m                      \n","\n","                       Computation: 158175 steps/s (collection: 0.263s, learning 0.358s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 1.8955\n","               Mean surrogate loss: 0.0016\n","                 Mean entropy loss: 8.9803\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 32145408\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:23\n","                               ETA: 00:09:03\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 327/1000 \u001b[0m                      \n","\n","                       Computation: 152041 steps/s (collection: 0.284s, learning 0.363s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 1.7820\n","               Mean surrogate loss: -0.0016\n","                 Mean entropy loss: 8.9809\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 32243712\n","                    Iteration time: 0.65s\n","                      Time elapsed: 00:04:24\n","                               ETA: 00:09:02\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 328/1000 \u001b[0m                      \n","\n","                       Computation: 147581 steps/s (collection: 0.298s, learning 0.368s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 1.8255\n","               Mean surrogate loss: 0.0002\n","                 Mean entropy loss: 8.9805\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 32342016\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:04:25\n","                               ETA: 00:09:01\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 329/1000 \u001b[0m                      \n","\n","                       Computation: 148087 steps/s (collection: 0.303s, learning 0.360s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 1.7556\n","               Mean surrogate loss: -0.0007\n","                 Mean entropy loss: 8.9782\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 32440320\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:04:25\n","                               ETA: 00:09:00\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 330/1000 \u001b[0m                      \n","\n","                       Computation: 145938 steps/s (collection: 0.314s, learning 0.360s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 1.4389\n","               Mean surrogate loss: 0.0012\n","                 Mean entropy loss: 8.9782\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 32538624\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:04:26\n","                               ETA: 00:08:59\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 331/1000 \u001b[0m                      \n","\n","                       Computation: 138342 steps/s (collection: 0.355s, learning 0.356s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 1.4036\n","               Mean surrogate loss: 0.0055\n","                 Mean entropy loss: 8.9863\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 32636928\n","                    Iteration time: 0.71s\n","                      Time elapsed: 00:04:27\n","                               ETA: 00:08:58\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 332/1000 \u001b[0m                      \n","\n","                       Computation: 158071 steps/s (collection: 0.263s, learning 0.359s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 1.2993\n","               Mean surrogate loss: 0.0054\n","                 Mean entropy loss: 8.9866\n","                       Mean reward: 451.43\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 32735232\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:27\n","                               ETA: 00:08:57\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 333/1000 \u001b[0m                      \n","\n","                       Computation: 156866 steps/s (collection: 0.279s, learning 0.348s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 0.9773\n","               Mean surrogate loss: 0.0045\n","                 Mean entropy loss: 8.9868\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 32833536\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:28\n","                               ETA: 00:08:55\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 334/1000 \u001b[0m                      \n","\n","                       Computation: 156636 steps/s (collection: 0.268s, learning 0.359s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 1.2564\n","               Mean surrogate loss: -0.0007\n","                 Mean entropy loss: 8.9854\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 32931840\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:28\n","                               ETA: 00:08:54\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 335/1000 \u001b[0m                      \n","\n","                       Computation: 158408 steps/s (collection: 0.271s, learning 0.349s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 1.4886\n","               Mean surrogate loss: -0.0007\n","                 Mean entropy loss: 8.9794\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 33030144\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:29\n","                               ETA: 00:08:53\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 336/1000 \u001b[0m                      \n","\n","                       Computation: 158383 steps/s (collection: 0.265s, learning 0.356s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 3.3477\n","               Mean surrogate loss: 0.0076\n","                 Mean entropy loss: 8.9732\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 33128448\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:30\n","                               ETA: 00:08:52\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 337/1000 \u001b[0m                      \n","\n","                       Computation: 159570 steps/s (collection: 0.265s, learning 0.351s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 1.9970\n","               Mean surrogate loss: 0.0013\n","                 Mean entropy loss: 8.9729\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 33226752\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:30\n","                               ETA: 00:08:51\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 338/1000 \u001b[0m                      \n","\n","                       Computation: 156397 steps/s (collection: 0.270s, learning 0.358s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 2.0916\n","               Mean surrogate loss: 0.0008\n","                 Mean entropy loss: 8.9723\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 33325056\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:31\n","                               ETA: 00:08:50\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 339/1000 \u001b[0m                      \n","\n","                       Computation: 158717 steps/s (collection: 0.268s, learning 0.352s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 2.5086\n","               Mean surrogate loss: 0.0061\n","                 Mean entropy loss: 8.9695\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 33423360\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:32\n","                               ETA: 00:08:48\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 340/1000 \u001b[0m                      \n","\n","                       Computation: 158859 steps/s (collection: 0.264s, learning 0.354s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 2.1384\n","               Mean surrogate loss: 0.0019\n","                 Mean entropy loss: 8.9705\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 33521664\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:32\n","                               ETA: 00:08:47\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 341/1000 \u001b[0m                      \n","\n","                       Computation: 157102 steps/s (collection: 0.269s, learning 0.357s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 2.6167\n","               Mean surrogate loss: 0.0050\n","                 Mean entropy loss: 8.9690\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 33619968\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:33\n","                               ETA: 00:08:46\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 342/1000 \u001b[0m                      \n","\n","                       Computation: 158424 steps/s (collection: 0.267s, learning 0.354s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 3.0297\n","               Mean surrogate loss: 0.0009\n","                 Mean entropy loss: 8.9682\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 33718272\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:33\n","                               ETA: 00:08:45\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 343/1000 \u001b[0m                      \n","\n","                       Computation: 156889 steps/s (collection: 0.271s, learning 0.355s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 3.0203\n","               Mean surrogate loss: 0.0065\n","                 Mean entropy loss: 8.9703\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 33816576\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:34\n","                               ETA: 00:08:44\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 344/1000 \u001b[0m                      \n","\n","                       Computation: 157343 steps/s (collection: 0.265s, learning 0.360s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 3.9750\n","               Mean surrogate loss: 0.0010\n","                 Mean entropy loss: 8.9709\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 33914880\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:35\n","                               ETA: 00:08:43\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 345/1000 \u001b[0m                      \n","\n","                       Computation: 159784 steps/s (collection: 0.263s, learning 0.352s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 3.6944\n","               Mean surrogate loss: 0.0017\n","                 Mean entropy loss: 8.9720\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 34013184\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:35\n","                               ETA: 00:08:42\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 346/1000 \u001b[0m                      \n","\n","                       Computation: 156026 steps/s (collection: 0.269s, learning 0.361s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 4.8222\n","               Mean surrogate loss: 0.0058\n","                 Mean entropy loss: 8.9846\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 34111488\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:36\n","                               ETA: 00:08:41\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 347/1000 \u001b[0m                      \n","\n","                       Computation: 157025 steps/s (collection: 0.266s, learning 0.360s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 3.5836\n","               Mean surrogate loss: 0.0032\n","                 Mean entropy loss: 8.9903\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 34209792\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:37\n","                               ETA: 00:08:39\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 348/1000 \u001b[0m                      \n","\n","                       Computation: 144912 steps/s (collection: 0.321s, learning 0.357s)\n","             Mean action noise std: 1.11\n","          Mean value_function loss: 4.0742\n","               Mean surrogate loss: 0.0030\n","                 Mean entropy loss: 8.9955\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 34308096\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:04:37\n","                               ETA: 00:08:38\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 349/1000 \u001b[0m                      \n","\n","                       Computation: 141408 steps/s (collection: 0.325s, learning 0.370s)\n","             Mean action noise std: 1.12\n","          Mean value_function loss: 3.6885\n","               Mean surrogate loss: 0.0032\n","                 Mean entropy loss: 9.0011\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 34406400\n","                    Iteration time: 0.70s\n","                      Time elapsed: 00:04:38\n","                               ETA: 00:08:37\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 350/1000 \u001b[0m                      \n","\n","                       Computation: 143490 steps/s (collection: 0.329s, learning 0.356s)\n","             Mean action noise std: 1.12\n","          Mean value_function loss: 3.1871\n","               Mean surrogate loss: 0.0003\n","                 Mean entropy loss: 9.0049\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 34504704\n","                    Iteration time: 0.69s\n","                      Time elapsed: 00:04:39\n","                               ETA: 00:08:36\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 351/1000 \u001b[0m                      \n","\n","                       Computation: 141700 steps/s (collection: 0.329s, learning 0.365s)\n","             Mean action noise std: 1.12\n","          Mean value_function loss: 3.8995\n","               Mean surrogate loss: 0.0009\n","                 Mean entropy loss: 9.0136\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 34603008\n","                    Iteration time: 0.69s\n","                      Time elapsed: 00:04:39\n","                               ETA: 00:08:35\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 352/1000 \u001b[0m                      \n","\n","                       Computation: 156231 steps/s (collection: 0.267s, learning 0.362s)\n","             Mean action noise std: 1.12\n","          Mean value_function loss: 3.3224\n","               Mean surrogate loss: 0.0050\n","                 Mean entropy loss: 9.0219\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 34701312\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:40\n","                               ETA: 00:08:34\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 353/1000 \u001b[0m                      \n","\n","                       Computation: 158895 steps/s (collection: 0.270s, learning 0.348s)\n","             Mean action noise std: 1.12\n","          Mean value_function loss: 3.7673\n","               Mean surrogate loss: 0.0052\n","                 Mean entropy loss: 9.0327\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 34799616\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:41\n","                               ETA: 00:08:33\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 354/1000 \u001b[0m                      \n","\n","                       Computation: 154784 steps/s (collection: 0.276s, learning 0.359s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 3.4777\n","               Mean surrogate loss: 0.0007\n","                 Mean entropy loss: 9.0445\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 34897920\n","                    Iteration time: 0.64s\n","                      Time elapsed: 00:04:41\n","                               ETA: 00:08:32\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 355/1000 \u001b[0m                      \n","\n","                       Computation: 158135 steps/s (collection: 0.270s, learning 0.351s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 3.7602\n","               Mean surrogate loss: 0.0058\n","                 Mean entropy loss: 9.0520\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 34996224\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:42\n","                               ETA: 00:08:31\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 356/1000 \u001b[0m                      \n","\n","                       Computation: 156613 steps/s (collection: 0.266s, learning 0.362s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 3.8664\n","               Mean surrogate loss: 0.0088\n","                 Mean entropy loss: 9.0564\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 35094528\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:42\n","                               ETA: 00:08:30\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 357/1000 \u001b[0m                      \n","\n","                       Computation: 158270 steps/s (collection: 0.268s, learning 0.353s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 3.4963\n","               Mean surrogate loss: 0.0099\n","                 Mean entropy loss: 9.0577\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 35192832\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:43\n","                               ETA: 00:08:29\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 358/1000 \u001b[0m                      \n","\n","                       Computation: 156231 steps/s (collection: 0.264s, learning 0.365s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 3.1950\n","               Mean surrogate loss: 0.0071\n","                 Mean entropy loss: 9.0584\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 35291136\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:44\n","                               ETA: 00:08:28\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 359/1000 \u001b[0m                      \n","\n","                       Computation: 157120 steps/s (collection: 0.275s, learning 0.351s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.9214\n","               Mean surrogate loss: 0.0081\n","                 Mean entropy loss: 9.0590\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 35389440\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:44\n","                               ETA: 00:08:27\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 360/1000 \u001b[0m                      \n","\n","                       Computation: 156917 steps/s (collection: 0.267s, learning 0.359s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.4691\n","               Mean surrogate loss: 0.0060\n","                 Mean entropy loss: 9.0594\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 35487744\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:45\n","                               ETA: 00:08:26\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 361/1000 \u001b[0m                      \n","\n","                       Computation: 158360 steps/s (collection: 0.267s, learning 0.353s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.8255\n","               Mean surrogate loss: 0.0090\n","                 Mean entropy loss: 9.0601\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 35586048\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:46\n","                               ETA: 00:08:25\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 362/1000 \u001b[0m                      \n","\n","                       Computation: 156080 steps/s (collection: 0.266s, learning 0.364s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 3.0801\n","               Mean surrogate loss: 0.0046\n","                 Mean entropy loss: 9.0607\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 35684352\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:46\n","                               ETA: 00:08:23\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 363/1000 \u001b[0m                      \n","\n","                       Computation: 160136 steps/s (collection: 0.261s, learning 0.353s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.9654\n","               Mean surrogate loss: 0.0073\n","                 Mean entropy loss: 9.0614\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 35782656\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:04:47\n","                               ETA: 00:08:22\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 364/1000 \u001b[0m                      \n","\n","                       Computation: 157931 steps/s (collection: 0.271s, learning 0.351s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.7624\n","               Mean surrogate loss: 0.0047\n","                 Mean entropy loss: 9.0618\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 35880960\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:47\n","                               ETA: 00:08:21\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 365/1000 \u001b[0m                      \n","\n","                       Computation: 159054 steps/s (collection: 0.265s, learning 0.353s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.6215\n","               Mean surrogate loss: 0.0086\n","                 Mean entropy loss: 9.0621\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 35979264\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:48\n","                               ETA: 00:08:20\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 366/1000 \u001b[0m                      \n","\n","                       Computation: 156941 steps/s (collection: 0.267s, learning 0.359s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.3766\n","               Mean surrogate loss: 0.0075\n","                 Mean entropy loss: 9.0627\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 36077568\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:49\n","                               ETA: 00:08:19\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 367/1000 \u001b[0m                      \n","\n","                       Computation: 157577 steps/s (collection: 0.268s, learning 0.356s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.1544\n","               Mean surrogate loss: 0.0097\n","                 Mean entropy loss: 9.0634\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 36175872\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:49\n","                               ETA: 00:08:18\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 368/1000 \u001b[0m                      \n","\n","                       Computation: 143291 steps/s (collection: 0.328s, learning 0.358s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.3023\n","               Mean surrogate loss: 0.0056\n","                 Mean entropy loss: 9.0636\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 36274176\n","                    Iteration time: 0.69s\n","                      Time elapsed: 00:04:50\n","                               ETA: 00:08:17\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 369/1000 \u001b[0m                      \n","\n","                       Computation: 145878 steps/s (collection: 0.309s, learning 0.364s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.8848\n","               Mean surrogate loss: 0.0071\n","                 Mean entropy loss: 9.0636\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 36372480\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:04:51\n","                               ETA: 00:08:16\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 370/1000 \u001b[0m                      \n","\n","                       Computation: 147948 steps/s (collection: 0.301s, learning 0.364s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.8044\n","               Mean surrogate loss: 0.0016\n","                 Mean entropy loss: 9.0640\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 36470784\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:04:51\n","                               ETA: 00:08:15\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 371/1000 \u001b[0m                      \n","\n","                       Computation: 145925 steps/s (collection: 0.305s, learning 0.369s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.6332\n","               Mean surrogate loss: 0.0013\n","                 Mean entropy loss: 9.0669\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 36569088\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:04:52\n","                               ETA: 00:08:14\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 372/1000 \u001b[0m                      \n","\n","                       Computation: 144895 steps/s (collection: 0.330s, learning 0.348s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 3.0997\n","               Mean surrogate loss: -0.0009\n","                 Mean entropy loss: 9.0720\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 36667392\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:04:53\n","                               ETA: 00:08:13\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 373/1000 \u001b[0m                      \n","\n","                       Computation: 156506 steps/s (collection: 0.267s, learning 0.361s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 3.2448\n","               Mean surrogate loss: -0.0012\n","                 Mean entropy loss: 9.0821\n","                       Mean reward: 418.24\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 36765696\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:53\n","                               ETA: 00:08:12\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 374/1000 \u001b[0m                      \n","\n","                       Computation: 158493 steps/s (collection: 0.266s, learning 0.354s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 3.2003\n","               Mean surrogate loss: 0.0044\n","                 Mean entropy loss: 9.0891\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 36864000\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:54\n","                               ETA: 00:08:11\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 375/1000 \u001b[0m                      \n","\n","                       Computation: 156448 steps/s (collection: 0.269s, learning 0.359s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 1.4845\n","               Mean surrogate loss: -0.0033\n","                 Mean entropy loss: 9.0883\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 36962304\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:55\n","                               ETA: 00:08:10\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 376/1000 \u001b[0m                      \n","\n","                       Computation: 158009 steps/s (collection: 0.268s, learning 0.354s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 0.6840\n","               Mean surrogate loss: 0.0016\n","                 Mean entropy loss: 9.0799\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 37060608\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:55\n","                               ETA: 00:08:09\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 377/1000 \u001b[0m                      \n","\n","                       Computation: 157378 steps/s (collection: 0.270s, learning 0.355s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 1.2296\n","               Mean surrogate loss: 0.0115\n","                 Mean entropy loss: 9.0719\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 37158912\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:56\n","                               ETA: 00:08:08\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 378/1000 \u001b[0m                      \n","\n","                       Computation: 158133 steps/s (collection: 0.264s, learning 0.358s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 4.8033\n","               Mean surrogate loss: 0.0048\n","                 Mean entropy loss: 9.0707\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 37257216\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:56\n","                               ETA: 00:08:07\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 379/1000 \u001b[0m                      \n","\n","                       Computation: 158979 steps/s (collection: 0.267s, learning 0.351s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.2336\n","               Mean surrogate loss: 0.0016\n","                 Mean entropy loss: 9.0708\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 37355520\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:57\n","                               ETA: 00:08:06\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 380/1000 \u001b[0m                      \n","\n","                       Computation: 156916 steps/s (collection: 0.264s, learning 0.362s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.0113\n","               Mean surrogate loss: -0.0005\n","                 Mean entropy loss: 9.0723\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 37453824\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:58\n","                               ETA: 00:08:05\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 381/1000 \u001b[0m                      \n","\n","                       Computation: 156632 steps/s (collection: 0.270s, learning 0.358s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.2544\n","               Mean surrogate loss: 0.0152\n","                 Mean entropy loss: 9.0796\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 37552128\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:04:58\n","                               ETA: 00:08:04\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 382/1000 \u001b[0m                      \n","\n","                       Computation: 157388 steps/s (collection: 0.272s, learning 0.353s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 3.0045\n","               Mean surrogate loss: 0.0094\n","                 Mean entropy loss: 9.0856\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 37650432\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:04:59\n","                               ETA: 00:08:03\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 383/1000 \u001b[0m                      \n","\n","                       Computation: 158468 steps/s (collection: 0.265s, learning 0.355s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 2.2505\n","               Mean surrogate loss: 0.0020\n","                 Mean entropy loss: 9.0882\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 37748736\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:00\n","                               ETA: 00:08:02\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 384/1000 \u001b[0m                      \n","\n","                       Computation: 158583 steps/s (collection: 0.263s, learning 0.357s)\n","             Mean action noise std: 1.13\n","          Mean value_function loss: 3.2661\n","               Mean surrogate loss: 0.0038\n","                 Mean entropy loss: 9.0916\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 37847040\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:00\n","                               ETA: 00:08:01\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 385/1000 \u001b[0m                      \n","\n","                       Computation: 158186 steps/s (collection: 0.272s, learning 0.350s)\n","             Mean action noise std: 1.14\n","          Mean value_function loss: 3.4410\n","               Mean surrogate loss: 0.0026\n","                 Mean entropy loss: 9.0977\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 37945344\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:01\n","                               ETA: 00:08:00\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 386/1000 \u001b[0m                      \n","\n","                       Computation: 156875 steps/s (collection: 0.269s, learning 0.358s)\n","             Mean action noise std: 1.14\n","          Mean value_function loss: 3.4314\n","               Mean surrogate loss: 0.0043\n","                 Mean entropy loss: 9.1047\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 38043648\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:01\n","                               ETA: 00:07:59\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 387/1000 \u001b[0m                      \n","\n","                       Computation: 157519 steps/s (collection: 0.274s, learning 0.350s)\n","             Mean action noise std: 1.14\n","          Mean value_function loss: 3.6132\n","               Mean surrogate loss: 0.0050\n","                 Mean entropy loss: 9.1138\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 38141952\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:02\n","                               ETA: 00:07:58\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 388/1000 \u001b[0m                      \n","\n","                       Computation: 153923 steps/s (collection: 0.266s, learning 0.372s)\n","             Mean action noise std: 1.14\n","          Mean value_function loss: 3.8580\n","               Mean surrogate loss: 0.0053\n","                 Mean entropy loss: 9.1234\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 38240256\n","                    Iteration time: 0.64s\n","                      Time elapsed: 00:05:03\n","                               ETA: 00:07:57\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 389/1000 \u001b[0m                      \n","\n","                       Computation: 147035 steps/s (collection: 0.313s, learning 0.356s)\n","             Mean action noise std: 1.14\n","          Mean value_function loss: 4.0556\n","               Mean surrogate loss: 0.0022\n","                 Mean entropy loss: 9.1302\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 38338560\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:05:03\n","                               ETA: 00:07:56\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 390/1000 \u001b[0m                      \n","\n","                       Computation: 145165 steps/s (collection: 0.321s, learning 0.357s)\n","             Mean action noise std: 1.15\n","          Mean value_function loss: 3.0030\n","               Mean surrogate loss: 0.0059\n","                 Mean entropy loss: 9.1405\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 38436864\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:05:04\n","                               ETA: 00:07:55\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 391/1000 \u001b[0m                      \n","\n","                       Computation: 148599 steps/s (collection: 0.304s, learning 0.358s)\n","             Mean action noise std: 1.15\n","          Mean value_function loss: 4.6552\n","               Mean surrogate loss: -0.0014\n","                 Mean entropy loss: 9.1547\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 38535168\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:05:05\n","                               ETA: 00:07:54\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 392/1000 \u001b[0m                      \n","\n","                       Computation: 140654 steps/s (collection: 0.342s, learning 0.357s)\n","             Mean action noise std: 1.15\n","          Mean value_function loss: 3.6269\n","               Mean surrogate loss: 0.0003\n","                 Mean entropy loss: 9.1788\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 38633472\n","                    Iteration time: 0.70s\n","                      Time elapsed: 00:05:05\n","                               ETA: 00:07:53\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 393/1000 \u001b[0m                      \n","\n","                       Computation: 156518 steps/s (collection: 0.269s, learning 0.359s)\n","             Mean action noise std: 1.15\n","          Mean value_function loss: 3.2169\n","               Mean surrogate loss: 0.0070\n","                 Mean entropy loss: 9.1861\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 38731776\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:06\n","                               ETA: 00:07:52\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 394/1000 \u001b[0m                      \n","\n","                       Computation: 159139 steps/s (collection: 0.268s, learning 0.350s)\n","             Mean action noise std: 1.15\n","          Mean value_function loss: 3.0212\n","               Mean surrogate loss: 0.0033\n","                 Mean entropy loss: 9.1913\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 38830080\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:07\n","                               ETA: 00:07:51\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 395/1000 \u001b[0m                      \n","\n","                       Computation: 156461 steps/s (collection: 0.272s, learning 0.357s)\n","             Mean action noise std: 1.16\n","          Mean value_function loss: 2.7311\n","               Mean surrogate loss: -0.0019\n","                 Mean entropy loss: 9.1990\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 38928384\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:07\n","                               ETA: 00:07:50\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 396/1000 \u001b[0m                      \n","\n","                       Computation: 156564 steps/s (collection: 0.268s, learning 0.360s)\n","             Mean action noise std: 1.16\n","          Mean value_function loss: 2.6830\n","               Mean surrogate loss: -0.0014\n","                 Mean entropy loss: 9.2107\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 39026688\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:08\n","                               ETA: 00:07:49\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 397/1000 \u001b[0m                      \n","\n","                       Computation: 158122 steps/s (collection: 0.264s, learning 0.358s)\n","             Mean action noise std: 1.16\n","          Mean value_function loss: 3.7775\n","               Mean surrogate loss: -0.0015\n","                 Mean entropy loss: 9.2280\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 39124992\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:09\n","                               ETA: 00:07:48\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 398/1000 \u001b[0m                      \n","\n","                       Computation: 159417 steps/s (collection: 0.264s, learning 0.353s)\n","             Mean action noise std: 1.16\n","          Mean value_function loss: 6.0531\n","               Mean surrogate loss: 0.0171\n","                 Mean entropy loss: 9.2323\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 39223296\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:09\n","                               ETA: 00:07:47\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 399/1000 \u001b[0m                      \n","\n","                       Computation: 160209 steps/s (collection: 0.263s, learning 0.351s)\n","             Mean action noise std: 1.16\n","          Mean value_function loss: 2.0709\n","               Mean surrogate loss: 0.0038\n","                 Mean entropy loss: 9.2351\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 39321600\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:05:10\n","                               ETA: 00:07:46\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 400/1000 \u001b[0m                      \n","\n","                       Computation: 158954 steps/s (collection: 0.270s, learning 0.349s)\n","             Mean action noise std: 1.16\n","          Mean value_function loss: 2.6930\n","               Mean surrogate loss: 0.0022\n","                 Mean entropy loss: 9.2384\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 39419904\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:10\n","                               ETA: 00:07:45\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 401/1000 \u001b[0m                      \n","\n","                       Computation: 158499 steps/s (collection: 0.265s, learning 0.356s)\n","             Mean action noise std: 1.16\n","          Mean value_function loss: 2.6989\n","               Mean surrogate loss: 0.0014\n","                 Mean entropy loss: 9.2430\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 39518208\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:11\n","                               ETA: 00:07:44\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 402/1000 \u001b[0m                      \n","\n","                       Computation: 160675 steps/s (collection: 0.265s, learning 0.347s)\n","             Mean action noise std: 1.16\n","          Mean value_function loss: 2.6621\n","               Mean surrogate loss: -0.0004\n","                 Mean entropy loss: 9.2446\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 39616512\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:05:12\n","                               ETA: 00:07:43\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 403/1000 \u001b[0m                      \n","\n","                       Computation: 158821 steps/s (collection: 0.261s, learning 0.358s)\n","             Mean action noise std: 1.17\n","          Mean value_function loss: 2.5849\n","               Mean surrogate loss: 0.0006\n","                 Mean entropy loss: 9.2568\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 39714816\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:12\n","                               ETA: 00:07:42\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 404/1000 \u001b[0m                      \n","\n","                       Computation: 159890 steps/s (collection: 0.262s, learning 0.352s)\n","             Mean action noise std: 1.17\n","          Mean value_function loss: 2.8868\n","               Mean surrogate loss: -0.0006\n","                 Mean entropy loss: 9.2620\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 39813120\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:05:13\n","                               ETA: 00:07:41\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 405/1000 \u001b[0m                      \n","\n","                       Computation: 156503 steps/s (collection: 0.275s, learning 0.353s)\n","             Mean action noise std: 1.17\n","          Mean value_function loss: 2.7240\n","               Mean surrogate loss: 0.0141\n","                 Mean entropy loss: 9.2728\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 39911424\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:13\n","                               ETA: 00:07:40\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 406/1000 \u001b[0m                      \n","\n","                       Computation: 158140 steps/s (collection: 0.269s, learning 0.353s)\n","             Mean action noise std: 1.17\n","          Mean value_function loss: 2.0488\n","               Mean surrogate loss: 0.0014\n","                 Mean entropy loss: 9.2748\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 40009728\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:14\n","                               ETA: 00:07:39\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 407/1000 \u001b[0m                      \n","\n","                       Computation: 156763 steps/s (collection: 0.272s, learning 0.355s)\n","             Mean action noise std: 1.17\n","          Mean value_function loss: 2.2589\n","               Mean surrogate loss: 0.0034\n","                 Mean entropy loss: 9.2746\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 40108032\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:15\n","                               ETA: 00:07:38\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 408/1000 \u001b[0m                      \n","\n","                       Computation: 157554 steps/s (collection: 0.265s, learning 0.359s)\n","             Mean action noise std: 1.17\n","          Mean value_function loss: 2.1724\n","               Mean surrogate loss: 0.0008\n","                 Mean entropy loss: 9.2749\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 40206336\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:15\n","                               ETA: 00:07:37\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 409/1000 \u001b[0m                      \n","\n","                       Computation: 146552 steps/s (collection: 0.318s, learning 0.352s)\n","             Mean action noise std: 1.17\n","          Mean value_function loss: 2.2815\n","               Mean surrogate loss: 0.0088\n","                 Mean entropy loss: 9.2775\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 40304640\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:05:16\n","                               ETA: 00:07:36\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 410/1000 \u001b[0m                      \n","\n","                       Computation: 142677 steps/s (collection: 0.325s, learning 0.364s)\n","             Mean action noise std: 1.17\n","          Mean value_function loss: 2.6050\n","               Mean surrogate loss: 0.0016\n","                 Mean entropy loss: 9.2798\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 40402944\n","                    Iteration time: 0.69s\n","                      Time elapsed: 00:05:17\n","                               ETA: 00:07:35\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 411/1000 \u001b[0m                      \n","\n","                       Computation: 146804 steps/s (collection: 0.304s, learning 0.366s)\n","             Mean action noise std: 1.17\n","          Mean value_function loss: 2.4673\n","               Mean surrogate loss: 0.0038\n","                 Mean entropy loss: 9.2816\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 40501248\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:05:17\n","                               ETA: 00:07:34\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 412/1000 \u001b[0m                      \n","\n","                       Computation: 143803 steps/s (collection: 0.321s, learning 0.362s)\n","             Mean action noise std: 1.17\n","          Mean value_function loss: 2.0807\n","               Mean surrogate loss: 0.0007\n","                 Mean entropy loss: 9.2852\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 40599552\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:05:18\n","                               ETA: 00:07:33\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 413/1000 \u001b[0m                      \n","\n","                       Computation: 149504 steps/s (collection: 0.310s, learning 0.348s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.0807\n","               Mean surrogate loss: 0.0020\n","                 Mean entropy loss: 9.2941\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 40697856\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:05:19\n","                               ETA: 00:07:32\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 414/1000 \u001b[0m                      \n","\n","                       Computation: 157623 steps/s (collection: 0.265s, learning 0.359s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.1406\n","               Mean surrogate loss: -0.0007\n","                 Mean entropy loss: 9.2970\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 40796160\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:19\n","                               ETA: 00:07:31\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 415/1000 \u001b[0m                      \n","\n","                       Computation: 158629 steps/s (collection: 0.268s, learning 0.352s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.0190\n","               Mean surrogate loss: 0.0039\n","                 Mean entropy loss: 9.2986\n","                       Mean reward: 539.98\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 40894464\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:20\n","                               ETA: 00:07:30\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 416/1000 \u001b[0m                      \n","\n","                       Computation: 156948 steps/s (collection: 0.269s, learning 0.357s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 1.7315\n","               Mean surrogate loss: -0.0004\n","                 Mean entropy loss: 9.3034\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 40992768\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:21\n","                               ETA: 00:07:29\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 417/1000 \u001b[0m                      \n","\n","                       Computation: 159677 steps/s (collection: 0.268s, learning 0.348s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 1.5857\n","               Mean surrogate loss: 0.0089\n","                 Mean entropy loss: 9.3063\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 41091072\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:21\n","                               ETA: 00:07:28\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 418/1000 \u001b[0m                      \n","\n","                       Computation: 156880 steps/s (collection: 0.273s, learning 0.354s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 1.1856\n","               Mean surrogate loss: 0.0110\n","                 Mean entropy loss: 9.3025\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 41189376\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:22\n","                               ETA: 00:07:27\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 419/1000 \u001b[0m                      \n","\n","                       Computation: 159054 steps/s (collection: 0.268s, learning 0.350s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.6236\n","               Mean surrogate loss: 0.0062\n","                 Mean entropy loss: 9.3018\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 41287680\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:22\n","                               ETA: 00:07:26\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 420/1000 \u001b[0m                      \n","\n","                       Computation: 155508 steps/s (collection: 0.272s, learning 0.360s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.1083\n","               Mean surrogate loss: 0.0113\n","                 Mean entropy loss: 9.3017\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 41385984\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:23\n","                               ETA: 00:07:25\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 421/1000 \u001b[0m                      \n","\n","                       Computation: 158209 steps/s (collection: 0.265s, learning 0.356s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 1.8133\n","               Mean surrogate loss: 0.0081\n","                 Mean entropy loss: 9.3017\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 41484288\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:24\n","                               ETA: 00:07:24\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 422/1000 \u001b[0m                      \n","\n","                       Computation: 158065 steps/s (collection: 0.267s, learning 0.355s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.1163\n","               Mean surrogate loss: 0.0040\n","                 Mean entropy loss: 9.3019\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 41582592\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:24\n","                               ETA: 00:07:23\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 423/1000 \u001b[0m                      \n","\n","                       Computation: 155643 steps/s (collection: 0.279s, learning 0.353s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.2804\n","               Mean surrogate loss: 0.0128\n","                 Mean entropy loss: 9.3018\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 41680896\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:25\n","                               ETA: 00:07:22\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 424/1000 \u001b[0m                      \n","\n","                       Computation: 155477 steps/s (collection: 0.270s, learning 0.363s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 1.8264\n","               Mean surrogate loss: 0.0090\n","                 Mean entropy loss: 9.3021\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 41779200\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:26\n","                               ETA: 00:07:21\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 425/1000 \u001b[0m                      \n","\n","                       Computation: 160062 steps/s (collection: 0.265s, learning 0.349s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 1.3292\n","               Mean surrogate loss: 0.0094\n","                 Mean entropy loss: 9.3025\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 41877504\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:05:26\n","                               ETA: 00:07:20\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 426/1000 \u001b[0m                      \n","\n","                       Computation: 158176 steps/s (collection: 0.261s, learning 0.360s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 1.7058\n","               Mean surrogate loss: 0.0067\n","                 Mean entropy loss: 9.3027\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 41975808\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:27\n","                               ETA: 00:07:19\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 427/1000 \u001b[0m                      \n","\n","                       Computation: 159401 steps/s (collection: 0.266s, learning 0.350s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 1.8636\n","               Mean surrogate loss: 0.0163\n","                 Mean entropy loss: 9.3029\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 42074112\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:27\n","                               ETA: 00:07:19\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 428/1000 \u001b[0m                      \n","\n","                       Computation: 156224 steps/s (collection: 0.272s, learning 0.357s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 1.7949\n","               Mean surrogate loss: 0.0042\n","                 Mean entropy loss: 9.3030\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 42172416\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:28\n","                               ETA: 00:07:18\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 429/1000 \u001b[0m                      \n","\n","                       Computation: 147712 steps/s (collection: 0.305s, learning 0.361s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.0876\n","               Mean surrogate loss: 0.0153\n","                 Mean entropy loss: 9.3035\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 42270720\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:05:29\n","                               ETA: 00:07:17\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 430/1000 \u001b[0m                      \n","\n","                       Computation: 148156 steps/s (collection: 0.303s, learning 0.361s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.3527\n","               Mean surrogate loss: 0.0149\n","                 Mean entropy loss: 9.3038\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 42369024\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:05:29\n","                               ETA: 00:07:16\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 431/1000 \u001b[0m                      \n","\n","                       Computation: 145680 steps/s (collection: 0.318s, learning 0.357s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.4745\n","               Mean surrogate loss: 0.0127\n","                 Mean entropy loss: 9.3041\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 42467328\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:05:30\n","                               ETA: 00:07:15\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 432/1000 \u001b[0m                      \n","\n","                       Computation: 147022 steps/s (collection: 0.303s, learning 0.366s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.4246\n","               Mean surrogate loss: 0.0071\n","                 Mean entropy loss: 9.3044\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 42565632\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:05:31\n","                               ETA: 00:07:14\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 433/1000 \u001b[0m                      \n","\n","                       Computation: 143754 steps/s (collection: 0.332s, learning 0.352s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.5375\n","               Mean surrogate loss: 0.0086\n","                 Mean entropy loss: 9.3050\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 42663936\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:05:31\n","                               ETA: 00:07:13\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 434/1000 \u001b[0m                      \n","\n","                       Computation: 157810 steps/s (collection: 0.268s, learning 0.355s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.4302\n","               Mean surrogate loss: 0.0070\n","                 Mean entropy loss: 9.3056\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 42762240\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:32\n","                               ETA: 00:07:12\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 435/1000 \u001b[0m                      \n","\n","                       Computation: 158759 steps/s (collection: 0.271s, learning 0.349s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 3.0313\n","               Mean surrogate loss: 0.0089\n","                 Mean entropy loss: 9.3061\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 42860544\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:33\n","                               ETA: 00:07:11\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 436/1000 \u001b[0m                      \n","\n","                       Computation: 154635 steps/s (collection: 0.276s, learning 0.359s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.5298\n","               Mean surrogate loss: 0.0056\n","                 Mean entropy loss: 9.3064\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 42958848\n","                    Iteration time: 0.64s\n","                      Time elapsed: 00:05:33\n","                               ETA: 00:07:10\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 437/1000 \u001b[0m                      \n","\n","                       Computation: 156159 steps/s (collection: 0.270s, learning 0.360s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.9742\n","               Mean surrogate loss: 0.0099\n","                 Mean entropy loss: 9.3067\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 43057152\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:34\n","                               ETA: 00:07:09\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 438/1000 \u001b[0m                      \n","\n","                       Computation: 158566 steps/s (collection: 0.266s, learning 0.354s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 3.4369\n","               Mean surrogate loss: 0.0041\n","                 Mean entropy loss: 9.3071\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 43155456\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:35\n","                               ETA: 00:07:08\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 439/1000 \u001b[0m                      \n","\n","                       Computation: 159004 steps/s (collection: 0.262s, learning 0.356s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 3.0837\n","               Mean surrogate loss: 0.0083\n","                 Mean entropy loss: 9.3077\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 43253760\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:35\n","                               ETA: 00:07:07\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 440/1000 \u001b[0m                      \n","\n","                       Computation: 157799 steps/s (collection: 0.268s, learning 0.355s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.7763\n","               Mean surrogate loss: 0.0033\n","                 Mean entropy loss: 9.3086\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 43352064\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:36\n","                               ETA: 00:07:07\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 441/1000 \u001b[0m                      \n","\n","                       Computation: 155911 steps/s (collection: 0.278s, learning 0.353s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 3.0887\n","               Mean surrogate loss: 0.0093\n","                 Mean entropy loss: 9.3093\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 43450368\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:36\n","                               ETA: 00:07:06\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 442/1000 \u001b[0m                      \n","\n","                       Computation: 157498 steps/s (collection: 0.268s, learning 0.356s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 2.4881\n","               Mean surrogate loss: 0.0028\n","                 Mean entropy loss: 9.3097\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 43548672\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:37\n","                               ETA: 00:07:05\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 443/1000 \u001b[0m                      \n","\n","                       Computation: 159062 steps/s (collection: 0.268s, learning 0.351s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 3.0474\n","               Mean surrogate loss: 0.0002\n","                 Mean entropy loss: 9.3144\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 43646976\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:38\n","                               ETA: 00:07:04\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 444/1000 \u001b[0m                      \n","\n","                       Computation: 155384 steps/s (collection: 0.267s, learning 0.365s)\n","             Mean action noise std: 1.18\n","          Mean value_function loss: 3.6399\n","               Mean surrogate loss: 0.0013\n","                 Mean entropy loss: 9.3275\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 43745280\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:38\n","                               ETA: 00:07:03\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 445/1000 \u001b[0m                      \n","\n","                       Computation: 159968 steps/s (collection: 0.266s, learning 0.348s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 3.5912\n","               Mean surrogate loss: -0.0001\n","                 Mean entropy loss: 9.3367\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 43843584\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:05:39\n","                               ETA: 00:07:02\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 446/1000 \u001b[0m                      \n","\n","                       Computation: 155632 steps/s (collection: 0.272s, learning 0.360s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 5.9960\n","               Mean surrogate loss: 0.0510\n","                 Mean entropy loss: 9.3515\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 43941888\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:40\n","                               ETA: 00:07:01\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 447/1000 \u001b[0m                      \n","\n","                       Computation: 161252 steps/s (collection: 0.260s, learning 0.349s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.7335\n","               Mean surrogate loss: -0.0009\n","                 Mean entropy loss: 9.3548\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 44040192\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:05:40\n","                               ETA: 00:07:00\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 448/1000 \u001b[0m                      \n","\n","                       Computation: 157908 steps/s (collection: 0.262s, learning 0.361s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.2889\n","               Mean surrogate loss: 0.0008\n","                 Mean entropy loss: 9.3538\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 44138496\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:41\n","                               ETA: 00:06:59\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 449/1000 \u001b[0m                      \n","\n","                       Computation: 156884 steps/s (collection: 0.267s, learning 0.360s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.8194\n","               Mean surrogate loss: -0.0009\n","                 Mean entropy loss: 9.3578\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 44236800\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:41\n","                               ETA: 00:06:58\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 450/1000 \u001b[0m                      \n","\n","                       Computation: 149273 steps/s (collection: 0.301s, learning 0.357s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 3.2663\n","               Mean surrogate loss: 0.0068\n","                 Mean entropy loss: 9.3568\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 44335104\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:05:42\n","                               ETA: 00:06:57\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 451/1000 \u001b[0m                      \n","\n","                       Computation: 145059 steps/s (collection: 0.324s, learning 0.354s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.1013\n","               Mean surrogate loss: 0.0091\n","                 Mean entropy loss: 9.3574\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 44433408\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:05:43\n","                               ETA: 00:06:56\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 452/1000 \u001b[0m                      \n","\n","                       Computation: 148259 steps/s (collection: 0.299s, learning 0.364s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 1.7598\n","               Mean surrogate loss: 0.0026\n","                 Mean entropy loss: 9.3582\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 44531712\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:05:43\n","                               ETA: 00:06:56\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 453/1000 \u001b[0m                      \n","\n","                       Computation: 142917 steps/s (collection: 0.323s, learning 0.364s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 1.9731\n","               Mean surrogate loss: 0.0008\n","                 Mean entropy loss: 9.3587\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 44630016\n","                    Iteration time: 0.69s\n","                      Time elapsed: 00:05:44\n","                               ETA: 00:06:55\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 454/1000 \u001b[0m                      \n","\n","                       Computation: 154202 steps/s (collection: 0.282s, learning 0.355s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 1.7835\n","               Mean surrogate loss: 0.0021\n","                 Mean entropy loss: 9.3595\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 44728320\n","                    Iteration time: 0.64s\n","                      Time elapsed: 00:05:45\n","                               ETA: 00:06:54\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 455/1000 \u001b[0m                      \n","\n","                       Computation: 157410 steps/s (collection: 0.271s, learning 0.354s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.0937\n","               Mean surrogate loss: 0.0002\n","                 Mean entropy loss: 9.3623\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 44826624\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:45\n","                               ETA: 00:06:53\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 456/1000 \u001b[0m                      \n","\n","                       Computation: 157050 steps/s (collection: 0.267s, learning 0.359s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 3.1531\n","               Mean surrogate loss: 0.0196\n","                 Mean entropy loss: 9.3724\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 44924928\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:46\n","                               ETA: 00:06:52\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 457/1000 \u001b[0m                      \n","\n","                       Computation: 157673 steps/s (collection: 0.277s, learning 0.346s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.2972\n","               Mean surrogate loss: 0.0062\n","                 Mean entropy loss: 9.3780\n","                       Mean reward: 595.57\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 45023232\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:47\n","                               ETA: 00:06:51\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 458/1000 \u001b[0m                      \n","\n","                       Computation: 158736 steps/s (collection: 0.265s, learning 0.354s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 1.3161\n","               Mean surrogate loss: -0.0060\n","                 Mean entropy loss: 9.3758\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 45121536\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:47\n","                               ETA: 00:06:50\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 459/1000 \u001b[0m                      \n","\n","                       Computation: 158076 steps/s (collection: 0.271s, learning 0.351s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 0.9761\n","               Mean surrogate loss: -0.0046\n","                 Mean entropy loss: 9.3595\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 45219840\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:48\n","                               ETA: 00:06:49\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 460/1000 \u001b[0m                      \n","\n","                       Computation: 159692 steps/s (collection: 0.264s, learning 0.352s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 0.9132\n","               Mean surrogate loss: -0.0001\n","                 Mean entropy loss: 9.3354\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 45318144\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:48\n","                               ETA: 00:06:48\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 461/1000 \u001b[0m                      \n","\n","                       Computation: 158128 steps/s (collection: 0.268s, learning 0.354s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.9366\n","               Mean surrogate loss: -0.0015\n","                 Mean entropy loss: 9.3252\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 45416448\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:49\n","                               ETA: 00:06:47\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 462/1000 \u001b[0m                      \n","\n","                       Computation: 157078 steps/s (collection: 0.275s, learning 0.351s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.0805\n","               Mean surrogate loss: 0.0058\n","                 Mean entropy loss: 9.3327\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 45514752\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:50\n","                               ETA: 00:06:46\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 463/1000 \u001b[0m                      \n","\n","                       Computation: 156132 steps/s (collection: 0.270s, learning 0.360s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 1.3789\n","               Mean surrogate loss: 0.0118\n","                 Mean entropy loss: 9.3352\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 45613056\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:50\n","                               ETA: 00:06:46\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 464/1000 \u001b[0m                      \n","\n","                       Computation: 159258 steps/s (collection: 0.267s, learning 0.350s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 1.5718\n","               Mean surrogate loss: 0.0044\n","                 Mean entropy loss: 9.3355\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 45711360\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:51\n","                               ETA: 00:06:45\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 465/1000 \u001b[0m                      \n","\n","                       Computation: 159287 steps/s (collection: 0.263s, learning 0.354s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 1.8723\n","               Mean surrogate loss: 0.0137\n","                 Mean entropy loss: 9.3359\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 45809664\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:52\n","                               ETA: 00:06:44\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 466/1000 \u001b[0m                      \n","\n","                       Computation: 158879 steps/s (collection: 0.263s, learning 0.356s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.0624\n","               Mean surrogate loss: 0.0098\n","                 Mean entropy loss: 9.3362\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 45907968\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:52\n","                               ETA: 00:06:43\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 467/1000 \u001b[0m                      \n","\n","                       Computation: 157092 steps/s (collection: 0.273s, learning 0.353s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.0326\n","               Mean surrogate loss: 0.0077\n","                 Mean entropy loss: 9.3364\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 46006272\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:53\n","                               ETA: 00:06:42\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 468/1000 \u001b[0m                      \n","\n","                       Computation: 159836 steps/s (collection: 0.264s, learning 0.351s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.0628\n","               Mean surrogate loss: 0.0085\n","                 Mean entropy loss: 9.3366\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 46104576\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:53\n","                               ETA: 00:06:41\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 469/1000 \u001b[0m                      \n","\n","                       Computation: 157878 steps/s (collection: 0.270s, learning 0.353s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 1.8297\n","               Mean surrogate loss: 0.0055\n","                 Mean entropy loss: 9.3369\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 46202880\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:54\n","                               ETA: 00:06:40\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 470/1000 \u001b[0m                      \n","\n","                       Computation: 144764 steps/s (collection: 0.319s, learning 0.360s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.4700\n","               Mean surrogate loss: 0.0117\n","                 Mean entropy loss: 9.3370\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 46301184\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:05:55\n","                               ETA: 00:06:39\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 471/1000 \u001b[0m                      \n","\n","                       Computation: 144825 steps/s (collection: 0.312s, learning 0.367s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.3688\n","               Mean surrogate loss: 0.0029\n","                 Mean entropy loss: 9.3375\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 46399488\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:05:55\n","                               ETA: 00:06:38\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 472/1000 \u001b[0m                      \n","\n","                       Computation: 147107 steps/s (collection: 0.311s, learning 0.358s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.5071\n","               Mean surrogate loss: -0.0001\n","                 Mean entropy loss: 9.3413\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 46497792\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:05:56\n","                               ETA: 00:06:38\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 473/1000 \u001b[0m                      \n","\n","                       Computation: 146150 steps/s (collection: 0.306s, learning 0.366s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.7409\n","               Mean surrogate loss: 0.0041\n","                 Mean entropy loss: 9.3361\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 46596096\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:05:57\n","                               ETA: 00:06:37\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 474/1000 \u001b[0m                      \n","\n","                       Computation: 145484 steps/s (collection: 0.328s, learning 0.348s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 2.8563\n","               Mean surrogate loss: -0.0012\n","                 Mean entropy loss: 9.3377\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 46694400\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:05:57\n","                               ETA: 00:06:36\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 475/1000 \u001b[0m                      \n","\n","                       Computation: 156495 steps/s (collection: 0.274s, learning 0.355s)\n","             Mean action noise std: 1.19\n","          Mean value_function loss: 3.3866\n","               Mean surrogate loss: 0.0055\n","                 Mean entropy loss: 9.3397\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 46792704\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:05:58\n","                               ETA: 00:06:35\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 476/1000 \u001b[0m                      \n","\n","                       Computation: 158894 steps/s (collection: 0.266s, learning 0.352s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.3363\n","               Mean surrogate loss: 0.0024\n","                 Mean entropy loss: 9.3451\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 46891008\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:59\n","                               ETA: 00:06:34\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 477/1000 \u001b[0m                      \n","\n","                       Computation: 157971 steps/s (collection: 0.267s, learning 0.355s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.6489\n","               Mean surrogate loss: 0.0123\n","                 Mean entropy loss: 9.3529\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 46989312\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:05:59\n","                               ETA: 00:06:33\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 478/1000 \u001b[0m                      \n","\n","                       Computation: 156709 steps/s (collection: 0.270s, learning 0.357s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.4787\n","               Mean surrogate loss: 0.0079\n","                 Mean entropy loss: 9.3542\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 47087616\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:00\n","                               ETA: 00:06:32\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 479/1000 \u001b[0m                      \n","\n","                       Computation: 157790 steps/s (collection: 0.272s, learning 0.351s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.3357\n","               Mean surrogate loss: 0.0073\n","                 Mean entropy loss: 9.3543\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 47185920\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:01\n","                               ETA: 00:06:31\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 480/1000 \u001b[0m                      \n","\n","                       Computation: 157829 steps/s (collection: 0.272s, learning 0.351s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.7583\n","               Mean surrogate loss: 0.0072\n","                 Mean entropy loss: 9.3545\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 47284224\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:01\n","                               ETA: 00:06:30\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 481/1000 \u001b[0m                      \n","\n","                       Computation: 156790 steps/s (collection: 0.272s, learning 0.355s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.7275\n","               Mean surrogate loss: 0.0056\n","                 Mean entropy loss: 9.3548\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 47382528\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:02\n","                               ETA: 00:06:30\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 482/1000 \u001b[0m                      \n","\n","                       Computation: 157190 steps/s (collection: 0.269s, learning 0.356s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.8731\n","               Mean surrogate loss: 0.0030\n","                 Mean entropy loss: 9.3556\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 47480832\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:02\n","                               ETA: 00:06:29\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 483/1000 \u001b[0m                      \n","\n","                       Computation: 159941 steps/s (collection: 0.263s, learning 0.351s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.1687\n","               Mean surrogate loss: 0.0008\n","                 Mean entropy loss: 9.3594\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 47579136\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:06:03\n","                               ETA: 00:06:28\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 484/1000 \u001b[0m                      \n","\n","                       Computation: 158289 steps/s (collection: 0.267s, learning 0.354s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 3.1049\n","               Mean surrogate loss: 0.0025\n","                 Mean entropy loss: 9.3649\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 47677440\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:04\n","                               ETA: 00:06:27\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 485/1000 \u001b[0m                      \n","\n","                       Computation: 157222 steps/s (collection: 0.275s, learning 0.351s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.1088\n","               Mean surrogate loss: 0.0127\n","                 Mean entropy loss: 9.3685\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 47775744\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:04\n","                               ETA: 00:06:26\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 486/1000 \u001b[0m                      \n","\n","                       Computation: 157525 steps/s (collection: 0.267s, learning 0.357s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 1.7928\n","               Mean surrogate loss: 0.0022\n","                 Mean entropy loss: 9.3693\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 47874048\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:05\n","                               ETA: 00:06:25\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 487/1000 \u001b[0m                      \n","\n","                       Computation: 158783 steps/s (collection: 0.270s, learning 0.349s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 1.9982\n","               Mean surrogate loss: 0.0030\n","                 Mean entropy loss: 9.3713\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 47972352\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:06\n","                               ETA: 00:06:24\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 488/1000 \u001b[0m                      \n","\n","                       Computation: 157388 steps/s (collection: 0.264s, learning 0.360s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.1207\n","               Mean surrogate loss: 0.0071\n","                 Mean entropy loss: 9.3764\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 48070656\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:06\n","                               ETA: 00:06:23\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 489/1000 \u001b[0m                      \n","\n","                       Computation: 158404 steps/s (collection: 0.270s, learning 0.351s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 1.6634\n","               Mean surrogate loss: 0.0036\n","                 Mean entropy loss: 9.3775\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 48168960\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:07\n","                               ETA: 00:06:23\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 490/1000 \u001b[0m                      \n","\n","                       Computation: 151884 steps/s (collection: 0.273s, learning 0.374s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.0134\n","               Mean surrogate loss: 0.0060\n","                 Mean entropy loss: 9.3781\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 48267264\n","                    Iteration time: 0.65s\n","                      Time elapsed: 00:06:07\n","                               ETA: 00:06:22\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 491/1000 \u001b[0m                      \n","\n","                       Computation: 146374 steps/s (collection: 0.306s, learning 0.365s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.6178\n","               Mean surrogate loss: 0.0066\n","                 Mean entropy loss: 9.3786\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 48365568\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:06:08\n","                               ETA: 00:06:21\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 492/1000 \u001b[0m                      \n","\n","                       Computation: 142867 steps/s (collection: 0.336s, learning 0.352s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.5381\n","               Mean surrogate loss: 0.0096\n","                 Mean entropy loss: 9.3796\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 48463872\n","                    Iteration time: 0.69s\n","                      Time elapsed: 00:06:09\n","                               ETA: 00:06:20\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 493/1000 \u001b[0m                      \n","\n","                       Computation: 148193 steps/s (collection: 0.304s, learning 0.359s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 1.8249\n","               Mean surrogate loss: 0.0013\n","                 Mean entropy loss: 9.3805\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 48562176\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:06:09\n","                               ETA: 00:06:19\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 494/1000 \u001b[0m                      \n","\n","                       Computation: 142882 steps/s (collection: 0.328s, learning 0.360s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.4014\n","               Mean surrogate loss: -0.0017\n","                 Mean entropy loss: 9.3838\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 48660480\n","                    Iteration time: 0.69s\n","                      Time elapsed: 00:06:10\n","                               ETA: 00:06:18\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 495/1000 \u001b[0m                      \n","\n","                       Computation: 155773 steps/s (collection: 0.274s, learning 0.357s)\n","             Mean action noise std: 1.20\n","          Mean value_function loss: 2.5826\n","               Mean surrogate loss: 0.0031\n","                 Mean entropy loss: 9.3872\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 48758784\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:11\n","                               ETA: 00:06:17\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 496/1000 \u001b[0m                      \n","\n","                       Computation: 159296 steps/s (collection: 0.266s, learning 0.352s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 2.7976\n","               Mean surrogate loss: 0.0028\n","                 Mean entropy loss: 9.3912\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 48857088\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:11\n","                               ETA: 00:06:17\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 497/1000 \u001b[0m                      \n","\n","                       Computation: 157554 steps/s (collection: 0.269s, learning 0.355s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 2.5972\n","               Mean surrogate loss: 0.0036\n","                 Mean entropy loss: 9.3941\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 48955392\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:12\n","                               ETA: 00:06:16\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 498/1000 \u001b[0m                      \n","\n","                       Computation: 154649 steps/s (collection: 0.286s, learning 0.349s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 3.0935\n","               Mean surrogate loss: -0.0011\n","                 Mean entropy loss: 9.3963\n","                       Mean reward: 649.03\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 49053696\n","                    Iteration time: 0.64s\n","                      Time elapsed: 00:06:13\n","                               ETA: 00:06:15\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 499/1000 \u001b[0m                      \n","\n","                       Computation: 157302 steps/s (collection: 0.269s, learning 0.356s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 2.4363\n","               Mean surrogate loss: 0.0002\n","                 Mean entropy loss: 9.4055\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 49152000\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:13\n","                               ETA: 00:06:14\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 500/1000 \u001b[0m                      \n","\n","                       Computation: 159880 steps/s (collection: 0.268s, learning 0.347s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 1.6473\n","               Mean surrogate loss: 0.0017\n","                 Mean entropy loss: 9.4055\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 49250304\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:06:14\n","                               ETA: 00:06:13\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 501/1000 \u001b[0m                      \n","\n","                       Computation: 160916 steps/s (collection: 0.256s, learning 0.355s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 1.0432\n","               Mean surrogate loss: 0.0039\n","                 Mean entropy loss: 9.3977\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 49348608\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:06:14\n","                               ETA: 00:06:12\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 502/1000 \u001b[0m                      \n","\n","                       Computation: 158170 steps/s (collection: 0.267s, learning 0.354s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 1.0616\n","               Mean surrogate loss: 0.0039\n","                 Mean entropy loss: 9.3922\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 49446912\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:15\n","                               ETA: 00:06:11\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 503/1000 \u001b[0m                      \n","\n","                       Computation: 155071 steps/s (collection: 0.284s, learning 0.350s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 5.0101\n","               Mean surrogate loss: 0.0122\n","                 Mean entropy loss: 9.3896\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 49545216\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:16\n","                               ETA: 00:06:11\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 504/1000 \u001b[0m                      \n","\n","                       Computation: 156795 steps/s (collection: 0.267s, learning 0.360s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 4.3330\n","               Mean surrogate loss: 0.0009\n","                 Mean entropy loss: 9.3905\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 49643520\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:16\n","                               ETA: 00:06:10\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 505/1000 \u001b[0m                      \n","\n","                       Computation: 158240 steps/s (collection: 0.267s, learning 0.354s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 2.6757\n","               Mean surrogate loss: -0.0004\n","                 Mean entropy loss: 9.3961\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 49741824\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:17\n","                               ETA: 00:06:09\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 506/1000 \u001b[0m                      \n","\n","                       Computation: 157753 steps/s (collection: 0.268s, learning 0.355s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 2.2270\n","               Mean surrogate loss: 0.0043\n","                 Mean entropy loss: 9.4042\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 49840128\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:18\n","                               ETA: 00:06:08\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 507/1000 \u001b[0m                      \n","\n","                       Computation: 159022 steps/s (collection: 0.266s, learning 0.352s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 1.7144\n","               Mean surrogate loss: 0.0055\n","                 Mean entropy loss: 9.4124\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 49938432\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:18\n","                               ETA: 00:06:07\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 508/1000 \u001b[0m                      \n","\n","                       Computation: 157373 steps/s (collection: 0.275s, learning 0.350s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 1.4169\n","               Mean surrogate loss: 0.0022\n","                 Mean entropy loss: 9.4130\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 50036736\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:19\n","                               ETA: 00:06:06\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 509/1000 \u001b[0m                      \n","\n","                       Computation: 157144 steps/s (collection: 0.267s, learning 0.358s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 1.2766\n","               Mean surrogate loss: 0.0154\n","                 Mean entropy loss: 9.4064\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 50135040\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:19\n","                               ETA: 00:06:05\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 510/1000 \u001b[0m                      \n","\n","                       Computation: 157453 steps/s (collection: 0.267s, learning 0.357s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 1.5113\n","               Mean surrogate loss: 0.0013\n","                 Mean entropy loss: 9.4052\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 50233344\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:20\n","                               ETA: 00:06:04\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 511/1000 \u001b[0m                      \n","\n","                       Computation: 144123 steps/s (collection: 0.323s, learning 0.359s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 1.6106\n","               Mean surrogate loss: 0.0014\n","                 Mean entropy loss: 9.4057\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 50331648\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:06:21\n","                               ETA: 00:06:04\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 512/1000 \u001b[0m                      \n","\n","                       Computation: 144766 steps/s (collection: 0.310s, learning 0.369s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 1.5843\n","               Mean surrogate loss: 0.0115\n","                 Mean entropy loss: 9.4074\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 50429952\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:06:21\n","                               ETA: 00:06:03\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 513/1000 \u001b[0m                      \n","\n","                       Computation: 149083 steps/s (collection: 0.307s, learning 0.352s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 2.0890\n","               Mean surrogate loss: 0.0097\n","                 Mean entropy loss: 9.4087\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 50528256\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:06:22\n","                               ETA: 00:06:02\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 514/1000 \u001b[0m                      \n","\n","                       Computation: 143360 steps/s (collection: 0.311s, learning 0.374s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 2.3072\n","               Mean surrogate loss: 0.0107\n","                 Mean entropy loss: 9.4090\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 50626560\n","                    Iteration time: 0.69s\n","                      Time elapsed: 00:06:23\n","                               ETA: 00:06:01\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 515/1000 \u001b[0m                      \n","\n","                       Computation: 148884 steps/s (collection: 0.309s, learning 0.351s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 2.1001\n","               Mean surrogate loss: 0.0074\n","                 Mean entropy loss: 9.4094\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 50724864\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:06:23\n","                               ETA: 00:06:00\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 516/1000 \u001b[0m                      \n","\n","                       Computation: 155168 steps/s (collection: 0.276s, learning 0.357s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 2.4279\n","               Mean surrogate loss: 0.0051\n","                 Mean entropy loss: 9.4099\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 50823168\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:24\n","                               ETA: 00:06:00\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 517/1000 \u001b[0m                      \n","\n","                       Computation: 159091 steps/s (collection: 0.270s, learning 0.348s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 1.9012\n","               Mean surrogate loss: 0.0108\n","                 Mean entropy loss: 9.4104\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 50921472\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:25\n","                               ETA: 00:05:59\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 518/1000 \u001b[0m                      \n","\n","                       Computation: 154690 steps/s (collection: 0.275s, learning 0.360s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 2.4954\n","               Mean surrogate loss: 0.0025\n","                 Mean entropy loss: 9.4113\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 51019776\n","                    Iteration time: 0.64s\n","                      Time elapsed: 00:06:25\n","                               ETA: 00:05:58\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 519/1000 \u001b[0m                      \n","\n","                       Computation: 156120 steps/s (collection: 0.280s, learning 0.349s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 2.5846\n","               Mean surrogate loss: 0.0027\n","                 Mean entropy loss: 9.4135\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 51118080\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:26\n","                               ETA: 00:05:57\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 520/1000 \u001b[0m                      \n","\n","                       Computation: 155220 steps/s (collection: 0.268s, learning 0.365s)\n","             Mean action noise std: 1.21\n","          Mean value_function loss: 2.0709\n","               Mean surrogate loss: 0.0027\n","                 Mean entropy loss: 9.4148\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 51216384\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:27\n","                               ETA: 00:05:56\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 521/1000 \u001b[0m                      \n","\n","                       Computation: 159166 steps/s (collection: 0.269s, learning 0.348s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 2.1413\n","               Mean surrogate loss: 0.0028\n","                 Mean entropy loss: 9.4172\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 51314688\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:27\n","                               ETA: 00:05:55\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 522/1000 \u001b[0m                      \n","\n","                       Computation: 155262 steps/s (collection: 0.268s, learning 0.365s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 2.0832\n","               Mean surrogate loss: 0.0029\n","                 Mean entropy loss: 9.4183\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 51412992\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:28\n","                               ETA: 00:05:54\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 523/1000 \u001b[0m                      \n","\n","                       Computation: 160447 steps/s (collection: 0.263s, learning 0.350s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 1.9892\n","               Mean surrogate loss: 0.0060\n","                 Mean entropy loss: 9.4182\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 51511296\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:06:28\n","                               ETA: 00:05:54\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 524/1000 \u001b[0m                      \n","\n","                       Computation: 155424 steps/s (collection: 0.271s, learning 0.361s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 2.2851\n","               Mean surrogate loss: 0.0007\n","                 Mean entropy loss: 9.4228\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 51609600\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:29\n","                               ETA: 00:05:53\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 525/1000 \u001b[0m                      \n","\n","                       Computation: 159114 steps/s (collection: 0.266s, learning 0.352s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 2.4129\n","               Mean surrogate loss: 0.0028\n","                 Mean entropy loss: 9.4327\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 51707904\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:30\n","                               ETA: 00:05:52\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 526/1000 \u001b[0m                      \n","\n","                       Computation: 157050 steps/s (collection: 0.273s, learning 0.353s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 2.7114\n","               Mean surrogate loss: 0.0069\n","                 Mean entropy loss: 9.4390\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 51806208\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:30\n","                               ETA: 00:05:51\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 527/1000 \u001b[0m                      \n","\n","                       Computation: 158496 steps/s (collection: 0.263s, learning 0.357s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 1.9631\n","               Mean surrogate loss: 0.0048\n","                 Mean entropy loss: 9.4395\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 51904512\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:31\n","                               ETA: 00:05:50\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 528/1000 \u001b[0m                      \n","\n","                       Computation: 158443 steps/s (collection: 0.267s, learning 0.354s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 1.6658\n","               Mean surrogate loss: 0.0043\n","                 Mean entropy loss: 9.4386\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 52002816\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:32\n","                               ETA: 00:05:49\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 529/1000 \u001b[0m                      \n","\n","                       Computation: 154149 steps/s (collection: 0.271s, learning 0.367s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 1.9491\n","               Mean surrogate loss: 0.0061\n","                 Mean entropy loss: 9.4349\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 52101120\n","                    Iteration time: 0.64s\n","                      Time elapsed: 00:06:32\n","                               ETA: 00:05:49\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 530/1000 \u001b[0m                      \n","\n","                       Computation: 148449 steps/s (collection: 0.307s, learning 0.355s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 1.6750\n","               Mean surrogate loss: 0.0116\n","                 Mean entropy loss: 9.4352\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 52199424\n","                    Iteration time: 0.66s\n","                      Time elapsed: 00:06:33\n","                               ETA: 00:05:48\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 531/1000 \u001b[0m                      \n","\n","                       Computation: 132346 steps/s (collection: 0.369s, learning 0.373s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 1.9051\n","               Mean surrogate loss: 0.0203\n","                 Mean entropy loss: 9.4360\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 52297728\n","                    Iteration time: 0.74s\n","                      Time elapsed: 00:06:34\n","                               ETA: 00:05:47\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 532/1000 \u001b[0m                      \n","\n","                       Computation: 144730 steps/s (collection: 0.311s, learning 0.368s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 1.7566\n","               Mean surrogate loss: 0.0056\n","                 Mean entropy loss: 9.4365\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 52396032\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:06:34\n","                               ETA: 00:05:46\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 533/1000 \u001b[0m                      \n","\n","                       Computation: 140852 steps/s (collection: 0.334s, learning 0.364s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 2.0595\n","               Mean surrogate loss: 0.0102\n","                 Mean entropy loss: 9.4371\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 52494336\n","                    Iteration time: 0.70s\n","                      Time elapsed: 00:06:35\n","                               ETA: 00:05:45\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 534/1000 \u001b[0m                      \n","\n","                       Computation: 121923 steps/s (collection: 0.417s, learning 0.389s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 2.2249\n","               Mean surrogate loss: 0.0040\n","                 Mean entropy loss: 9.4376\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 52592640\n","                    Iteration time: 0.81s\n","                      Time elapsed: 00:06:36\n","                               ETA: 00:05:45\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 535/1000 \u001b[0m                      \n","\n","                       Computation: 136447 steps/s (collection: 0.348s, learning 0.372s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 2.1947\n","               Mean surrogate loss: 0.0113\n","                 Mean entropy loss: 9.4394\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 52690944\n","                    Iteration time: 0.72s\n","                      Time elapsed: 00:06:37\n","                               ETA: 00:05:44\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 536/1000 \u001b[0m                      \n","\n","                       Computation: 139716 steps/s (collection: 0.340s, learning 0.364s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 2.4453\n","               Mean surrogate loss: 0.0032\n","                 Mean entropy loss: 9.4399\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 52789248\n","                    Iteration time: 0.70s\n","                      Time elapsed: 00:06:37\n","                               ETA: 00:05:43\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 537/1000 \u001b[0m                      \n","\n","                       Computation: 147068 steps/s (collection: 0.321s, learning 0.347s)\n","             Mean action noise std: 1.22\n","          Mean value_function loss: 2.4966\n","               Mean surrogate loss: -0.0006\n","                 Mean entropy loss: 9.4418\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 52887552\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:06:38\n","                               ETA: 00:05:42\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 538/1000 \u001b[0m                      \n","\n","                       Computation: 152947 steps/s (collection: 0.286s, learning 0.357s)\n","             Mean action noise std: 1.23\n","          Mean value_function loss: 2.3618\n","               Mean surrogate loss: 0.0000\n","                 Mean entropy loss: 9.4467\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 52985856\n","                    Iteration time: 0.64s\n","                      Time elapsed: 00:06:39\n","                               ETA: 00:05:42\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 539/1000 \u001b[0m                      \n","\n","                       Computation: 158092 steps/s (collection: 0.266s, learning 0.356s)\n","             Mean action noise std: 1.23\n","          Mean value_function loss: 1.9952\n","               Mean surrogate loss: 0.0085\n","                 Mean entropy loss: 9.4500\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 53084160\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:39\n","                               ETA: 00:05:41\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 540/1000 \u001b[0m                      \n","\n","                       Computation: 156713 steps/s (collection: 0.274s, learning 0.354s)\n","             Mean action noise std: 1.23\n","          Mean value_function loss: 2.0953\n","               Mean surrogate loss: 0.0077\n","                 Mean entropy loss: 9.4522\n","                       Mean reward: 691.99\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 53182464\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:40\n","                               ETA: 00:05:40\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 541/1000 \u001b[0m                      \n","\n","                       Computation: 157911 steps/s (collection: 0.265s, learning 0.357s)\n","             Mean action noise std: 1.23\n","          Mean value_function loss: 1.7092\n","               Mean surrogate loss: -0.0016\n","                 Mean entropy loss: 9.4529\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 53280768\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:40\n","                               ETA: 00:05:39\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 542/1000 \u001b[0m                      \n","\n","                       Computation: 157870 steps/s (collection: 0.273s, learning 0.350s)\n","             Mean action noise std: 1.23\n","          Mean value_function loss: 1.4985\n","               Mean surrogate loss: -0.0018\n","                 Mean entropy loss: 9.4487\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 53379072\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:41\n","                               ETA: 00:05:38\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 543/1000 \u001b[0m                      \n","\n","                       Computation: 157722 steps/s (collection: 0.264s, learning 0.359s)\n","             Mean action noise std: 1.23\n","          Mean value_function loss: 1.7441\n","               Mean surrogate loss: -0.0009\n","                 Mean entropy loss: 9.4466\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 53477376\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:42\n","                               ETA: 00:05:37\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 544/1000 \u001b[0m                      \n","\n","                       Computation: 158039 steps/s (collection: 0.267s, learning 0.355s)\n","             Mean action noise std: 1.23\n","          Mean value_function loss: 2.6333\n","               Mean surrogate loss: -0.0020\n","                 Mean entropy loss: 9.4481\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 53575680\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:42\n","                               ETA: 00:05:37\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 545/1000 \u001b[0m                      \n","\n","                       Computation: 156600 steps/s (collection: 0.271s, learning 0.357s)\n","             Mean action noise std: 1.23\n","          Mean value_function loss: 2.8377\n","               Mean surrogate loss: -0.0006\n","                 Mean entropy loss: 9.4561\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 53673984\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:43\n","                               ETA: 00:05:36\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 546/1000 \u001b[0m                      \n","\n","                       Computation: 159118 steps/s (collection: 0.267s, learning 0.350s)\n","             Mean action noise std: 1.23\n","          Mean value_function loss: 3.1908\n","               Mean surrogate loss: 0.0005\n","                 Mean entropy loss: 9.4644\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 53772288\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:44\n","                               ETA: 00:05:35\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 547/1000 \u001b[0m                      \n","\n","                       Computation: 156899 steps/s (collection: 0.268s, learning 0.358s)\n","             Mean action noise std: 1.23\n","          Mean value_function loss: 2.5127\n","               Mean surrogate loss: 0.0012\n","                 Mean entropy loss: 9.4766\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 53870592\n","                    Iteration time: 0.63s\n","                      Time elapsed: 00:06:44\n","                               ETA: 00:05:34\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 548/1000 \u001b[0m                      \n","\n","                       Computation: 159451 steps/s (collection: 0.266s, learning 0.350s)\n","             Mean action noise std: 1.23\n","          Mean value_function loss: 2.2279\n","               Mean surrogate loss: 0.0041\n","                 Mean entropy loss: 9.4870\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 53968896\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:45\n","                               ETA: 00:05:33\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 549/1000 \u001b[0m                      \n","\n","                       Computation: 157942 steps/s (collection: 0.263s, learning 0.360s)\n","             Mean action noise std: 1.24\n","          Mean value_function loss: 1.7973\n","               Mean surrogate loss: 0.0031\n","                 Mean entropy loss: 9.4930\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 54067200\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:45\n","                               ETA: 00:05:32\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 550/1000 \u001b[0m                      \n","\n","                       Computation: 158783 steps/s (collection: 0.269s, learning 0.350s)\n","             Mean action noise std: 1.24\n","          Mean value_function loss: 1.4299\n","               Mean surrogate loss: 0.0019\n","                 Mean entropy loss: 9.4994\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 54165504\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:46\n","                               ETA: 00:05:32\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 551/1000 \u001b[0m                      \n","\n","                       Computation: 157951 steps/s (collection: 0.262s, learning 0.360s)\n","             Mean action noise std: 1.24\n","          Mean value_function loss: 1.3455\n","               Mean surrogate loss: 0.0020\n","                 Mean entropy loss: 9.5046\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 54263808\n","                    Iteration time: 0.62s\n","                      Time elapsed: 00:06:47\n","                               ETA: 00:05:31\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 552/1000 \u001b[0m                      \n","\n","                       Computation: 160160 steps/s (collection: 0.263s, learning 0.351s)\n","             Mean action noise std: 1.24\n","          Mean value_function loss: 1.2719\n","               Mean surrogate loss: 0.0039\n","                 Mean entropy loss: 9.5077\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 54362112\n","                    Iteration time: 0.61s\n","                      Time elapsed: 00:06:47\n","                               ETA: 00:05:30\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 553/1000 \u001b[0m                      \n","\n","                       Computation: 150396 steps/s (collection: 0.289s, learning 0.365s)\n","             Mean action noise std: 1.24\n","          Mean value_function loss: 1.4725\n","               Mean surrogate loss: 0.0005\n","                 Mean entropy loss: 9.5082\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 54460416\n","                    Iteration time: 0.65s\n","                      Time elapsed: 00:06:48\n","                               ETA: 00:05:29\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 554/1000 \u001b[0m                      \n","\n","                       Computation: 147709 steps/s (collection: 0.304s, learning 0.362s)\n","             Mean action noise std: 1.24\n","          Mean value_function loss: 1.4616\n","               Mean surrogate loss: 0.0018\n","                 Mean entropy loss: 9.5075\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 54558720\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:06:49\n","                               ETA: 00:05:28\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 555/1000 \u001b[0m                      \n","\n","                       Computation: 144720 steps/s (collection: 0.322s, learning 0.357s)\n","             Mean action noise std: 1.24\n","          Mean value_function loss: 1.7908\n","               Mean surrogate loss: 0.0023\n","                 Mean entropy loss: 9.5023\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 54657024\n","                    Iteration time: 0.68s\n","                      Time elapsed: 00:06:49\n","                               ETA: 00:05:27\n","\n","################################################################################\n","                     \u001b[1m Learning iteration 556/1000 \u001b[0m                      \n","\n","                       Computation: 146357 steps/s (collection: 0.307s, learning 0.365s)\n","             Mean action noise std: 1.24\n","          Mean value_function loss: 1.9811\n","               Mean surrogate loss: 0.0008\n","                 Mean entropy loss: 9.4967\n","                       Mean reward: 698.44\n","               Mean episode length: 1000.00\n","--------------------------------------------------------------------------------\n","                   Total timesteps: 54755328\n","                    Iteration time: 0.67s\n","                      Time elapsed: 00:06:50\n","                               ETA: 00:05:27\n","\n"]}],"source":["runner.learn(\n","    num_learning_iterations=1000,#train_cfg.max_iterations,\n","    init_at_random_ep_len=False,\n",")\n","print(\"Done training.\")"]},{"cell_type":"markdown","id":"c85a832d","metadata":{"id":"c85a832d"},"source":["### Evaluate the Cheetah Policy\n","\n","`TODO(student):` Similar to previous labs, collect a rollout using the trained policy. **(20 points)**"]},{"cell_type":"code","execution_count":null,"id":"58169df9","metadata":{"id":"58169df9"},"outputs":[],"source":["policy = runner.get_inference_policy(device=device)\n","eval_env = registry.load(_ENV_NAME, config=env_cfg)\n","\n","jit_reset = jax.jit(eval_env.reset)\n","jit_step = jax.jit(eval_env.step)\n","\n","rng = jax.random.PRNGKey(_SEED)\n","state = jit_reset(rng)\n","rollout = [state]\n","\n","#TODO(student): Rollout the policy in the environment.\n","# Note that the policy is a torch module, and accepts toch-based\n","# observations as input. (i.e. actions = policy(obs_t)) where obs_t is a torch tensor\n","for step_idx in range(env_cfg.episode_length):\n","    # Get observation from JAX state\n","    obs_jax = state.obs\n","\n","    obs_np = np.array(obs_jax)\n","    obs_torch = torch.from_numpy(obs_np).float().unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        action_torch = policy(obs_torch)\n","\n","    action_np = action_torch.cpu().numpy().squeeze()\n","    action_jax = jax.numpy.array(action_np)\n","\n","    state = jit_step(state, action_jax)\n","    rollout.append(state)\n","\n","print(f\"Rollout complete! Collected {len(rollout)} states.\")\n","print(f\"Final reward: {float(state.reward):.3f}\")\n"]},{"cell_type":"markdown","id":"4be7d27e","metadata":{"id":"4be7d27e"},"source":["Given the rollout you just collected, the following should render your Cheetah policy trained with the external RSL_RL library!"]},{"cell_type":"code","execution_count":null,"id":"d9304b20","metadata":{"id":"d9304b20"},"outputs":[],"source":["# Render\n","scene_option = mujoco.MjvOption()\n","scene_option.flags[mujoco.mjtVisFlag.mjVIS_TRANSPARENT] = True\n","scene_option.flags[mujoco.mjtVisFlag.mjVIS_PERTFORCE] = True\n","scene_option.flags[mujoco.mjtVisFlag.mjVIS_CONTACTFORCE] = False\n","\n","render_every = 2\n","\n","# If your environment is wrapped multiple times, adjust as needed:\n","base_env = eval_env  # or brax_env.env.env.env\n","fps = 1.0 / base_env.dt / render_every\n","traj = rollout[::render_every]\n","frames = eval_env.render(\n","    traj,\n","    camera=\"back\", # or \"side\"\n","    height=480,\n","    width=640,\n","    scene_option=scene_option,\n",")\n","\n","media.write_video(\"dm_control.mp4\", frames, fps=fps)\n","print(\"Rollout video saved as 'dm_control.mp4'.\")\n","\n","media.show_video(frames, fps=fps)"]},{"cell_type":"markdown","id":"1f9c2ab3","metadata":{"id":"1f9c2ab3"},"source":["## What to Turn In\n","\n","`#TODO(student):` Please zip the following files and turn them into the assignment on gradescope:\n","1. this `06_lab_student.ipynb` file. Please make sure to fill our your name and umich ID in the first cell\n","2. the `CSE598RSLWrapper.py` file\n","3. Your modified environment file `custom_env.py` file (if applicable)\n","\n","Please ensure all cell outputs (videos, plots, etc) are in tact when you download the .ipynb file."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}